{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 强化学习算法粗略分类表：\n",
    "\n",
    "| 阶段            | 时间范围        | 方法类别              | 代表算法 / 论文                                                                                                                                                                              | 特点与突破                                           |\n",
    "|---------------|-------------|-------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------|\n",
    "| **早期表格方法**    | 1950s~1980s | 动态规划、MC、TD        | - Dynamic Programming (Bellman, 1957)<br>- Monte Carlo (1950s)<br>- TD(0), TD(λ) (Sutton, 1988)<br>- Q-Learning (Watkins, 1989)<br>- SARSA (Rummery & Niranjan, 1994)                  | - 基于小状态空间，表格更新<br>- 强化学习理论基础奠定                  |\n",
    "| **函数逼近方法**    | 1990s~2013  | 线性逼近、核方法、浅层网络     | - Linear TD / LSTD<br>- Tile Coding / RBF<br>- Neural Fitted Q (NFQ, 2005)<br>- LSPI（Lagoudakis, 2003）                                                                                 | - 应对大状态空间<br>- 非线性近似开始尝试                        |\n",
    "| **深度强化学习兴起**  | 2013~2016   | 深度值函数方法           | - DQN (DeepMind, 2015)<br>- Double DQN (2016)<br>- Dueling DQN<br>- Prioritized Replay<br>- Rainbow DQN (2017)                                                                         | - 使用CNN处理图像<br>- 经验回放与目标网络稳定训练                  |\n",
    "| **策略梯度时代**    | 2016~2018   | 策略优化、Actor-Critic | - REINFORCE（1992,最早的策略梯度方法）<br>- A3C (2016)<br>- TRPO (2015)<br>- PPO (2017)<br>- DDPG (2015)<br>- TD3 (2018)<br>- SAC (2018)                                                          | - 可处理连续动作空间<br>- Off-policy策略可复用经验              |\n",
    "| **模型推理与探索智能** | 2018~至今     | 模型学习、元学习、人类反馈     | - World Models (Ha & Schmidhuber, 2018)<br>- MuZero (DeepMind, 2019)<br>- Dreamer (2020~)<br>- CQL / BCQ (Offline RL)<br>- Curiosity-driven RL<br>- RLHF (ChatGPT, InstructGPT, 2022~) | - 融合世界模型 + 策略优化<br>- 探索泛化与样本效率<br>- RL与自然语言对齐结合 |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2eef024271c09674"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 强化学习算法时间线表：\n",
    "\n",
    "| 年份   | 算法 / 方法                                       | 算法类型                        | 应用领域 / 技术特征              | 主要贡献与特征                  | 跳转链接                                                                                                                                            |\n",
    "|------|-----------------------------------------------|-----------------------------|--------------------------|--------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1957 | Dynamic Programming                           | 基于模型                        | 理论奠基                     | 提出贝尔曼方程，强化学习基础           | [打开学习文档](RL_Fundamentals/5.动态规划(dynamic_programming).ipynb)                                                                                     |\n",
    "| 1983 | TD(λ)                                         | TD学习                        | 表格法、Bootstrapping        | 引入 TD 学习，结合 MC 与 DP      | [打开学习文档](RL_Fundamentals/6.时序差分(temporal_difference,TD).ipynb)                                                                                  |\n",
    "| 1989 | Q‑Learning                                    | 值函数、Off-policy              | 表格法                      | 离策略 Q 学习，最大化长期回报         | [打开学习文档](RL_Fundamentals/6.时序差分(temporal_difference,TD).ipynb)                                                                                  |\n",
    "| 1992 | REINFORCE                                     | 策略梯度                        | 表格/神经网络 + Monte Carlo    | 最早的正式提出的策略梯度方法           | [打开学习文档](RL_Advanced_Algorithms/12.REINFORCE(CartPole-v1).ipynb)                                                                                |\n",
    "| 1994 | SARSA                                         | 值函数、On-policy               | 表格法                      | 基于当前策略更新 Q 值             | [打开学习文档](RL_Fundamentals/6.时序差分(temporal_difference,TD).ipynb)                                                                                  |\n",
    "| 2000 | Actor-Critic                                  | 值函数+策略优化                    | Actor（执行者） 和 Critic（评论者） | 结合策略优化与值函数估计             | [打开学习文档](RL_Advanced_Algorithms/13.基础Actor-Critic算法(CartPole-v1).ipynb)                                                                         |\n",
    "| 2003 | LSPI（Least-Squares Policy Iteration，最小二乘策略迭代） | 值函数逼近                       | 函数逼近、线性方法                | 最小二乘策略迭代                 | [最小二乘法的几何原理](https://www.bilibili.com/video/BV15zPBevERL/?spm_id_from=333.337.search-card.all.click&vd_source=aa578ebaefb6f9eaa7073f057d120c80) |\n",
    "| 2005 | NFQ（Neural Fitted Q-Iteration，神经拟合 Q 迭代）      | 值函数逼近                       | 神经网络、离线训练                | 早期使用 NN 逼近 Q 函数          | [论文学习](https://link.springer.com/chapter/10.1007/11564096_32#preview)                                                                           |\n",
    "| 2015 | DQN                                           | 值函数、Off-policy              | 深度 Q 网络，可图像输入            | 使用 CNN 逼近 Q 值，经验回放       | [打开学习文档](RL_Advanced_Algorithms/8.原始DQN算法(CartPole-v1).ipynb)                                                                                   |\n",
    "| 2015 | DDPG                                          | Actor-Critic                | 连续动作空间、确定性策略             | DQN + Policy Gradient 融合 | [打开学习文档](RL_Advanced_Algorithms/16.DDPG(Pendulum-v1).ipynb)                                                                                     |\n",
    "| 2015 | TRPO                                          | 策略优化                        | 强约束更新，连续动作空间             | 引入 Trust Region 提高稳定性    | [打开学习文档](RL_Advanced_Algorithms/14.1_TRPO算法原理.ipynb)                                                                                            |\n",
    "| 2016 | A3C                                           | Actor-Critic                | 并行异步训练                   | 多线程异步训练提高效率              | [论文学习](https://arxiv.org/abs/1602.01783)                                                                                                        |\n",
    "| 2016 | Double DQN                                    | 值函数                         | Q 值修正                    | 减少过估计偏差                  | [打开学习文档](./RL_Advanced_Algorithms/10.DoubleDQN(Pendulum-v1).ipynb)                                                                              |\n",
    "| 2016 | Dueling DQN                                   | 值函数结构改进                     | V/A 分离结构                 | 更好的泛化能力                  | [打开学习文档](RL_Advanced_Algorithms/9.DuelingDQN(Pendulum-v1).ipynb)                                                                                |\n",
    "| 2016 | Prioritized Experience Replay (PER)           | 改进传统经验回放（Experience Replay） | 每个经验样本分配一个优先级            | 提高训练效率                   | [论文学习](https://arxiv.org/abs/1511.05952)                                                                                                        |\n",
    "| 2017 | PPO                                           | 策略优化(TRPO简化)                | 易于实现，训练稳定                | 工业界主流策略优化算法              | [打开学习文档](RL_Advanced_Algorithms/15.1_PPO算法原理.ipynb)                                                                                             |\n",
    "| 2017 | Rainbow DQN                                   | 值函数                         | 6种 DQN 技术融合              | 整合多项改进方法                 | [论文学习](https://arxiv.org/abs/1710.02298)                                                                                                        |\n",
    "| 2018 | TD3                                           | Actor-Critic                | 连续控制、延迟更新                | 解决 DDPG 过估计问题            | [无]()                                                                                                                                           |\n",
    "| 2018 | SAC                                           | Actor-Critic                | 熵正则、探索增强                 | 训练更稳定、更强鲁棒性              | [无]()                                                                                                                                           |\n",
    "| 2018 | World Models                                  | 模型学习                        | 潜在空间建模                   | 用 VAE + RNN 模拟环境         | [无]()                                                                                                                                           |\n",
    "| 2019 | MuZero                                        | 模型学习 + 规划                   | 无需显式模型，结合 MCTS           | 强化 AlphaZero，提升泛化        | [无]()                                                                                                                                           |\n",
    "| 2020 | Dreamer                                       | 模型学习                        | 潜空间中优化策略                 | 世界模型强化 Dreaming 更新       | [无]()                                                                                                                                           |\n",
    "| 2022 | RLHF                                          | 人类反馈强化学习                    | NLP 模型对齐                 | 强化学习用于语言对齐训练             | [无]()                                                                                                                                           |\n",
    "| 2023 | AlphaDev                                      | 强化合成                        | 算法设计、系统优化                | RL 自动发现排序与哈希新算法          | [无]()                                                                                                                                           |\n",
    "| 2023 | DSAC                                          | 分布式值函数                      | 连续控制                     | 学习值的分布避免过估计              | [无]()                                                                                                                                           |\n",
    "| 2024 | CC-MARL                                       | 多智能体 RL                     | 合作 + 竞争                  | 支持复杂多智能体博弈               | [无]()                                                                                                                                           |\n",
    "| 2024 | OPEH                                          | 探索优化                        | hindsight 重用样本           | 提高离策略探索能力                | [无]()                                                                                                                                           |\n",
    "| 2024 | OREO                                          | 离线 RL + LLM                 | 多步推理、语言模型训练              | 改进多步逻辑推理表现               | [无]()                                                                                                                                           |\n",
    "| 2025 | Transformer World Models                      | 模型学习                        | 因果世界建模、Transformer       | 结构化表示复杂动态过程              | [无]()                                                                                                                                           |\n",
    "| 2025 | OpenAI o1 / DeepSeek R1                       | LLM + 强化推理                  | 编程/数学等多步任务               | 强化思维链能力                  | [无]()                                                                                                                                           |\n",
    "| 2025 | Databricks TAO                                | 数据增强优化                      | 自适应数据生成                  | 合成样本提升模型性能               | [无]()                                                                                                                                           |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "76663ee17bad5494"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
