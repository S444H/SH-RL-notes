{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 强化学习算法进化史：时间轴 + 代表算法\n",
    "\n",
    "| 年份 | 阶段           | 代表算法 / 事件                                                                 | 关键词                                         |\n",
    "|------|--------------|-------------------------------------------------------------------------------------|-------------------------------------------------|\n",
    "| 1957 | 启蒙期          | 贝尔曼方程，动态规划（DP）                                                          | 马尔可夫决策过程，规划                             |\n",
    "| 1989 | 表格型基础        | Q‑Learning                                                                          | off‑policy，值函数学习                             |\n",
    "| 1992 | 在线更新         | SARSA                                                                               | on‑policy                                         |\n",
    "| 1998 | 策略+值分离       | Actor‑Critic                                                                        | 策略和值分离                                       |\n",
    "| 2000s | 函数逼近尝试       | Tile Coding，线性逼近                                                              | 泛化状态空间                                       |\n",
    "| 2013 | 深度强化元年       | Deep Q‑Network (DQN)                                                               | CNN + Atari                                     |\n",
    "| 2015 | 误差纠正         | Double DQN                                                                          | 降低 Q 值高估                                      |\n",
    "| 2016 | 架构创新         | Dueling DQN，A3C                                                                   | 状态/动作分支、异步策略优化                        |\n",
    "| 2017 | 稳定策略优化       | PPO，TRPO                                                                           | 约束策略更新                                       |\n",
    "| 2018 | 连续控制强化       | DDPG，TD3，SAC                                                                      | 高效连续动作控制                                   |\n",
    "| 2019 | 模型与序列融合起步    | MuZero，Dreamer，Decision Transformer                                              | 模型学习+Transformer                              |\n",
    "| 2020 | 高级模型博弈       | MuZero 利用无模型博弈达峰值性能                                                     | 模型预测+搜索                                     |\n",
    "| 2023 | 算法发现新高地      | AlphaDev: RL发现排序/哈希算法入 C++ 标准库 :contentReference[oaicite:1]{index=1}     | RL用于编码优化                                    |\n",
    "| 2023–24| Meta & 层次化学习 | PEARL、VariBAD 等元学习算法；层次强化学习（HRL） :contentReference[oaicite:2]{index=2}             | 快速适应新任务                                    |\n",
    "| 2024 | VLA & 多模态强化  | OpenVLA, π_0, Helix（通用于机器人） :contentReference[oaicite:3]{index=3}                            | 视觉–语言–动作集成                                 |\n",
    "| 2024–25| 分布式 & 安全强化   | DSAC 分布式 SAC，安全+分布式 off‑policy 控制 :contentReference[oaicite:4]{index=4}                    | 估值分布 + 风险感知                               |\n",
    "| 2025 | 算法演化自动化      | AlphaEvolve（DeepMind）：LLM+RL 混合生成新算法 :contentReference[oaicite:5]{index=5}               | LLM 辅助进化算法                                   |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6da38632cda2c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
