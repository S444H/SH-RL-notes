{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 18.模仿学习（Imitation Learning，简称IL）\n",
    "> 虽然 **强化学习** 不需要有监督学习中的标签数据，但它十分依赖 **奖励函数** 的设置。有时在 **奖励函数** 上做一些微小的改动，训练出来的策略就会有天差地别。而且在现实场景中， **奖励信号** 总是难以明确：\n",
    "- 例如，对于无人驾驶车辆智能体的规控，其观测是当前的环境感知恢复的 3D 局部环境，动作是车辆接下来数秒的具体路径规划，那么奖励是什么？如果只是规定正常行驶而不发生碰撞的奖励为+1，发生碰撞为-100，那么智能体学习的结果则很可能是找个地方停滞不前。**奖励函数** 往往需要精心设计和调试。\n",
    "> 那有什么方法可以跳过 **奖励函数** 的设计呢？\n",
    "\n",
    "> **模仿学习** 研究的便是这一类问题，在模仿学习的框架下，存在 **专家智能体**，其在某个环境下提供的一系列 **状态动作对**，被认为是 **最优行为策略**。**模仿者**的任务则是利用这些 **专家数据** 进行训练，**无须奖励信号** 就可以达到一个接近专家的策略。\n",
    "> 学术界关于 **模仿学习** 的传统核心方法分为 **3 类**：\n",
    "\n",
    "| 方法                                                            | 概念                                                                                           | 缺点                      | 适用任务                                 | 最早时间 | 论文链接                                                                                                     |\n",
    "|---------------------------------------------------------------|----------------------------------------------------------------------------------------------|-------------------------|--------------------------------------|------|----------------------------------------------------------------------------------------------------------|\n",
    "| **行为克隆（behavior cloning，BC）**                                 | 通过监督学习模仿专家的状态-动作对，直接复制专家的行为                                                                  | - 缺乏长期奖励优化<br> - 数据依赖性强 | 数据充足且专家行为完美的任务，如机器人抓取、路径规划等          | 1988 | [论文学习](https://papers.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf) |\n",
    "| **逆强化学习（inverse RL）**                                         | 不同于行为克隆仅仅是复制专家的动作，它的目标是从专家的行为中推断出奖励函数。通过推断奖励函数，智能体就可以基于奖励来优化自己的策略了                           | - 计算复杂度高<br> - 数据需求大    | 无法直接获取奖励信号或奖励函数难以设计的任务，如自主驾驶、复杂策略规划。 | 2000 | [论文学习](https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2022_2023/papers/NG_ICML_2000.pdf)              |\n",
    "| **生成式对抗模仿学习（generative adversarial imitation learning，GAIL）** | 结合生成对抗网络（GANs）将模仿学习视为一个对抗过程，其中一个生成器（模仿者）试图模仿专家的行为，另一个判别器（判别器）则尝试判断动作是否来自专家。最终，生成器学会如何模仿专家的策略 | - 训练不稳定<br> - 对样本质量敏感   | 没有明确奖励信号的复杂任务，如机器人运动、视频游戏AI等。        | 2016 | [论文学习](https://arxiv.org/abs/1606.03476)                                                                 |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b524eb201dc3aee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 本节主要介绍 **行为克隆** 和 **生成式对抗模仿学习**\n",
    "> **逆强化学习** 有良好的学术贡献，但由于其计算复杂度较高，实际应用的价值较小"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a9f40f2bda35ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 18.1 行为克隆\n",
    "> **行为克隆（BC）** 直接使用 **监督学习** 方法：\n",
    "$$\\theta^*=\\arg\\min_\\theta\\mathbb{E}_{(s,a)\\sim B}[\\mathcal{L}(\\pi_\\theta(s),a)]$$\n",
    "- $B$是专家的数据集，$\\mathcal{L}$是对应监督学习框架下的损失函数\n",
    "- 若动作是离散的，该损失函数可以是最大似然估计得到的；若动作是连续的，该损失函数可以是均方误差函数\n",
    "> 但也造成了其存在很大的局限性：由于训练使用的 **专家数据** 难以囊括所有状态分布，因此 BC 只能在专家数据的状态分布下预测得比较准。而且，**强化学习** 面对的是一个序贯决策问题，，只要存在一点偏差，就有可能导致下一个遇到的状态是在专家数据中没有见过的。此时，由于没有在此状态（或者比较相近的状态）下训练过，策略可能就会随机选择一个动作，这会导致下一个状态 **进一步偏离** 专家策略遇到的的数据分布。最终，该策略在真实环境下不能得到比较好的效果，这被称为行为克隆的 **复合误差（compounding error）** 问题：\n",
    "![行为克隆带来的复合误差问题](Illustrations/行为克隆带来的复合误差问题.png)\n",
    "> 即使 **行为克隆** 有这些局限，但其实现十分简单，因此在很多实际场景下它都可以作为 **策略预训练** 的方法：使 **策略** 无须在 **最开始低效地** 通过和环境交互来探索较好的动作，而是通过模仿专家智能体的行为数据来 **快速达到较高水平**，为接下来的强化学习创造一个 **高起点**。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31feca806a33ccfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 18.2 生成式对抗模仿学习\n",
    "> **生成式对抗模仿学习** 是2016年由斯坦福大学研究团队提出的 **基于生成式对抗网络** 的**模仿学习**，它诠释了**生成式对抗网络**的 **本质** 其实就是**模仿学习**\n",
    "> **GAIL** 算法中有一个 **判别器** 和一个 **策略**：\n",
    "- **策略** 就相当于是生成式对抗网络中的 **生成器（generator）**，给定一个状态，**策略** 会输出这个状态下应该采取的动作\n",
    "- **判别器（discriminator）$D_\\phi$** 将状态动作对$(s,a)$作为输入，输出一个 0 到 1 之间的实数，表示 **判别器** 认为该状态动作对来自 **智能体策略$\\theta$** 而非 **专家$E$** 的概率。**判别器** 的目标是尽量将专家数据的输出靠近 0，将模仿者策略的输出靠近 1，这样就可以 **将两组数据分辨开**。\n",
    "\n",
    "> 于是**判别器$D_\\phi$** 的目标是最大化对数似然：\n",
    "$$\\mathcal{L}_D(\\phi)=\\mathbb{E}_{(s,a)\\sim\\pi_\\theta}[\\log D_\\phi(s,a)]+\\mathbb{E}_{(s,a)\\sim\\pi_E}[\\log(1-D_\\phi(s,a))]$$\n",
    "\n",
    "> 实际训练时，取负，最小化 **损失函数（Binary Cross-Entropy Loss for Discriminator）**：\n",
    "$$\\min_\\phi-\\mathcal{L}_D(\\phi)=-\\mathbb{E}_{(s,a)\\sim\\pi_\\theta}[\\log D_\\phi(s,a)]-\\mathbb{E}_{(s,a)\\sim\\pi_E}[\\log(1-D_\\phi(s,a))]$$\n",
    "\n",
    "> 本质上为 **二分类交叉熵损失 BCELoss（Binary Cross-Entropy Loss）**，依据**真实采样分布**计算与**预测分布**的差异：\n",
    "$$\\mathrm{BCE}(p,y)=-\\left(y\\cdot\\log(p)+(1-y)\\cdot\\log(1-p)\\right)$$\n",
    "\n",
    "> 有了 **判别器$D_\\phi$** 之后，**策略** 的目标就是其交互产生的轨迹能被 **判别器** 误认为专家轨迹，所以将 **判别器的输出** 作为 **奖励函数** 来训练 **模拟者的策略**：\n",
    "$$r(s,a)=-\\log D(s,a)$$\n",
    "\n",
    "- **在不断的对抗过程中**：训练初期，**判别器**能够轻易区分专家数据和策略数据，因而对专家样本输出接近 0、对策略样本输出接近 1；随着**策略**不断更新以获得更高奖励，它会逐渐生成更接近专家的数据分布，使得**判别器**的区分难度增大，输出趋于模糊；在理论上的收敛点（**纳什均衡**）时，**策略分布与专家分布**完全一致，**判别器**无法再区分二者，只能对所有样本给出约 0.5 的概率，从而体现出博弈的平衡。\n",
    "\n",
    "> 可见，**GAIL** 中的 **策略** 需要和环境进行交互，这一点和 **BC** 不同，**BC** 完全不需要和环境交互\n",
    "> **GAIL** 实质上是模仿了 **专家策略的占用度量**，即 **尽量使得** 在环境中 **策略的占用度量$\\rho_{\\pi}(s,a)$** 和 **专家策略的占用度量$\\rho_{E}(s,a)$** 一致：\n",
    "![GAIL的优化目标](Illustrations/GAIL的优化目标.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f664ac5ccf9ec8dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 18.3 代码实践（CartPole-v1）\n",
    "- 生成专家数据\n",
    "- **行为克隆** 算法实现\n",
    "- **生成式对抗模仿学习** 算法实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0bc7f3dd148e14a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入相关库："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b137cb5be58db81c"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# 基本库\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.smoothing import moving_average\n",
    "from utils.advantage import compute_advantage\n",
    "from utils.training import train_on_policy_agent\n",
    "# 神经网络\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "# Gymnasium 是一个用于开发和测试强化学习算法的工具库，为 OpenAI Gym 的更新版本（2021迁移开发）\n",
    "import gymnasium as gym"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T13:46:02.883386600Z",
     "start_time": "2025-08-18T13:46:02.868318600Z"
    }
   },
   "id": "a88b89b940a745ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 生成专家数据：\n",
    "> 通过 **PPO** 算法训练出一个表现良好的专家模型，再利用专家模型生成专家数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eba12fe8756579"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "class PPO:\n",
    "    \"\"\" PPO-Clip（离散动作） \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, epochs, eps, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.epochs = epochs   # 一条序列的数据用来训练 epochs 轮\n",
    "        self.eps = eps         # PPO-Clip 的范围参数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states_np = np.array(transition_dict['states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        states = torch.tensor(states_np, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states_np = np.array(transition_dict['next_states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        next_states = torch.tensor(next_states_np, dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        \n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)  # 时序差分目标\n",
    "        td_delta = td_target - self.critic(states)                                 # 时序差分误差\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
    "        \n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach()\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            \n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage  # Clip\n",
    "            \n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO 损失函数\n",
    "            critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:32:08.384592300Z",
     "start_time": "2025-08-18T12:32:08.281495500Z"
    }
   },
   "id": "1953dccc08298949"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment spec: EnvSpec(id='CartPole-v1', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, disable_env_checker=False, kwargs={}, namespace=None, name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv')\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)    # 设置 NumPy 的随机种子\n",
    "torch.manual_seed(0) # 设置 PyTorch CPU 随机种子\n",
    "torch.cuda.manual_seed_all(0) # 设置 PyTorch GPU 随机种子, 由于GPU并行性, 只能极大减小偏差\n",
    "\n",
    "env = gym.make('CartPole-v1')  # CartPole-v1 最大回合步数修改到了500步(v0为200)\n",
    "#env = env.unwrapped # 获取原始环境（绕过 TimeLimit 包装器）解除最大步数500限制\n",
    "env.reset(seed=0)   # 环境通常依赖于其他随机数生成器来初始化状态、进行探索(推荐位于以上随机之后)\n",
    "print(\"Environment spec:\", env.spec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:32:08.415589500Z",
     "start_time": "2025-08-18T12:32:08.291563500Z"
    }
   },
   "id": "eaff4af9013b45f0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "hidden_dim = 128\n",
    "\n",
    "lmbda = 0.95\n",
    "eps = 0.2\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-2\n",
    "gamma = 0.98\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device( \"cpu\")\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda,\n",
    "            epochs, eps, gamma, device)\n",
    "num_episodes = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:32:11.484486600Z",
     "start_time": "2025-08-18T12:32:08.312077Z"
    }
   },
   "id": "f3494c1482315205"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 50/50 [00:12<00:00,  4.08it/s, episode=50, return=269.300]\n",
      "Iteration 1: 100%|██████████| 50/50 [00:27<00:00,  1.80it/s, episode=100, return=397.900]\n",
      "Iteration 2: 100%|██████████| 50/50 [00:24<00:00,  2.07it/s, episode=150, return=177.000]\n",
      "Iteration 3: 100%|██████████| 50/50 [00:39<00:00,  1.26it/s, episode=200, return=500.000]\n",
      "Iteration 4: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, episode=250, return=500.000]\n",
      "Iteration 5: 100%|██████████| 50/50 [00:42<00:00,  1.19it/s, episode=300, return=500.000]\n",
      "Iteration 6: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, episode=350, return=500.000]\n",
      "Iteration 7: 100%|██████████| 50/50 [00:45<00:00,  1.10it/s, episode=400, return=500.000]\n",
      "Iteration 8: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, episode=450, return=500.000]\n",
      "Iteration 9: 100%|██████████| 50/50 [00:40<00:00,  1.23it/s, episode=500, return=500.000]\n"
     ]
    }
   ],
   "source": [
    "return_list = train_on_policy_agent(env, agent, num_episodes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:38:08.993383800Z",
     "start_time": "2025-08-18T12:32:11.486487600Z"
    }
   },
   "id": "40b0fbdd37465bdb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*采样 $n$ 条轨迹：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f70abce887ca1ebc"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def sample_expert_data(n_episode):\n",
    "    states = []\n",
    "    actions = []\n",
    "    for episode in range(n_episode):\n",
    "        episode_return = 0\n",
    "        state, info = env.reset()  # 初始环境状态可不固定，所经历的状态动作对仍具有指导意义\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):  # 任务失败或达到最大步数\n",
    "            action = agent.take_action(state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)  # Gymnasium返回值不一样\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "        print(episode_return)\n",
    "    return np.array(states), np.array(actions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T14:01:00.460943800Z",
     "start_time": "2025-08-18T14:01:00.445658800Z"
    }
   },
   "id": "99c76afdbd90093c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*只生成一条轨迹：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b304fe6e36b4678"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "n_episode = 1\n",
    "expert_s, expert_a = sample_expert_data(n_episode)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T14:01:03.906292700Z",
     "start_time": "2025-08-18T14:01:02.586455Z"
    }
   },
   "id": "b921a0bac3c2b54e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*再从中采样 $n$ 个数据对：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44c10e30991bd06"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.72294104e-01 -1.43354103e-01  1.28104910e-02  8.42272341e-02]\n",
      " [ 1.48016974e-01 -3.35696250e-01 -1.32817896e-02  3.15589637e-01]\n",
      " [ 3.12507570e-01  2.30062470e-01 -5.39715728e-03 -1.29989833e-01]\n",
      " [ 2.22945094e-01  2.30974346e-01 -1.72858331e-02 -1.50121838e-01]\n",
      " [ 2.64835984e-01  2.32678816e-01  1.98677983e-02 -1.87747002e-01]\n",
      " [ 1.27989084e-01 -1.57186851e-01  6.43744017e-04  3.89326453e-01]\n",
      " [ 2.11684808e-01 -1.49385318e-01  9.21558887e-02  2.18304589e-01]\n",
      " [ 1.75344989e-01 -1.43024594e-01 -1.91703420e-02  7.69324079e-02]\n",
      " [ 1.44222930e-01  2.32494533e-01 -4.75422805e-03 -1.83631346e-01]\n",
      " [ 2.28015080e-01  3.09144799e-02  9.77818370e-02  2.52831399e-01]\n",
      " [ 3.30983400e-01 -1.41496301e-01 -7.71247549e-03  4.32408191e-02]\n",
      " [ 3.72572511e-01 -3.39166164e-01 -5.57604879e-02  3.92826676e-01]\n",
      " [ 4.18744504e-01 -7.07926929e-01  1.12969112e-02  5.03783166e-01]\n",
      " [ 1.95612431e-01 -1.53549373e-01 -3.73520106e-02  3.09340477e-01]\n",
      " [ 1.83799192e-01  3.92020196e-02 -3.05673983e-02  6.87291548e-02]\n",
      " [ 2.40872949e-01 -1.43440351e-01  5.82501665e-02  8.64206403e-02]\n",
      " [ 1.95233330e-01  2.30854377e-01 -5.59087507e-02 -1.47706732e-01]\n",
      " [ 1.79120421e-01  2.33938962e-01 -2.62562558e-02 -2.15557128e-01]\n",
      " [ 9.85555723e-02  8.10458362e-01  2.60867700e-02 -8.98999393e-01]\n",
      " [ 4.93508220e-01 -9.04616773e-01 -4.64835241e-02  8.32308233e-01]\n",
      " [ 6.07369319e-02  2.38230333e-01  2.42919102e-02 -3.10275555e-01]\n",
      " [ 2.09810123e-01  2.30773941e-01 -5.56979096e-03 -1.45683631e-01]\n",
      " [ 1.61088184e-01  5.30942939e-02 -1.24837197e-02 -2.37768769e-01]\n",
      " [ 7.95701444e-02  5.22263832e-02  3.02265882e-02 -2.18728453e-01]\n",
      " [ 2.69937187e-01  6.10754788e-01  6.32477403e-02 -5.05852818e-01]\n",
      " [ 2.38004148e-01 -3.39346796e-01  5.99785782e-02  3.96897763e-01]\n",
      " [ 7.43978024e-02 -1.51252031e-01  1.82487741e-02  2.58439094e-01]\n",
      " [ 5.72589576e-01 -7.15712607e-01 -1.19610928e-01  6.80961311e-01]\n",
      " [ 1.05437927e-01 -3.46586078e-01 -3.21963802e-02  5.56168854e-01]\n",
      " [ 3.07287782e-01 -3.36675853e-01  3.83501244e-03  3.37192029e-01]\n",
      " [ 2.33428553e-01  3.67307812e-02 -2.86691822e-04  1.23202518e-01]\n",
      " [ 3.91430587e-01  9.90091503e-01  1.94043927e-02 -8.49643230e-01]\n",
      " [ 3.20818782e-01 -3.23009193e-01  7.44253993e-02  3.59857604e-02]\n",
      " [ 2.28712067e-01 -3.42964381e-01  5.63498028e-02  4.76555854e-01]\n",
      " [ 1.76299363e-01  4.21819150e-01 -5.25069796e-02 -3.48889470e-01]\n",
      " [ 1.17763072e-01 -3.35495323e-01  1.13942474e-03  3.11114997e-01]\n",
      " [ 2.07558095e-01  2.33858421e-01  9.21253264e-02 -2.14681238e-01]\n",
      " [ 1.98444784e-01 -3.45836401e-01 -3.89081687e-02  5.39760768e-01]\n",
      " [ 2.67838240e-01 -1.41272157e-01  3.05554848e-02  3.84712592e-02]\n",
      " [ 5.30795157e-02 -1.53339490e-01  8.65978841e-03  3.04454207e-01]\n",
      " [ 5.50869443e-02  2.39026740e-01  3.13608199e-02 -3.27925473e-01]\n",
      " [ 3.55380654e-01  4.06251252e-01  3.65552679e-02 -4.90689464e-03]\n",
      " [ 3.37719828e-01 -3.36821824e-01 -1.45218251e-02  3.40467483e-01]\n",
      " [ 4.81240243e-01  6.07073784e-01 -5.26158772e-02 -4.23963189e-01]\n",
      " [ 3.82951617e-01  4.10351790e-02 -7.02975243e-02  2.83481050e-02]\n",
      " [ 5.88402450e-02  4.30369556e-01  1.22647537e-02 -5.37133932e-01]\n",
      " [ 3.48366112e-01 -5.32314181e-01 -2.73545776e-02  6.41637564e-01]\n",
      " [ 3.23496163e-01  2.16587245e-01  3.67499404e-02  1.67364061e-01]\n",
      " [ 1.14764743e-01  1.00521719e+00  8.10678117e-03 -1.18336964e+00]\n",
      " [ 1.96673125e-01  2.30908528e-01  6.19090768e-03 -1.48655757e-01]\n",
      " [ 2.61603892e-01  4.16666210e-01  6.79548457e-02 -2.35355243e-01]\n",
      " [ 3.86053503e-01 -1.55095071e-01 -7.71842748e-02  3.44337255e-01]\n",
      " [ 4.84880134e-02  4.55244370e-02  4.06974889e-02 -7.08331540e-02]\n",
      " [ 3.10897678e-01  2.17673346e-01  3.66324149e-02  1.43390477e-01]\n",
      " [ 2.61122137e-01 -1.56959236e-01 -4.66849748e-03  3.84310931e-01]\n",
      " [ 2.30605841e-01  3.33361253e-02  8.34688023e-02  1.98924765e-01]\n",
      " [ 2.77902246e-01 -3.33656549e-01  3.61173227e-02  2.70900369e-01]\n",
      " [ 7.48231336e-02  4.24732976e-02  2.19800901e-02 -3.46710952e-03]\n",
      " [ 6.53447062e-02 -1.53541669e-01 -1.56553239e-02  3.08963329e-01]\n",
      " [ 7.58953989e-02  5.07006571e-02  2.55001243e-02 -1.85024738e-01]\n",
      " [ 2.71229088e-01 -1.39068097e-01  4.15353328e-02 -1.01757487e-02]\n",
      " [ 2.99399257e-01  2.30188474e-01  5.73108671e-03 -1.32771417e-01]\n",
      " [ 5.80704749e-01  4.23641115e-01 -1.16007358e-01 -3.91096234e-01]\n",
      " [ 3.84282559e-01  4.72162738e-02 -6.78591430e-02 -1.08317606e-01]\n",
      " [ 5.99205673e-01  4.30535674e-01 -1.28604561e-01 -5.44872463e-01]\n",
      " [ 1.99850410e-01  4.26730543e-01 -5.88628873e-02 -4.57490474e-01]\n",
      " [ 3.04003000e-01  4.25227851e-01  3.07565834e-03 -4.23640788e-01]\n",
      " [ 3.65789205e-01 -5.33454299e-01 -4.79039550e-02  6.67420864e-01]\n",
      " [ 2.38486528e-01 -3.41597050e-01  4.40749563e-02  4.46106344e-01]\n",
      " [ 3.86356056e-01  4.51833084e-02 -7.07149580e-02 -6.33848533e-02]\n",
      " [ 1.71779454e-01  2.25995421e-01 -5.16999103e-02 -4.03533392e-02]\n",
      " [ 6.00788713e-01  5.26012033e-02 -1.31726176e-01 -2.28738099e-01]\n",
      " [ 3.46118659e-02  4.07898314e-02  1.28785083e-02  3.36659327e-02]\n",
      " [ 2.98284829e-01  5.88156953e-02  3.60789709e-02 -3.64062279e-01]\n",
      " [ 2.12235272e-01  3.75484154e-02  8.78316984e-02  1.05581611e-01]\n",
      " [ 1.74569294e-01 -3.42560947e-01 -4.70522232e-03  4.66995656e-01]\n",
      " [ 1.63932607e-01 -1.42220929e-01 -1.36676170e-02  5.91948628e-02]\n",
      " [ 1.94792256e-01  4.10086997e-02 -3.79292443e-02  2.88617797e-02]\n",
      " [ 2.57357329e-01  3.72326933e-02 -1.49668474e-02  1.12145104e-01]\n",
      " [ 5.99032402e-01 -3.33347172e-01 -1.35907650e-01  2.66428053e-01]\n",
      " [ 3.47886443e-01 -5.15228093e-01  6.31394237e-02  2.65023112e-01]\n",
      " [ 1.18769862e-01  3.79014574e-02 -6.90332148e-03  9.73872021e-02]\n",
      " [ 2.72439808e-01  4.26214516e-01  2.57092491e-02 -4.45599318e-01]\n",
      " [ 5.07715225e-01 -7.10349500e-01 -5.76501451e-02  5.58331072e-01]\n",
      " [ 1.34869084e-01  8.09991062e-01 -1.55606121e-02 -8.88156652e-01]\n",
      " [ 3.15251142e-01  4.12252039e-01  3.95002253e-02 -1.37514234e-01]\n",
      " [ 8.13571662e-02  5.31177409e-02  3.37014906e-02 -2.38432556e-01]\n",
      " [ 2.13034779e-01  3.84017825e-02 -7.16912374e-02  8.65489244e-02]\n",
      " [ 2.85587251e-01  2.46986732e-01  8.71049520e-03 -5.03329337e-01]\n",
      " [ 2.06542090e-01  4.15793061e-02  1.01194344e-01  1.65121146e-02]\n",
      " [ 2.03359663e-01 -3.40968013e-01 -4.57797609e-02  4.32388574e-01]\n",
      " [ 2.60360777e-01  3.80683355e-02 -6.54248334e-03  9.36992988e-02]\n",
      " [ 1.21809952e-01  3.79642360e-02  7.11871951e-04  9.59993675e-02]\n",
      " [ 1.19473331e-01  5.48490770e-02  6.30522566e-03 -2.76477486e-01]\n",
      " [ 2.19300672e-01 -1.53560475e-01 -7.36783817e-02  3.10229659e-01]\n",
      " [ 4.93985042e-02  2.40040004e-01  3.92808244e-02 -3.50403160e-01]\n",
      " [ 2.66751379e-01  5.43427914e-02  3.58615555e-02 -2.65303552e-01]\n",
      " [ 1.92193598e-01 -3.43262017e-01 -2.96779983e-02  4.82727677e-01]\n",
      " [ 6.03530891e-02 -3.42773408e-01  2.52056215e-02  4.71766233e-01]\n",
      " [ 5.25843382e-01 -9.06408072e-01 -7.51232430e-02  8.73654962e-01]] [0 1 0 0 0 1 1 0 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0\n",
      " 1 0 1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 0 1 1 0 0 0 0 0 1 1 0 1 0 1 0\n",
      " 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "random_index = random.sample(range(expert_s.shape[0]), n_samples)\n",
    "expert_s = expert_s[random_index]\n",
    "expert_a = expert_a[random_index]\n",
    "print(expert_s,expert_a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T14:01:13.987642700Z",
     "start_time": "2025-08-18T14:01:13.971210300Z"
    }
   },
   "id": "3b57bc35bd990f18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 行为克隆的代码实践:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b506ea5b031e9190"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*行为克隆算法：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c800487c8a23c5fd"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class BehaviorClone:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, lr):\n",
    "        self.policy = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "\n",
    "    def learn(self, states, actions):\n",
    "        states = torch.tensor(states, dtype=torch.float).to(device)\n",
    "        actions = torch.tensor(actions).view(-1, 1).to(device)\n",
    "        log_probs = torch.log(self.policy(states).gather(1, actions))  # 提取当前策略网络对应动作的概率\n",
    "        bc_loss = torch.mean(-log_probs)  # 最大似然估计，最大化对应动作概率\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        bc_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(device)\n",
    "        probs = self.policy(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T14:01:19.419183Z",
     "start_time": "2025-08-18T14:01:19.405857600Z"
    }
   },
   "id": "5c8af3bab91ef4c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*一次克隆后，测试策略网络效果：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cfb5e053d48a26e"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def test_agent(agent, env, n_episode):\n",
    "    return_list = []\n",
    "    for episode in range(n_episode):\n",
    "        episode_return = 0\n",
    "        state, info = env.reset()  # 初始环境状态可不固定\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):  # 任务失败或达到最大步数\n",
    "            action = agent.take_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)  # Gymnasium返回值不一样\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "        return_list.append(episode_return)\n",
    "    return np.mean(return_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T14:01:39.600740700Z",
     "start_time": "2025-08-18T14:01:39.588907800Z"
    }
   },
   "id": "694127df6c51db7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*参数设置：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "588fcd71ea0d9e48"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "bc_agent = BehaviorClone(state_dim, hidden_dim, action_dim, lr)\n",
    "n_iterations = 1000\n",
    "batch_size = 128"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T14:01:41.108166100Z",
     "start_time": "2025-08-18T14:01:41.087284300Z"
    }
   },
   "id": "97623c3b0db0b927"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*训练：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a719a5f525f2546"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "进度条: 100%|██████████| 1000/1000 [38:43<00:00,  2.32s/it, return=303.140]\n"
     ]
    }
   ],
   "source": [
    "test_returns = []\n",
    "with tqdm(total=n_iterations, desc=\"进度条\") as pbar:\n",
    "    for i in range(n_iterations):\n",
    "        sample_indices = np.random.randint(low=0,\n",
    "                                           high=expert_s.shape[0],\n",
    "                                           size=batch_size)  # randint 是有放回采样\n",
    "        bc_agent.learn(expert_s[sample_indices], expert_a[sample_indices])\n",
    "        current_return = test_agent(bc_agent, env, 5)\n",
    "        test_returns.append(current_return)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            pbar.set_postfix({'return': '%.3f' % np.mean(test_returns[-10:])})\n",
    "        pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T14:40:26.368524500Z",
     "start_time": "2025-08-18T14:01:42.865613300Z"
    }
   },
   "id": "5fc613b32c9f7931"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*实际上，在**初始环境固定**的情况下，**BC** 更加难以学习到 **最优策略**，因为依旧**不完全**的信息数据，会导致学习容易发生**过拟合***\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "604f92daabb84c00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 生成式对抗模仿学习的代码实践:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d51670302e5b222b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*判别器模型：*\n",
    "> 两层的全连接网络\n",
    "> 模型输入为一个状态动作对，输出一个概率标量"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d365eee44671add5"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        cat = torch.cat([x, a], dim=1)\n",
    "        x = F.relu(self.fc1(cat))\n",
    "        return torch.sigmoid(self.fc2(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T15:16:58.457780200Z",
     "start_time": "2025-08-18T15:16:58.438700800Z"
    }
   },
   "id": "fa65e57a3e09fb1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***生成式对抗模仿学习：***\n",
    "> 每一轮迭代中：\n",
    "> 1. **GAIL** 中的策略和环境交互，采样**新的状态动作对**\n",
    "> 2. 基于已有 **专家数据** 和 **策略新采样的数据** ，首先训练 **判别器**\n",
    "> 3. 然后将 **判别器** 的输出转换为 **策略的奖励信号**，指导策略用 **PPO** 算法做训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7444f00dfdca9088"
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "class GAIL:\n",
    "    def __init__(self, agent, state_dim, action_dim, hidden_dim, lr_d):\n",
    "        self.discriminator = Discriminator(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.discriminator_optimizer = torch.optim.Adam(self.discriminator.parameters(), lr=lr_d)\n",
    "        self.agent = agent\n",
    "\n",
    "    def learn(self, expert_s, expert_a, agent_s, agent_a, next_s, dones):\n",
    "        expert_states = torch.tensor(expert_s, dtype=torch.float).to(device)\n",
    "        expert_actions = torch.tensor(expert_a).to(device)\n",
    "        agent_states = torch.tensor(np.array(agent_s), dtype=torch.float).to(device)\n",
    "        agent_actions = torch.tensor(agent_a).to(device)\n",
    "        # 独热编码\n",
    "        expert_actions = F.one_hot(expert_actions, num_classes=2).float()\n",
    "        agent_actions = F.one_hot(agent_actions, num_classes=2).float()\n",
    "        # 判断器更新\n",
    "        expert_prob = self.discriminator(expert_states, expert_actions)\n",
    "        agent_prob = self.discriminator(agent_states, agent_actions)\n",
    "        discriminator_loss = nn.BCELoss()(\n",
    "            agent_prob, torch.ones_like(agent_prob)) + nn.BCELoss()(\n",
    "                expert_prob, torch.zeros_like(expert_prob))  # 判别器的二分类交叉熵损失\n",
    "        self.discriminator_optimizer.zero_grad()\n",
    "        discriminator_loss.backward()\n",
    "        self.discriminator_optimizer.step()\n",
    "        # 策略更新\n",
    "        rewards = -torch.log(agent_prob).detach().cpu().numpy()  # 不使用环境反馈的奖励\n",
    "        transition_dict = {\n",
    "            'states': agent_s,\n",
    "            'actions': agent_a,\n",
    "            'rewards': rewards,\n",
    "            'next_states': next_s,\n",
    "            'dones': dones\n",
    "        }\n",
    "        self.agent.update(transition_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T15:17:00.816334100Z",
     "start_time": "2025-08-18T15:17:00.802823200Z"
    }
   },
   "id": "2b579e8de2b704e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*参数设置：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41a87971fcc94414"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "agent_gail = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda,\n",
    "            epochs, eps, gamma, device)\n",
    "lr_d = 1e-3\n",
    "gail = GAIL(agent_gail, state_dim, action_dim, hidden_dim, lr_d)\n",
    "n_episode = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T15:17:04.095252300Z",
     "start_time": "2025-08-18T15:17:04.077726600Z"
    }
   },
   "id": "6292b27e2a90d0b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*训练：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5faf1a8862d5b502"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "进度条: 100%|██████████| 500/500 [06:22<00:00,  1.31it/s, return=500.000]\n"
     ]
    }
   ],
   "source": [
    "return_list = []\n",
    "with tqdm(total=n_episode, desc=\"进度条\") as pbar:\n",
    "    for i in range(n_episode):\n",
    "        episode_return = 0\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        next_state_list = []\n",
    "        done_list = []\n",
    "        while not (done or truncated):  # 任务失败或达到最大步数\n",
    "            action = agent_gail.take_action(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)  # Gymnasium返回值不一样\n",
    "            state_list.append(state)\n",
    "            action_list.append(action)\n",
    "            next_state_list.append(next_state)\n",
    "            done_list.append(done)\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "        return_list.append(episode_return)\n",
    "        gail.learn(expert_s, expert_a, state_list, action_list, next_state_list, done_list)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            pbar.set_postfix({'return': '%.3f' % np.mean(return_list[-10:])})\n",
    "        pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T15:23:28.488547Z",
     "start_time": "2025-08-18T15:17:06.319904Z"
    }
   },
   "id": "e753e5c7a577b4c7"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "episodes_list_bc = list(range(len(test_returns)))\n",
    "mv1_return = moving_average(test_returns, 9)\n",
    "episodes_list_gail = list(range(len(return_list)))\n",
    "mv2_return = moving_average(return_list, 9)\n",
    "# 创建 DataFrame\n",
    "df1 = pd.DataFrame({'Episodes': episodes_list_bc, 'Returns': mv1_return})\n",
    "df2 = pd.DataFrame({'Episodes': episodes_list_gail, 'Returns': mv2_return})\n",
    "# 保存为 CSV 文件\n",
    "df1.to_csv('PPO_bc_CartPole-v1_mv_returns_data.csv', index=False)\n",
    "df2.to_csv('PPO_gail_CartPole-v1_mv_returns_data.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T16:23:42.160392Z",
     "start_time": "2025-08-18T16:23:41.479578800Z"
    }
   },
   "id": "7a91d303fd9caa44"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "df = pd.read_csv('PPO_bc_CartPole-v1_mv_returns_data.csv')  # 从 CSV 文件中读取数据\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['Episodes'], y=df['Returns'], mode='lines', name='Returns'))\n",
    "fig.update_layout(\n",
    "    title='BC on CartPole-v1',\n",
    "    xaxis_title='Episodes',\n",
    "    yaxis_title='Returns',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39e76be73e145e62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "df = pd.read_csv('PPO_gail_CartPole-v1_mv_returns_data.csv')  # 从 CSV 文件中读取数据\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['Episodes'], y=df['Returns'], mode='lines', name='Returns'))\n",
    "fig.update_layout(\n",
    "    title='GAIL on CartPole-v1',\n",
    "    xaxis_title='Episodes',\n",
    "    yaxis_title='Returns',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35b3a628d91be03e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*通过两个实验的对比，可以感受到，在 **数据样本有限** 的情况下，**BC** 不能学习到最优策略，但是 **GAIL** 在 **相同的专家数据** 下可以取得非常好的结果*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45bdceb09037ca30"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "96f1cc474b0cbae8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
