{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 18.模仿学习（Imitation Learning，简称IL）\n",
    "> 虽然 **强化学习** 不需要有监督学习中的标签数据，但它十分依赖 **奖励函数** 的设置。有时在 **奖励函数** 上做一些微小的改动，训练出来的策略就会有天差地别。而且在现实场景中， **奖励信号** 总是难以明确：\n",
    "- 例如，对于无人驾驶车辆智能体的规控，其观测是当前的环境感知恢复的 3D 局部环境，动作是车辆接下来数秒的具体路径规划，那么奖励是什么？如果只是规定正常行驶而不发生碰撞的奖励为+1，发生碰撞为-100，那么智能体学习的结果则很可能是找个地方停滞不前。**奖励函数** 往往需要精心设计和调试。\n",
    "> 那有什么方法可以跳过 **奖励函数** 的设计呢？\n",
    "\n",
    "> **模仿学习** 研究的便是这一类问题，在模仿学习的框架下，存在 **专家智能体**，其在某个环境下提供的一系列 **状态动作对**，被认为是 **最优行为策略**。**模仿者**的任务则是利用这些 **专家数据** 进行训练，**无须奖励信号** 就可以达到一个接近专家的策略。\n",
    "> 学术界关于 **模仿学习** 的传统核心方法分为 **3 类**：\n",
    "\n",
    "| 方法                                                            | 概念                                                                                           | 缺点                      | 适用任务                                 | 最早时间 | 论文链接                                                                                                     |\n",
    "|---------------------------------------------------------------|----------------------------------------------------------------------------------------------|-------------------------|--------------------------------------|------|----------------------------------------------------------------------------------------------------------|\n",
    "| **行为克隆（behavior cloning，BC）**                                 | 通过监督学习模仿专家的状态-动作对，直接复制专家的行为                                                                  | - 缺乏长期奖励优化<br> - 数据依赖性强 | 数据充足且专家行为完美的任务，如机器人抓取、路径规划等          | 1988 | [论文学习](https://papers.neurips.cc/paper_files/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf) |\n",
    "| **逆强化学习（inverse RL）**                                         | 不同于行为克隆仅仅是复制专家的动作，它的目标是从专家的行为中推断出奖励函数。通过推断奖励函数，智能体就可以基于奖励来优化自己的策略了                           | - 计算复杂度高<br> - 数据需求大    | 无法直接获取奖励信号或奖励函数难以设计的任务，如自主驾驶、复杂策略规划。 | 2000 | [论文学习](https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2022_2023/papers/NG_ICML_2000.pdf)              |\n",
    "| **生成式对抗模仿学习（generative adversarial imitation learning，GAIL）** | 结合生成对抗网络（GANs）将模仿学习视为一个对抗过程，其中一个生成器（模仿者）试图模仿专家的行为，另一个判别器（判别器）则尝试判断动作是否来自专家。最终，生成器学会如何模仿专家的策略 | - 训练不稳定<br> - 对样本质量敏感   | 没有明确奖励信号的复杂任务，如机器人运动、视频游戏AI等。        | 2016 | [论文学习](https://arxiv.org/abs/1606.03476)                                                                 |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b524eb201dc3aee7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 本节主要介绍 **行为克隆** 和 **生成式对抗模仿学习**\n",
    "> **逆强化学习** 有良好的学术贡献，但由于其计算复杂度较高，实际应用的价值较小"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a9f40f2bda35ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 18.1 行为克隆\n",
    "> **行为克隆（BC）** 直接使用 **监督学习** 方法：\n",
    "$$\\theta^*=\\arg\\min_\\theta\\mathbb{E}_{(s,a)\\sim B}[\\mathcal{L}(\\pi_\\theta(s),a)]$$\n",
    "- $B$是专家的数据集，$\\mathcal{L}$是对应监督学习框架下的损失函数\n",
    "- 若动作是离散的，该损失函数可以是最大似然估计得到的；若动作是连续的，该损失函数可以是均方误差函数\n",
    "> 但也造成了其存在很大的局限性：由于训练使用的 **专家数据** 难以囊括所有状态分布，因此 BC 只能在专家数据的状态分布下预测得比较准。而且，**强化学习** 面对的是一个序贯决策问题，，只要存在一点偏差，就有可能导致下一个遇到的状态是在专家数据中没有见过的。此时，由于没有在此状态（或者比较相近的状态）下训练过，策略可能就会随机选择一个动作，这会导致下一个状态 **进一步偏离** 专家策略遇到的的数据分布。最终，该策略在真实环境下不能得到比较好的效果，这被称为行为克隆的 **复合误差（compounding error）** 问题：\n",
    "![行为克隆带来的复合误差问题](Illustrations/行为克隆带来的复合误差问题.png)\n",
    "> 即使 **行为克隆** 有这些局限，但其实现十分简单，因此在很多实际场景下它都可以作为 **策略预训练** 的方法：使 **策略** 无须在 **最开始低效地** 通过和环境交互来探索较好的动作，而是通过模仿专家智能体的行为数据来 **快速达到较高水平**，为接下来的强化学习创造一个 **高起点**。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31feca806a33ccfd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 18.2 生成式对抗模仿学习\n",
    "> **生成式对抗模仿学习** 是2016年由斯坦福大学研究团队提出的 **基于生成式对抗网络** 的**模仿学习**，它诠释了**生成式对抗网络**的 **本质** 其实就是**模仿学习**\n",
    "> **GAIL** 算法中有一个 **判别器** 和一个 **策略**：\n",
    "- **策略** 就相当于是生成式对抗网络中的 **生成器（generator）**，给定一个状态，**策略** 会输出这个状态下应该采取的动作\n",
    "- **判别器（discriminator）$D_\\phi$** 将状态动作对$(s,a)$作为输入，输出一个 0 到 1 之间的实数，表示 **判别器** 认为该状态动作对来自 **智能体策略$\\theta$** 而非 **专家$E$** 的概率。**判别器** 的目标是尽量将专家数据的输出靠近 0，将模仿者策略的输出靠近 1，这样就可以 **将两组数据分辨开**。\n",
    "\n",
    "> 于是**判别器$D_\\phi$** 的目标是最大化对数似然：\n",
    "$$\\mathcal{L}_D(\\phi)=\\mathbb{E}_{(s,a)\\sim\\pi_\\theta}[\\log D_\\phi(s,a)]+\\mathbb{E}_{(s,a)\\sim\\pi_E}[\\log(1-D_\\phi(s,a))]$$\n",
    "\n",
    "> 实际训练时，取负，最小化损失函数（本质上为 **二分类交叉熵损失**，依据**真实采样分布**计算与**预测分布**的差异）：\n",
    "$$\\min_\\phi-\\mathcal{L}_D(\\phi)=-\\mathbb{E}_{(s,a)\\sim\\pi_\\theta}[\\log D_\\phi(s,a)]-\\mathbb{E}_{(s,a)\\sim\\pi_E}[\\log(1-D_\\phi(s,a))]$$\n",
    "\n",
    "> 有了 **判别器$D_\\phi$** 之后，**策略** 的目标就是其交互产生的轨迹能被 **判别器** 误认为专家轨迹，所以将 **判别器的输出** 作为 **奖励函数** 来训练 **模拟者的策略**：\n",
    "$$r(s,a)=-\\log D(s,a)$$\n",
    "\n",
    "- **在不断的对抗过程中**：训练初期，**判别器**能够轻易区分专家数据和策略数据，因而对专家样本输出接近 0、对策略样本输出接近 1；随着**策略**不断更新以获得更高奖励，它会逐渐生成更接近专家的数据分布，使得**判别器**的区分难度增大，输出趋于模糊；在理论上的收敛点（**纳什均衡**）时，**策略分布与专家分布**完全一致，**判别器**无法再区分二者，只能对所有样本给出约 0.5 的概率，从而体现出博弈的平衡。\n",
    "\n",
    "> 可见，**GAIL** 中的 **策略** 需要和环境进行交互，这一点和 **BC** 不同，**BC** 完全不需要和环境交互\n",
    "> **GAIL** 实质上是模仿了 **专家策略的占用度量**，即 **尽量使得** 在环境中 **策略的占用度量$\\rho_{\\pi}(s,a)$** 和 **专家策略的占用度量$\\rho_{E}(s,a)$** 一致：\n",
    "![GAIL的优化目标](Illustrations/GAIL的优化目标.png)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f664ac5ccf9ec8dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 18.3 代码实践（CartPole-v1）\n",
    "- 生成专家数据\n",
    "- **行为克隆** 算法实现\n",
    "- **生成式对抗模仿学习** 算法实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0bc7f3dd148e14a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入相关库："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b137cb5be58db81c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# 基本库\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from utils.smoothing import moving_average\n",
    "from utils.advantage import compute_advantage\n",
    "from utils.training import train_on_policy_agent\n",
    "# 神经网络\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Gymnasium 是一个用于开发和测试强化学习算法的工具库，为 OpenAI Gym 的更新版本（2021迁移开发）\n",
    "import gymnasium as gym"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:32:08.339078Z",
     "start_time": "2025-08-18T12:32:08.268280Z"
    }
   },
   "id": "a88b89b940a745ec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 生成专家数据：\n",
    "> 通过 **PPO** 算法训练出一个表现良好的专家模型，再利用专家模型生成专家数据"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eba12fe8756579"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "class PPO:\n",
    "    \"\"\" PPO-Clip（离散动作） \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, epochs, eps, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.epochs = epochs   # 一条序列的数据用来训练 epochs 轮\n",
    "        self.eps = eps         # PPO-Clip 的范围参数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states_np = np.array(transition_dict['states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        states = torch.tensor(states_np, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states_np = np.array(transition_dict['next_states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        next_states = torch.tensor(next_states_np, dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        \n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)  # 时序差分目标\n",
    "        td_delta = td_target - self.critic(states)                                 # 时序差分误差\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
    "        \n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach()\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            \n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage  # Clip\n",
    "            \n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO 损失函数\n",
    "            critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:32:08.384592300Z",
     "start_time": "2025-08-18T12:32:08.281495500Z"
    }
   },
   "id": "1953dccc08298949"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment spec: EnvSpec(id='CartPole-v1', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, disable_env_checker=False, kwargs={}, namespace=None, name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv')\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)    # 设置 NumPy 的随机种子\n",
    "torch.manual_seed(0) # 设置 PyTorch CPU 随机种子\n",
    "torch.cuda.manual_seed_all(0) # 设置 PyTorch GPU 随机种子, 由于GPU并行性, 只能极大减小偏差\n",
    "\n",
    "env = gym.make('CartPole-v1')  # CartPole-v1 最大回合步数修改到了500步(v0为200)\n",
    "#env = env.unwrapped # 获取原始环境（绕过 TimeLimit 包装器）解除最大步数500限制\n",
    "env.reset(seed=0)   # 环境通常依赖于其他随机数生成器来初始化状态、进行探索(推荐位于以上随机之后)\n",
    "print(\"Environment spec:\", env.spec)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:32:08.415589500Z",
     "start_time": "2025-08-18T12:32:08.291563500Z"
    }
   },
   "id": "eaff4af9013b45f0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "hidden_dim = 128\n",
    "\n",
    "lmbda = 0.95\n",
    "eps = 0.2\n",
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-2\n",
    "gamma = 0.98\n",
    "epochs = 10\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device( \"cpu\")\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda,\n",
    "            epochs, eps, gamma, device)\n",
    "num_episodes = 500"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:32:11.484486600Z",
     "start_time": "2025-08-18T12:32:08.312077Z"
    }
   },
   "id": "f3494c1482315205"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 50/50 [00:12<00:00,  4.08it/s, episode=50, return=269.300]\n",
      "Iteration 1: 100%|██████████| 50/50 [00:27<00:00,  1.80it/s, episode=100, return=397.900]\n",
      "Iteration 2: 100%|██████████| 50/50 [00:24<00:00,  2.07it/s, episode=150, return=177.000]\n",
      "Iteration 3: 100%|██████████| 50/50 [00:39<00:00,  1.26it/s, episode=200, return=500.000]\n",
      "Iteration 4: 100%|██████████| 50/50 [00:41<00:00,  1.21it/s, episode=250, return=500.000]\n",
      "Iteration 5: 100%|██████████| 50/50 [00:42<00:00,  1.19it/s, episode=300, return=500.000]\n",
      "Iteration 6: 100%|██████████| 50/50 [00:42<00:00,  1.17it/s, episode=350, return=500.000]\n",
      "Iteration 7: 100%|██████████| 50/50 [00:45<00:00,  1.10it/s, episode=400, return=500.000]\n",
      "Iteration 8: 100%|██████████| 50/50 [00:40<00:00,  1.22it/s, episode=450, return=500.000]\n",
      "Iteration 9: 100%|██████████| 50/50 [00:40<00:00,  1.23it/s, episode=500, return=500.000]\n"
     ]
    }
   ],
   "source": [
    "return_list = train_on_policy_agent(env, agent, num_episodes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:38:08.993383800Z",
     "start_time": "2025-08-18T12:32:11.486487600Z"
    }
   },
   "id": "40b0fbdd37465bdb"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def sample_expert_data(n_episode):\n",
    "    states = []\n",
    "    actions = []\n",
    "    for episode in range(n_episode):\n",
    "        episode_return = 0\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):  # 任务失败或达到最大步数\n",
    "            action = agent.take_action(state)\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)  # Gymnasium返回值不一样\n",
    "            state = next_state\n",
    "            episode_return += reward\n",
    "        print(episode_return)\n",
    "    return np.array(states), np.array(actions)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:38:08.994381800Z",
     "start_time": "2025-08-18T12:38:08.986930900Z"
    }
   },
   "id": "99c76afdbd90093c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*只生成一条轨迹：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b304fe6e36b4678"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500.0\n"
     ]
    }
   ],
   "source": [
    "n_episode = 1\n",
    "expert_s, expert_a = sample_expert_data(n_episode)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:42:56.923132400Z",
     "start_time": "2025-08-18T12:42:55.706248800Z"
    }
   },
   "id": "b921a0bac3c2b54e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*再从中采样30个数据对：*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44c10e30991bd06"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.39583883 -0.03567474  0.00170786  0.2194962 ]\n",
      " [ 0.5103478  -0.02527446  0.00406822 -0.00990284]\n",
      " [ 0.21728075  0.16475405  0.01060449 -0.19022518]\n",
      " [ 0.22987857 -0.02546909 -0.00151555 -0.00558947]\n",
      " [ 0.10275465 -0.02711774  0.01680802  0.03078401]\n",
      " [ 0.39381614  0.17072532 -0.04097706 -0.32216683]\n",
      " [-0.05393257 -0.41384828  0.00906736  0.5386299 ]\n",
      " [ 0.13334946  0.35180548 -0.05276134 -0.305301  ]\n",
      " [-0.04145392 -0.02577929 -0.02393424  0.00123776]\n",
      " [ 0.3124886   0.7391604   0.03887375 -0.8274716 ]\n",
      " [ 0.38361657 -0.02544561 -0.02262531 -0.00611614]\n",
      " [-0.128087    0.16444615  0.03517561 -0.18358117]\n",
      " [ 0.08470736 -0.03322641  0.05178037  0.1657412 ]\n",
      " [ 0.09634226 -0.2221612   0.02287248  0.3217922 ]\n",
      " [ 0.2244689   0.16452599 -0.01178189 -0.18518966]\n",
      " [ 0.54271394 -0.4165149  -0.03481344  0.5979482 ]\n",
      " [ 0.10556748  0.3626281   0.01229272 -0.54368544]\n",
      " [ 0.06340914  0.3436054  -0.03589171 -0.12411058]\n",
      " [ 0.03599897  0.3420276  -0.02769358 -0.08926207]\n",
      " [ 0.23825373  0.1646374  -0.01605117 -0.18765593]\n",
      " [ 0.20335445 -0.22310738  0.02434602  0.34267834]\n",
      " [ 0.09237733 -0.03026341  0.03089642  0.10021558]\n",
      " [ 0.20463224 -0.03617335 -0.03528329  0.23069929]\n",
      " [ 0.1009005  -0.04306512 -0.04848374  0.38303712]\n",
      " [ 0.13023011  0.15596719 -0.05283278  0.00357213]\n",
      " [ 0.5008984   0.15620859 -0.02003007 -0.00174471]\n",
      " [ 0.3324656  -0.21442549  0.00758364  0.1510879 ]\n",
      " [ 0.5250484  -0.4152323  -0.01143057  0.5692366 ]\n",
      " [ 0.20940048 -0.03348827 -0.01324535  0.17132296]\n",
      " [ 0.20146342  0.15844098 -0.0342637  -0.05097929]] [1 1 1 0 1 0 1 0 0 1 1 0 1 1 0 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "n_samples = 30\n",
    "random_index = random.sample(range(expert_s.shape[0]), n_samples)\n",
    "expert_s = expert_s[random_index]\n",
    "expert_a = expert_a[random_index]\n",
    "print(expert_s,expert_a)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-18T12:44:09.339501500Z",
     "start_time": "2025-08-18T12:44:09.324541800Z"
    }
   },
   "id": "3b57bc35bd990f18"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 行为克隆的代码实践:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b506ea5b031e9190"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5c8af3bab91ef4c5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 生成式对抗模仿学习的代码实践:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d51670302e5b222b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
