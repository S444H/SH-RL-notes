{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 19.模型预测控制（model predictive control，MPC）算法\n",
    "1. 基于值函数的方法 DQN、基于策略的方法 REINFORCE 以及两者结合的方法 Actor-Critic，都是 **无模型（model-free）** 的方法，没有建立一个 **环境模型** 来帮助智能体决策。\n",
    "2. 在 **深度强化学习** 领域下，**基于模型（model-based）** 的方法不仅依赖于与 **真实环境** 交互获取的奖励信号，还试图用 **神经网络** 学习一个 **环境模型**，通过这个模型来**预测**环境的动态和奖励来帮助**智能体**训练和决策。\n",
    "3. 利用 **环境模型** 帮助智能体训练和决策的方法有很多种，例如类似之前 [Dyna](../RL_Fundamentals/7.Dyna-Q算法.ipynb) 的思想：生成一些数据来加入策略训练中。\n",
    "> **模型预测控制（MPC）** 算法：思想源于控制理论中的 **最优控制（Optimal Control）** + **动态规划（Dynamic Programming）**，结合了系统动态建模、实时优化和反馈控制。\n",
    "> **MPC** 并不构建一个显式的策略，而是使用 **环境模型** 预测未来一段时间内环境系统的状态变化，从而选择 **当前步** 要采取的动作。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b352de6d7f01e168"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **MPC** 方法在 **每次采取动作时**，首先会生成一些 **候选动作序列**（生成候选动作序列的过程被称为 **打靶(shooting)**），然后根据历史数据学习得到的 **环境模型$\\hat{P}(s,a)$** 来确定每一条候选序列能得到多好的结果，最终选择结果最好的那条动作序列的 **第一个动作** 来执行：\n",
    "$$\\arg\\max_{a_{k:k+H}}\\sum_{t=k}^{k+H}r(s_t,a_t)\\mathrm{~s.t.~}s_{t+1}=\\hat{P}(s_t,a_t)$$\n",
    "- $H$为推演的长度\n",
    "- $\\arg\\max_{a_{k:k+H}}$表示从所有动作序列中选取累积奖励最大的序列"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "517b3f91fb9552d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 19.1 打靶法：交叉熵方法（cross entropy method，CEM）\n",
    "> **MPC** 方法中的一个关键是如何生成一些 **候选动作序列**。\n",
    "> 常见的 **随机打靶法（random shooting method）** 随机生成$N$条长度为$H$的动作序列，对于一些简单的环境，这个方法不但十分简单，而且效果还不错。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2372e6d5051a83bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*相比于 **随机打靶法**，**交叉熵方法** 通过维护一个 **带参数的分布**，能够利用之前采样到的比较好的结果 **更新分布中的参数**，**提高**分布中能获得较高累积奖励的动作序列的**概率**，在一定程度上减少采样到一些较差动作的概率，从而使得算法更加高效*：\n",
    "> 为了实现 **交叉熵方法**，**带参数的分布** 将采用 **截断正态分布（Truncated Normal Distribution）** ：在标准正态分布的基础上，对其定义域进行 **截断**，使得超出指定区间的部分被剪切掉，区间外的概率密度为零，但概率密度函数在指定的截断区间内被规范化积分依然为 1。由于截断了分布的尾部，**截断正态分布** 的分布范围会变得更加集中，从而做到了**提高**高奖励动作序列**概率**的效果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d03c6b5478f42f1"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mscipy\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mstats\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m truncnorm  \u001B[38;5;66;03m# 用于生成和操作截断正态分布（Truncated Normal Distribution）\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mCEM\u001B[39;00m:\n\u001B[32m      5\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, n_sequence, elite_ratio, fake_env, upper_bound,\n\u001B[32m      6\u001B[39m                  lower_bound):\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm  # 用于生成和操作截断正态分布（Truncated Normal Distribution）\n",
    "\n",
    "class CEM:\n",
    "    def __init__(self, n_sequence, elite_ratio, fake_env, upper_bound,\n",
    "                 lower_bound):\n",
    "        self.n_sequence = n_sequence\n",
    "        self.elite_ratio = elite_ratio\n",
    "        self.upper_bound = upper_bound\n",
    "        self.lower_bound = lower_bound\n",
    "        self.fake_env = fake_env\n",
    "\n",
    "    def optimize(self, state, init_mean, init_var):\n",
    "        mean, var = init_mean, init_var\n",
    "        X = truncnorm(-2, 2, loc=np.zeros_like(mean), scale=np.ones_like(var))\n",
    "        state = np.tile(state, (self.n_sequence, 1))\n",
    "\n",
    "        for _ in range(5):\n",
    "            lb_dist, ub_dist = mean - self.lower_bound, self.upper_bound - mean\n",
    "            constrained_var = np.minimum(\n",
    "                np.minimum(np.square(lb_dist / 2), np.square(ub_dist / 2)),\n",
    "                var)\n",
    "            # 生成动作序列\n",
    "            action_sequences = [X.rvs() for _ in range(self.n_sequence)\n",
    "                                ] * np.sqrt(constrained_var) + mean\n",
    "            # 计算每条动作序列的累积奖励\n",
    "            returns = self.fake_env.propagate(state, action_sequences)[:, 0]\n",
    "            # 选取累积奖励最高的若干条动作序列\n",
    "            elites = action_sequences[np.argsort(\n",
    "                returns)][-int(self.elite_ratio * self.n_sequence):]\n",
    "            new_mean = np.mean(elites, axis=0)\n",
    "            new_var = np.var(elites, axis=0)\n",
    "            # 更新动作序列分布\n",
    "            mean = 0.1 * mean + 0.9 * new_mean\n",
    "            var = 0.1 * var + 0.9 * new_var\n",
    "\n",
    "        return mean"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-21T04:30:32.368372200Z",
     "start_time": "2025-08-21T04:30:32.285656500Z"
    }
   },
   "id": "a3d7c690a2295ec7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
