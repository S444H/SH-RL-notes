{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 5.动态规划算法\n",
    "**动态规划（dynamic programming）** 的基本思想是将待求解问题分解成若干个子问题，先求解子问题并保存已解决的子问题的答案，在求解目标问题的过程中，需要这些子问题答案时就可以直接利用，避免重复计算。\n",
    "基于**动态规划**的强化学习算法主要有两种：\n",
    "- **策略迭代（policy iteration）** ，策略迭代中的**策略评估**使用贝尔曼期望方程来得到一个策略的状态价值函数\n",
    "- **价值迭代（value iteration）**，价值迭代直接使用贝尔曼最优方程来进行动态规划\n",
    "\n",
    "不同于**4.蒙特卡洛方法**和之后要介绍的**时序差分算法**，基于动态规划的这两种强化学习算法：\n",
    "- 要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程这样的一个白盒环境\n",
    "- 但是，现实中的白盒环境很少\n",
    "- 通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的\n",
    "\n",
    "本节通过两个环境（满足以上条件）来学习这两种算法：\n",
    "1. **悬崖漫步（Cliff Walking）环境** \n",
    "2. **冰湖（Frozen Lake）环境** "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "573334d69e896b0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入相关库"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "956379cd4a49d6cc"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import copy\n",
    "import gymnasium as gym\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:26.179737500Z",
     "start_time": "2025-05-24T18:34:26.069540500Z"
    }
   },
   "id": "89dc86e720933806"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 悬崖漫步环境:\n",
    "### Cliff Walking环境示意图（与代码定义不同）:\n",
    "![Cliff Walking环境示意图](images/悬崖漫步环境示意图.png)\n",
    "### 规则：\n",
    "- 智能体在每一个状态采取以下 4 种动作之一：上、下、左、右\n",
    "- 触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态\n",
    "- 环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态\n",
    "- 智能体每走一步的奖励是 −1，掉入悬崖的奖励是 −100。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62bc32dc252dfdbd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cliff Walking 环境代码定义："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c586ce7bde98718"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    \"\"\" 悬崖漫步环境\"\"\"\n",
    "    def __init__(self, ncol=12, nrow=4):\n",
    "        self.ncol = ncol  # 定义网格世界的列\n",
    "        self.nrow = nrow  # 定义网格世界的行\n",
    "        # 转移矩阵P[state][action] = [(p, next_state, reward, done)]包含下一个状态和奖励，p为执行某个动作后，转移到下一个状态的概率（本环境都为1）\n",
    "        self.P = self.createP()\n",
    "\n",
    "    def createP(self):\n",
    "        # 初始化状态转移矩阵P (48*4)\n",
    "        P = [[[] for j in range(4)] for i in range(self.nrow * self.ncol)]\n",
    "        # 4种动作, change[0]:上,change[1]:下, change[2]:左, change[3]:右。坐标系原点(0,0)\n",
    "        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]\n",
    "        for i in range(self.nrow):\n",
    "            for j in range(self.ncol):\n",
    "                for a in range(4):\n",
    "                    # 位置在悬崖或者目标状态,因为无法继续交互,任何动作奖励都为0\n",
    "                    if i == self.nrow - 1 and j > 0:\n",
    "                        P[i * self.ncol + j][a] = [(1, i * self.ncol + j, 0, True)]\n",
    "                        continue\n",
    "                    # 其他位置\n",
    "                    next_x = min(self.ncol - 1, max(0, j + change[a][0]))\n",
    "                    next_y = min(self.nrow - 1, max(0, i + change[a][1]))\n",
    "                    next_state = next_y * self.ncol + next_x\n",
    "                    reward = -1\n",
    "                    done = False\n",
    "                    # 下一个位置在悬崖或者终点\n",
    "                    if next_y == self.nrow - 1 and next_x > 0:\n",
    "                        done = True\n",
    "                        if next_x != self.ncol - 1:  # 下一个位置在悬崖\n",
    "                            reward = -100\n",
    "                    P[i * self.ncol + j][a] = [(1, next_state, reward, done)]\n",
    "        return P"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:26.212813400Z",
     "start_time": "2025-05-24T18:34:26.081579200Z"
    }
   },
   "id": "3cc30ae6aa2bd874"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转移概率: 1, 下一个状态: 0, 奖励: -1, 是否终止: False\n"
     ]
    }
   ],
   "source": [
    "env = CliffWalkingEnv()\n",
    "s = 0  # 当前状态\n",
    "a = 0  # 执行动作 0\n",
    "for res in env.P[s][a]:\n",
    "    p, next_state, reward, done = res\n",
    "    print(f\"转移概率: {p}, 下一个状态: {next_state}, 奖励: {reward}, 是否终止: {done}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:26.276418700Z",
     "start_time": "2025-05-24T18:34:26.090532100Z"
    }
   },
   "id": "5384d1d7804e24f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.1 策略迭代算法\n",
    "策略迭代由两部分组成：**策略评估（policy evaluation）** 和**策略提升（policy improvement）**\n",
    "**策略评估**：计算评估某个策略下的**价值函数**（动作<==>状态）\n",
    "**策略提升**：依据 **策略提升定理（policy improvement theorem）** 更新动作策略"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4ad7667d5a59006"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 策略提升定理\n",
    "由于贪心地在每一个状态选择动作价值最大的动作$\\pi^{\\prime}(s)$：\n",
    "$$\\pi^{\\prime}(s)=\\arg\\max_aQ^\\pi(s,a)=\\arg\\max_a\\{r(s,a)+\\gamma\\sum_{s^{\\prime}}P(s^{\\prime}|s,a)V^\\pi(s^{\\prime})\\}$$\n",
    "此时动作价值函数与状态价值函数无数值差别，即：\n",
    "$$Q^\\pi(s,\\pi^{\\prime}(s))\\geq V^\\pi(s)$$\n",
    "最终会使：\n",
    "$$V^{\\pi^{\\prime}}(s)\\geq V^\\pi(s)$$\n",
    "**证明：**\n",
    "$$\\begin{aligned}\n",
    "V^{\\pi}(s) & \\leq Q^{\\pi}(s,\\pi^{\\prime}(s)) \\\\\n",
    " & =\\mathbb{E}_{\\pi^{\\prime}}[R_t+\\gamma V^{\\pi}(S_{t+1})|S_t=s] \\\\\n",
    " & \\leq\\mathbb{E}_{\\pi^{\\prime}}[R_t+\\gamma Q^\\pi(S_{t+1},\\pi^{\\prime}(S_{t+1}))|S_t=s] \\\\\n",
    " & =\\mathbb{E}_{\\pi^{\\prime}}[R_t+\\gamma R_{t+1}+\\gamma^2V^\\pi(S_{t+2})|S_t=s] \\\\\n",
    " & \\leq\\mathbb{E}_{\\pi^{\\prime}}[R_t+\\gamma R_{t+1}+\\gamma^2R_{t+2}+\\gamma^3V^\\pi(S_{t+3})|S_t=s] \\\\\n",
    " & \\leq\\mathbb{E}_{\\pi^{\\prime}}[R_t+\\gamma R_{t+1}+\\gamma^2R_{t+2}+\\gamma^3R_{t+3}+\\cdots|S_t=s] \\\\\n",
    " & =V^{\\pi^{\\prime}}(s)\n",
    "\\end{aligned}$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d588402c04ab7321"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**总体过程：** 对当前的策略进行策略评估，得到其状态价值函数，然后根据该状态价值函数进行策略提升以得到一个更好的新策略，接着继续评估新策略、提升策略……直至最后收敛到最优策略：\n",
    "$$\\pi^0\\overset{\\text{策略评估}}{\\operatorname{\\operatorname{\\longrightarrow}}}V^{\\pi^0}\\overset{\\text{ 策略提升}}{\\operatorname{\\operatorname{\\longrightarrow}}}\\pi^1\\overset{\\text{策略评估}}{\\operatorname{\\operatorname{\\longrightarrow}}}V\\overset{\\pi^1}{\\operatorname{\\operatorname{\\longrightarrow}}}\\overset{\\text{策略提升}}{\\operatorname{\\operatorname{\\longrightarrow}}}\\pi^2\\overset{\\text{策略评估}}{\\operatorname{\\operatorname{\\longrightarrow}}}\\ldots\\overset{\\text{策略提升}}{\\operatorname{\\operatorname{\\longrightarrow}}}\\pi^*$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b7ca519c2807282"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**收敛性证明：**\n",
    "$$\\begin{aligned}\n",
    "V^{\\pi}(s) \n",
    " & =\\sum_{a\\in A}\\pi(a|s)\\left(r(s,a)+\\gamma\\sum_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)V^{\\pi}(s^{\\prime})\\right) \\quad (6) \n",
    "\\end{aligned}$$\n",
    "$$(1-\\gamma)\\sum_{t=0}^\\infty\\gamma^t=1$$\n",
    "$$V<R\\sum_{t=0}^\\infty\\gamma^t=R/(1-\\gamma)$$\n",
    "显然在有限马尔可夫决策过程中，V存在一个**上界**\n",
    "再根据策略提升定理，更新后的策略的价值函数满足**单调性**\n",
    "最后根据实数列的**单调有界准则**，该数列一定收敛，也即是策略迭代算法一定收敛\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "483ea2f0cc12248"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    \"\"\" 策略迭代算法 \"\"\"\n",
    "    def __init__(self, env, theta, gamma):\n",
    "        self.env = env\n",
    "        self.v = [0] * self.env.ncol * self.env.nrow  # 状态价值初始化价值为0\n",
    "        self.pi = [[0.25, 0.25, 0.25, 0.25]\n",
    "                   for i in range(self.env.ncol * self.env.nrow)]  # 初始化为均匀随机策略\n",
    "        self.theta = theta  # 策略评估收敛阈值\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "\n",
    "    def policy_evaluation(self):  # 策略评估\n",
    "        cnt = 1  # 计数器\n",
    "        while 1:\n",
    "            max_diff = 0\n",
    "            new_v = [0] * self.env.ncol * self.env.nrow\n",
    "            for s in range(self.env.ncol * self.env.nrow):\n",
    "                qsa_list = []  # 计算状态s下的所有Q值\n",
    "                for a in range(4):\n",
    "                    qsa = 0\n",
    "                    \n",
    "                    for res in self.env.P[s][a]:\n",
    "                        p, next_state, r, done = res\n",
    "                        qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))  # 源:第3节公式（5）------- +此加号用于后面的冰湖环境\n",
    "                        \n",
    "                    qsa_list.append(self.pi[s][a] * qsa)  # 初始四个动作均分为0.25概率\n",
    "                new_v[s] = sum(qsa_list)  # 存储当前状态新价值， 源:第3节公式（6）状态价值函数V和动作价值函数Q之间的关系\n",
    "                \n",
    "                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))\n",
    "            self.v = new_v\n",
    "            if max_diff < self.theta: break  # 如果一次评估中各状态价值更改幅度最大的都小于收敛阈值，则满足收敛条件,退出评估迭代\n",
    "            cnt += 1\n",
    "        print(\"策略评估进行%d轮后完成\" % cnt)\n",
    "\n",
    "    def policy_improvement(self):  # 策略提升（针对每个状态的动作策略）\n",
    "        for s in range(self.env.nrow * self.env.ncol):\n",
    "            qsa_list = []\n",
    "            for a in range(4):\n",
    "                qsa = 0\n",
    "                for res in self.env.P[s][a]:  # 本环境中仅1次循环\n",
    "                    p, next_state, r, done = res\n",
    "                    qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\n",
    "                qsa_list.append(qsa)\n",
    "            maxq = max(qsa_list)  # 4个值中选最大值\n",
    "            cntq = qsa_list.count(maxq)  # 计算有几个动作得到了最大的Q值\n",
    "            # 让这些动作均分概率\n",
    "            self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]\n",
    "        print(\"策略提升完成\")\n",
    "        return self.pi\n",
    "\n",
    "    def policy_iteration(self):  # 策略迭代\n",
    "        while 1:\n",
    "            self.policy_evaluation()\n",
    "            old_pi = copy.deepcopy(self.pi)  # 将 动作策略概率列表pi 进行深拷贝,方便接下来进行比较\n",
    "            new_pi = self.policy_improvement()\n",
    "            if old_pi == new_pi: break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:26.281701200Z",
     "start_time": "2025-05-24T18:34:26.105915Z"
    }
   },
   "id": "72f9744bd8890f9f"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def print_agent(agent, action_meaning, disaster=[], end=[]):\n",
    "    \"\"\" \n",
    "    打印策略在每个状态下的价值以及智能体会采取的动作\n",
    "    用^o<o表示等概率采取向左和向上两种动作，ooo>表示在当前状态只采取向右动作\n",
    "    \"\"\"\n",
    "    print(\"状态价值：\")\n",
    "    for i in range(agent.env.nrow):\n",
    "        for j in range(agent.env.ncol):\n",
    "            # 为了输出美观,保持输出6个字符\n",
    "            print('%6.6s' % ('%.3f' % agent.v[i * agent.env.ncol + j]), end=' ')\n",
    "        print()\n",
    "\n",
    "    print(\"策略：\")\n",
    "    for i in range(agent.env.nrow):\n",
    "        for j in range(agent.env.ncol):\n",
    "            # 一些特殊的状态,例如悬崖漫步中的悬崖\n",
    "            if (i * agent.env.ncol + j) in disaster:\n",
    "                print('****', end=' ')\n",
    "            elif (i * agent.env.ncol + j) in end:  # 目标状态\n",
    "                print('EEEE', end=' ')\n",
    "            else:\n",
    "                a = agent.pi[i * agent.env.ncol + j]\n",
    "                pi_str = ''\n",
    "                for k in range(len(action_meaning)):\n",
    "                    pi_str += action_meaning[k] if a[k] > 0 else 'o'\n",
    "                print(pi_str, end=' ')\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:26.298199700Z",
     "start_time": "2025-05-24T18:34:26.116433700Z"
    }
   },
   "id": "75753f1f03dcd838"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略评估进行60轮后完成\n",
      "策略提升完成\n",
      "策略评估进行72轮后完成\n",
      "策略提升完成\n",
      "策略评估进行44轮后完成\n",
      "策略提升完成\n",
      "策略评估进行12轮后完成\n",
      "策略提升完成\n",
      "策略评估进行1轮后完成\n",
      "策略提升完成\n",
      "状态价值：\n",
      "-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n",
      "-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n",
      "-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n",
      "-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n",
      "策略：\n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n",
      "^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n"
     ]
    }
   ],
   "source": [
    "env = CliffWalkingEnv()\n",
    "theta = 0.001\n",
    "gamma = 0.9\n",
    "\n",
    "agent = PolicyIteration(env, theta, gamma)\n",
    "agent.policy_iteration()\n",
    "\n",
    "action_meaning = ['^', 'v', '<', '>']\n",
    "print_agent(agent, action_meaning, list(range(37, 47)), [47])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:26.309711100Z",
     "start_time": "2025-05-24T18:34:26.123606300Z"
    }
   },
   "id": "24503c346607da5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到，智能体最终沿着悬崖边到达终点，为最优策略."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be7c83225d880b14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1.2 价值迭代算法\n",
    "**策略迭代**中的**策略评估**需要进行很多轮才能收敛得到某一策略的状态函数，这需要很大的计算量，尤其是在状态和动作空间比较大的情况下\n",
    "**价值迭代算法**可以被认为是一种策略评估只进行了一轮更新的**策略迭代算法**，此时，因为**每一轮**选择的是**最优动作**\n",
    "根据3.5，价值迭代算法利用的是**贝尔曼最优方程**：\n",
    "$$V^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)\\quad (7) $$\n",
    "$$Q^\\pi(s,a)=r(s,a)+\\gamma\\sum_{s^{\\prime}\\in S}P(s^{\\prime}|s,a)V^\\pi(s^{\\prime})\\quad (5)$$\n",
    "所以迭代更新公式为：\n",
    "$$V^{k+1}(s)=\\max_{a\\in\\mathcal{A}}\\{r(s,a)+\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)V^k(s^{\\prime})\\}$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "43e0735ac2248257"
  },
  {
   "cell_type": "markdown",
   "source": [
    "由于每一轮直接根据**状态价值函数V**选择动作\n",
    "该方法不存在显式的策略，只需维护一个V\n",
    "当V趋于**收敛**后，恢复出最优策略即可."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa1eed3df7cb4a4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**收敛性证明：**\n",
    "$$\\begin{aligned}\n",
    "\\|V^{k+1}-V^*\\|_\\infty & =\\max_{s\\in\\mathcal{S}}\\left|\\max_{a\\in\\mathcal{A}}\\{r(s,a)+\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)V^{k}(s^{\\prime})\\}-\\max_{a^{\\prime}\\in\\mathcal{A}}\\{r(s,a^{\\prime})+\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a^{\\prime})V^*(s^{\\prime})\\}\\right| \\\\\n",
    " & \\leq\\max_{s,a}\\left|r(s,a)+\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)V^{k}(s^{\\prime})-r(s,a)-\\gamma\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)V^*(s^{\\prime})\\right| \\\\\n",
    " & =\\gamma\\max_{s,a}|\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)(V^{k}(s^{\\prime})-V^*(s^{\\prime}))| \\\\\n",
    " & \\leq\\gamma\\max_{s,a}\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)\\max_{s^{\\prime}}|V^{k}(s^{\\prime})-V^*(s^{\\prime})| \\\\\n",
    " & =\\gamma\\|V^{k}-V^*\\|_{\\infty}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- $V^{*}$ 为最优价值函数，可在迭代过程中保持不变\n",
    "- 由于每个阶段仅选择最优动作，所以$max_{s,a}\\sum_{s^{\\prime}\\in\\mathcal{S}}P(s^{\\prime}|s,a)=1$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40c1d051c48b32db"
  },
  {
   "cell_type": "markdown",
   "source": [
    "所以有：\n",
    "$$\\|V^{k+1}-V^*\\|_\\infty\\leq\\gamma\\|V^k-V^*\\|_\\infty\\leq\\cdots\\leq\\gamma^{k+1}\\|V^0-V^*\\|_\\infty$$\n",
    "- $\\|V^0-V^*\\|_\\infty$为某个固定值\n",
    "- $\\lim_{k\\to\\infty}V^{k}=V^{*}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6274c6bbadb4e8da"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    \"\"\" 价值迭代算法 \"\"\"\n",
    "    def __init__(self, env, theta, gamma):\n",
    "        self.env = env\n",
    "        self.v = [0] * self.env.ncol * self.env.nrow  # 初始化价值为0\n",
    "        self.theta = theta  # 价值收敛阈值\n",
    "        self.gamma = gamma\n",
    "        # 价值迭代策略\n",
    "        self.pi = [None for i in range(self.env.ncol * self.env.nrow)]\n",
    "\n",
    "    def value_iteration(self):\n",
    "        cnt = 0\n",
    "        while 1:\n",
    "            max_diff = 0\n",
    "            new_v = [0] * self.env.ncol * self.env.nrow\n",
    "            for s in range(self.env.ncol * self.env.nrow):\n",
    "                qsa_list = []  # 计算状态s下的所有Q价值\n",
    "                for a in range(4):\n",
    "                    qsa = 0\n",
    "                    for res in self.env.P[s][a]:  # 本环境中仅1次循环\n",
    "                        p, next_state, r, done = res\n",
    "                        qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\n",
    "                    qsa_list.append(qsa)  # 这一行和下一行代码是价值迭代和策略迭代的主要区别\n",
    "                new_v[s] = max(qsa_list)\n",
    "                max_diff = max(max_diff, abs(new_v[s] - self.v[s]))\n",
    "            self.v = new_v\n",
    "            if max_diff < self.theta: break  # 如果一次评估中各状态价值更改幅度最大的都小于收敛阈值，则满足收敛条件,退出评估迭代\n",
    "            cnt += 1\n",
    "        print(\"价值迭代一共进行%d轮\" % cnt)\n",
    "        self.get_policy()\n",
    "\n",
    "    def get_policy(self):  # 根据价值函数导出一个贪婪策略\n",
    "        for s in range(self.env.nrow * self.env.ncol):\n",
    "            qsa_list = []\n",
    "            for a in range(4):\n",
    "                qsa = 0\n",
    "                for res in self.env.P[s][a]:\n",
    "                    p, next_state, r, done = res\n",
    "                    qsa += p * (r + self.gamma * self.v[next_state] * (1 - done))\n",
    "                qsa_list.append(qsa)\n",
    "            maxq = max(qsa_list)\n",
    "            cntq = qsa_list.count(maxq)  # 计算有几个动作得到了最大的Q值\n",
    "            # 让这些动作均分概率\n",
    "            self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:26.309711100Z",
     "start_time": "2025-05-24T18:34:26.159424700Z"
    }
   },
   "id": "bff09c952f8b5786"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价值迭代一共进行14轮\n",
      "状态价值：\n",
      "-7.712 -7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 \n",
      "-7.458 -7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 \n",
      "-7.176 -6.862 -6.513 -6.126 -5.695 -5.217 -4.686 -4.095 -3.439 -2.710 -1.900 -1.000 \n",
      "-7.458  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000 \n",
      "策略：\n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovo> ovoo \n",
      "ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ooo> ovoo \n",
      "^ooo **** **** **** **** **** **** **** **** **** **** EEEE \n"
     ]
    }
   ],
   "source": [
    "env = CliffWalkingEnv()\n",
    "theta = 0.001\n",
    "gamma = 0.9\n",
    "\n",
    "agent = ValueIteration(env, theta, gamma)\n",
    "agent.value_iteration()\n",
    "\n",
    "action_meaning = ['^', 'v', '<', '>']\n",
    "print_agent(agent, action_meaning, list(range(37, 47)), [47])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:26.310760200Z",
     "start_time": "2025-05-24T18:34:26.167409300Z"
    }
   },
   "id": "1d3e3750f03fbeb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到，解决同样的训练任务，价值迭代总共进行了数十轮，而策略迭代中的策略评估总共进行了数百轮，价值迭代中的循环次数远少于策略迭代."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47760dd11728e7b4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 冰湖环境:\n",
    "冰湖是 **OpenAI Gym 库** 中的一个环境。OpenAI Gym 库中包含了很多有名的环境，例如 Atari 和 MuJoCo，并且支持定制自己的环境\n",
    "在之后的章节中，会使用到更多来自 OpenAI Gym 库的环境  (自 2021 年以来一直维护 Gym 的团队已将所有未来开发转移到 Gymnasium):\n",
    "### gymnasium库下载:\n",
    "![gymnasium库下载](images/gymnasium库下载.png)\n",
    "### Frozen Lake环境示意图（与代码定义不同）:\n",
    "![Frozen Lake环境示意图](images/冰湖环境示意图.png)\n",
    "### 规则：\n",
    "- 智能体在每一个状态采取以下 4 种动作之一：上、下、左、右\n",
    "- 由于智能体在冰面行走，因此每次行走都有一定的概率滑行到附近的其它状态\n",
    "- 到达冰洞或目标状态时行走会提前结束\n",
    "- 每一步行走的奖励是 0，到达目标的奖励是 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f6bb9c4f14e05a23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 使用 Gymnasium 创建 FrozenLake-v0 环境："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5d9bf88f6c6f9f0"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "冰洞的索引: {11, 12, 5, 7}\n",
      "目标的索引: {15}\n",
      "[(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "[(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "[(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "[(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "# 创建环境\n",
    "# 使用 gymnasium 创建环境，FrozenLake-v1 是推荐使用的版本,通过图形界面显示环境，适合人类用户观察\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "\n",
    "# 获取原始环境（绕过 TimeLimit 包装器）\n",
    "env = env.unwrapped\n",
    "# 渲染环境\n",
    "env.reset()\n",
    "# 显示初始环境\n",
    "env.render()\n",
    "time.sleep(2)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# 查找冰洞和目标\n",
    "holes = set()\n",
    "ends = set()\n",
    "for s in env.P:  # 16个状态\n",
    "    for a in env.P[s]:  # 4个动作/状态\n",
    "        for s_ in env.P[s][a]:  # s_ 是一个元组，包含 [prob, next_state, reward, done]\n",
    "            if s_[2] == 1.0:  # 获得奖励为1，代表目标\n",
    "                ends.add(s_[1])\n",
    "            if s_[3] == True:  # 该位置是冰洞\n",
    "                holes.add(s_[1])\n",
    "holes = holes - ends\n",
    "\n",
    "# 打印冰洞和目标的索引\n",
    "print(\"冰洞的索引:\", holes)\n",
    "print(\"目标的索引:\", ends)\n",
    "\n",
    "# 查看目标左边一格的状态转移信息\n",
    "for a in env.P[14]:  # 查看目标左边一格的状态转移信息\n",
    "    print(env.P[14][a])\n",
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 关闭环境\n",
    "env.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:29.031900400Z",
     "start_time": "2025-05-24T18:34:26.178231700Z"
    }
   },
   "id": "e453a979397632c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 策略迭代算法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "262ae59fdd95d067"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略评估进行25轮后完成\n",
      "策略提升完成\n",
      "策略评估进行58轮后完成\n",
      "策略提升完成\n",
      "状态价值：\n",
      " 0.069  0.061  0.074  0.056 \n",
      " 0.092  0.000  0.112  0.000 \n",
      " 0.145  0.247  0.300  0.000 \n",
      " 0.000  0.380  0.639  0.000 \n",
      "策略：\n",
      "<ooo ooo^ <ooo ooo^ \n",
      "<ooo **** <o>o **** \n",
      "ooo^ ovoo <ooo **** \n",
      "**** oo>o ovoo EEEE \n"
     ]
    }
   ],
   "source": [
    "theta = 1e-5\n",
    "gamma = 0.9\n",
    "\n",
    "agent = PolicyIteration(env, theta, gamma)\n",
    "agent.policy_iteration()\n",
    "\n",
    "action_meaning = ['<', 'v', '>', '^']\n",
    "print_agent(agent, action_meaning, [5, 7, 11, 12], [15])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:29.051673500Z",
     "start_time": "2025-05-24T18:34:29.025353Z"
    }
   },
   "id": "d0ad6c364150a45a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**为何反直觉？**\n",
    "因为智能体会随机滑向其他状态的冰冻湖面\n",
    "例如，在目标左边一格的状态，采取向右的动作时，它有可能会滑到目标左上角的位置，从该位置再次到达目标会更加困难，所以此时采取向下的动作是更为保险的."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13b6653cadfa7f4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 价值迭代算法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f460f4c8851aa821"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "价值迭代一共进行60轮\n",
      "状态价值：\n",
      " 0.069  0.061  0.074  0.056 \n",
      " 0.092  0.000  0.112  0.000 \n",
      " 0.145  0.247  0.300  0.000 \n",
      " 0.000  0.380  0.639  0.000 \n",
      "策略：\n",
      "<ooo ooo^ <ooo ooo^ \n",
      "<ooo **** <o>o **** \n",
      "ooo^ ovoo <ooo **** \n",
      "**** oo>o ovoo EEEE \n"
     ]
    }
   ],
   "source": [
    "theta = 1e-5\n",
    "gamma = 0.9\n",
    "\n",
    "agent = ValueIteration(env, theta, gamma)\n",
    "agent.value_iteration()\n",
    "\n",
    "action_meaning = ['<', 'v', '>', '^']\n",
    "print_agent(agent, action_meaning, [5, 7, 11, 12], [15])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-24T18:34:29.052683700Z",
     "start_time": "2025-05-24T18:34:29.039951100Z"
    }
   },
   "id": "cff16e3c991f050a"
  },
  {
   "cell_type": "markdown",
   "source": [
    " **结果一致，互相验证.**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c10442fccb312aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
