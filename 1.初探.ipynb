{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1.初探\n",
    "1. 什么是强化学习？\n",
    "2. 占用度量与状态访问分布\n",
    "3. 探索与利用"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3ee3f50b98f71a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 什么是强化学习？\n",
    "**强化学习**是机器通过**与环境交互**来实现目标的一种计算方法。\n",
    "机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，这个环境发生相应的改变并且将相应的奖励反馈和下一轮状态传回机器。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的**累积奖励的期望**。\n",
    "**强化学习** 用 **智能体（agent）** 这个概念来表示做决策的机器。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95111a5487a75e6e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 占用度量与状态访问分布:\n",
    "### 占用度量 (Occupancy Measure):\n",
    "在强化学习中，占用度量 $\\rho_{\\pi}(s,a)$ 定义为在策略 $\\pi$ 下，状态 $s$ 和动作 $a$ 的联合分布。其数学表达式为：\n",
    "\n",
    "$$\n",
    "\\rho_{\\pi}(s,a) = \\sum_{i=0}^{\\infty} \\gamma^i \\cdot P(s_t = s, a_t = a | \\pi)\n",
    "$$\n",
    "\n",
    "- 在策略 $\\pi$ 下，智能体长期运行中处于状态 $s$ 并选择动作 $a$ 的\"密度\",是一个时间加权和，较早的时间步权重更大.\n",
    "\n",
    "或者写为：\n",
    "$$\\rho^\\pi(s,a)=(1-\\gamma)\\sum_{t=0}^\\infty\\gamma^tP_t^\\pi(s)\\pi(a|s)$$\n",
    "$$(1-\\gamma)\\sum_{t=0}^\\infty\\gamma^t=1$$\n",
    "引入$(1-\\gamma)$,\n",
    "1. 可以防止无穷时间步的加权和可能“爆炸”\n",
    "2. 将无限步折扣序列变成一个合法的概率分布，让整个折扣加权和变成一个**期望意义上**的访问概率分布，使得状态和动作的“频率”可以被解释为“长期平均概率”"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94dbe75239569f43"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 符号解释:\n",
    "| 符号                           | 含义                     | 说明                                             |\n",
    "|------------------------------|------------------------|------------------------------------------------|\n",
    "| $\\rho_{\\pi}(s,a)$            | 占用度量                   | 在策略 $\\pi$ 下，状态 $s$ 和动作 $a$ 的联合分布               |\n",
    "| $s$                          | 状态 (state)             | 表示环境的当前情况                                      |\n",
    "| $a$                          | 动作 (action)            | 智能体在当前状态下采取的行为                                 |\n",
    "| $\\pi$                        | 策略 (policy)            | 从状态到动作的映射，$\\pi(a \\| s)$ 是在状态 $s$ 下选择动作 $a$ 的概率 |\n",
    "| $t$                          | 时间步 (time step)        | 智能体与环境交互的阶段                                    |\n",
    "| $\\gamma$                     | 折扣因子 (discount factor) | 取值范围 $[0,1]$，权衡当前奖励和未来奖励的重要性                   |\n",
    "| $P(s_t = s, a_t = a \\| \\pi)$ | 状态-动作对概率               | 在策略 $\\pi$ 下，时间步 $t$ 时状态为 $s$ 且动作为 $a$ 的概率      |"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dd1083df8840c89"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 与奖励函数的关系:\n",
    "期望回报 $J(\\pi)$ 可表示为：\n",
    "\n",
    "$$\n",
    "J(\\pi) = \\sum_{s,a} \\rho_{\\pi}(s,a) \\cdot R(s,a)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $R(s,a)$ 是奖励函数\n",
    "- $\\rho_{\\pi}(s,a)$ 是占用度量"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85b631ae5a08445"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 抽象理解:\n",
    "强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变\n",
    "因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的\n",
    "而占用度量可以从整体上刻画了策略的行为习惯，不是某一刻的动作选择\n",
    "它像一张动态地图，记录了智能体在环境中“留下脚印”的密集程度，通过最大化这些脚印在“高价值区域”的密度，可找到‘最优策略’\n",
    "**是理解策略行为、进行优化、评估模仿的一个核心工具**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d915d2be5d049e54"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 状态访问分布（state visitation distribution）:\n",
    "不同策略会使智能体访问到不同概率分布的状态+动作：占用度量\n",
    "不同策略会使智能体访问到不同概率分布的状态：**状态访问分布**，表示在策略 π 下，长期来看“处于状态 s ”的折扣加权频率\n",
    "$$\\nu^\\pi(s)=(1-\\gamma)\\sum_{t=0}^\\infty\\gamma^tP_t^\\pi(s)$$\n",
    "两者关系：\n",
    "$$\\rho^\\pi(s,a)=\\nu^\\pi(s)\\pi(a|s)$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7408a81d51e10517"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 探索与利用：\n",
    "**强化学习** 中的过程本质上就是一个 **不断试错** 的过程，智能体通过与环境交互，尝试不同的行为，并根据反馈调整自己的策略，以期获得更高的长期回报。这种试错探索的过程通常称为 **探索与利用** 的平衡。\n",
    "- 探索（Exploration）：智能体会尝试不同的动作，以便了解环境和不同动作的效果。这通常会带来不确定性和随机性，因为智能体需要在不完全了解的情况下去探索可能的最佳策略。\n",
    "- 利用（Exploitation）：一旦智能体对某些动作的效果有了较为清晰的理解，它就会选择那些被认为能最大化回报的动作，从而最大化当前的表现。\n",
    "\n",
    "在策略优化的过程中，智能体通过多次交互和评估逐步积累经验，并根据经验调整策略\n",
    "每一次与环境的交互都有可能是试错的，可能会导致暂时的回报减少，但也有助于长远的策略优化\n",
    "通过这一系列 **探索与试错** 的行为，去不断逼近最优策略，最大化长期回报\n",
    "这种过程本质上和人类通过试探、错误纠正的学习方式相似。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2b22638d6896902"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
