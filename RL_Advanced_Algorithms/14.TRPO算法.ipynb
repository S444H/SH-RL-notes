{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 14.TRPO （Trust Region Policy Optimization）：信任区域策略优化\n",
    "> 即使 **Actor-Critic** 在一定程度上能够降低训练过程中的方差，在实际应用过程中仍与传统策略梯度算法一样存在着 **训练不稳定** 的情况。根本原因在于策略更新过大：网络参数对样本传递的环境信息 **“过度拟合”**，导致训练时全局策略变差，造成训练的不稳定。特别是当面临情况复杂，策略网络是深度模型时，沿着策略梯度更新参数，步长太长，在一段时间内陷入对部分样本的过分信任，会导致全局策略突然显著变差。\n",
    "\n",
    "> **TRPO（Trust Region Policy Optimization）算法** 最初由 John Schulman 等人于2015年在 [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) 中提出。\n",
    "> 论文引入了 **信任区域（trust region）和KL散度约束** 的概念。在 **信任区域** 上更新策略时能够得到某种策略性能的安全性保证，是其主要思想。**TRPO** 描述了一种优化策略的迭代过程：在理论上能够保证策略学习的 **性能单调性**，并在实际应用中取得了比策略梯度算法更好的效果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14f3a280b466199d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "23d72c29cf3cc7cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
