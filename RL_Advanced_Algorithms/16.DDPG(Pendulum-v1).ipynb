{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 16.DDPG（deep deterministic policy gradient）：深度确定性策略梯度\n",
    "1. **在线策略算法** 的 **样本效率（sample efficiency）** 都比较低。 **DQN 算法** 虽然为 **离线策略学习**，但是它只能处理 **动作空间有限** 的环境，即使将动作空间离散化，但这会比较粗糙，无法精细控制。\n",
    "2. ***深度确定性策略梯度（deep deterministic policy gradient，DDPG）*** 结合了 **确定性策略梯度（Deterministic Policy Gradient, DPG）** 与 **DQN 算法** 的思想，通过构造一个 **确定性策略**，来处理 **动作空间无限** 的环境并且是 **离线策略算法**。\n",
    "3. **DDPG** 也属于一种 **Actor-Critic** 算法。不过之前学习的 REINFORCE、TRPO 和 PPO 学习 **随机性策略**，而 **DDPG** 则学习一个 **确定性策略**。\n",
    "> **DDPG** 最早由 **Google DeepMind** 于 2015 年在 [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971#) 中提出，主要用于解决传统 DQN **不能处理连续动作空间** 的问题"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "548bab84cb44a093"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 16.1 DDPG 算法原理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f370ce3b4b80d56"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 确定性策略梯度定理\n",
    "> 传统**策略梯度方法（PG）** ：\n",
    "$$J(\\theta)=\\mathbb{E}_{s\\sim\\nu^\\pi,a\\sim\\pi_\\theta}\n",
    "\\begin{bmatrix}\n",
    "R\n",
    "\\end{bmatrix}$$\n",
    "> 策略是 **随机性** 的，计算需要依赖于 **概率性动作分布：$a\\sim\\pi_{\\theta}(\\cdot|s)$**，这在面对 **连续动作空间** 时变得非常复杂\n",
    "> **确定性策略梯度定理（Deterministic Policy Gradient Theorem, DPG 定理）** 假设有一个 **确定性策略 $\\mu$**，**直接输出动作：$a=\\mu_\\theta(s)$**，而不是动作的概率分布：$a\\sim\\pi_{\\theta}(\\cdot|s)$\n",
    "> 此时：\n",
    "$$J(\\mu_\\theta)=\\int_{\\mathcal{S}}\\nu^{\\mu_\\theta}(s)r(s,\\mu_\\theta(s))\\mathrm{d}s=\\mathbb{E}_{s\\sim\\nu^{\\mu_\\theta}}[r(s,\\mu_\\theta(s))]$$\n",
    "> 可以避免计算**概率分布**的导数，简化梯度计算过程\n",
    "> **离线策略形式**：\n",
    "$$J(\\theta)=\\int_{\\mathcal{S}}\\nu^{\\pi_\\beta} V^{\\mu_\\theta}(s)\\mathrm{d}s=\\int_{\\mathcal{S}}\\nu^{\\pi_\\beta} Q^{\\mu_\\theta}(s,\\mu_\\theta(s))\\mathrm{d}s=\\mathbb{E}_{s\\sim\\nu^{\\pi_\\beta}}\n",
    "\\begin{bmatrix}\n",
    "Q^{\\mu_\\theta}(s,a)|_{a=\\mu_\\theta(s)}\n",
    "\\end{bmatrix}$$\n",
    "- $\\pi_{\\beta}$是用来收集数据的行为策略\n",
    "> 其 **梯度** 为（证明过程与 **策略梯度定理** 相似，详细证明过程可看原始论文[Deterministic Policy Gradient Algorithms](https://proceedings.mlr.press/v32/silver14.pdf)）：\n",
    "$$\\nabla_\\theta J(\\pi_\\theta)=\\mathbb{E}_{s\\sim\\nu^{\\pi_\\beta}}\n",
    "\\begin{bmatrix}\n",
    "\\nabla_\\theta\\mu_\\theta(s)\\nabla_aQ_\\omega^{\\mu_\\theta}(s,a)|_{a=\\mu_\\theta(s)}\n",
    "\\end{bmatrix}$$\n",
    "- 可见，$Q_\\omega$ 就是**Critic**，$\\mu_\\theta$ 就是**Actor**，这是一个 **Actor-Critic** 的框架"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc309812584c84c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DDPG 中的目标网络\n",
    "> **DDPG** 要用到4个神经网络，其中 **Actor** 和 **Critic** 各用一个网络，此外还有各自对应的： **目标网络**$\\mu_{\\theta^{\\prime}}$ 和 $Q_{\\omega^{\\prime}}$（为了 **稳定性**）\n",
    "> **DDPG** 中目标网络的更新与 **DQN** 中略有不同：在 **DQN** 中，**每隔一段时间**将网络**直接复制**给目标网络；而在 **DDPG** 中，目标网络的更新采取的是一种 **软更新（soft update）** 的方式，即让 **目标网络** 缓慢更新，逐渐接近 **主网络**：\n",
    "\n",
    "$$\\theta^{\\prime}\\leftarrow\\tau\\theta+(1-\\tau)\\theta^{\\prime}$$\n",
    "$$[\\omega^{\\prime}\\leftarrow\\tau\\omega+(1-\\tau)\\omega^{\\prime}]$$\n",
    "\n",
    "- $\\theta^{\\prime}$ 是 **目标网络** 的参数\n",
    "- $\\theta$ 是 **主网络** 的参数\n",
    "- $\\tau$是 **软更新** 的步长，一般设置为一个小的值，如 0.001\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f2c957243d894dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 其他细节\n",
    "1. 由于 Q函数 存在 **值过高估计** 的问题，**DDPG** 采用了 **Double DQN** 中的技术来更新网络\n",
    "2. 由于 **DDPG** 使用的是 **确定性策略**，即每个状态下都会输出一个确定的动作，这可能会导致模型在训练过程中过度依赖当前策略而陷入 **局部最优解**，缺乏多样性和探索。因此，引入 **随机噪声（Random Noise）$\\mathcal{N}$** 来鼓励探索（常用的噪声类型包括 **Ornstein-Uhlenbeck 噪声** 和 **高斯噪声**）：\n",
    "\n",
    "$$a_t=\\mu(s_t)+\\mathcal{N}_t$$\n",
    "> **DDPG** 最常用的噪声（原始论文）是 **Ornstein-Uhlenbeck 噪声（OU 噪声）** ，适用于连续控制任务，能够生成 **平滑的、具有时序相关性** 的噪声，适合连续动作空间中的探索：\n",
    "$$x_{t+1}=\\theta(\\mu-x_t)+\\sigma\\cdot\\mathcal{N}(0,1)$$\n",
    "\n",
    "- $\\mu$是噪声的均值 (通常为0)\n",
    "- $\\theta$是回归速率，控制噪声回归到均值的速度\n",
    "- $\\sigma$是噪声的强度，控制噪声的幅度\n",
    "- $\\mathcal{N}(0,1)$是标准正态分布噪声\n",
    "- 表现为向均值靠拢，适用于有惯性的系统\n",
    "\n",
    "> **高斯噪声** 适用于简单的任务，生成方式非常简单，但在连续空间中通常不如 **OU 噪声** 平滑：\n",
    "$$a_t=\\mu(s_t)+\\mathcal{N}(0,\\sigma)$$\n",
    "\n",
    "- $a_t$是动作，$\\mu(s_t)$是从 Actor 网络输出的动作，$\\mathcal{N}(0,\\sigma)$是从标准正态分布中生成的噪声"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e628c06a8ef0a8f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 16.2 DDPG 代码实践（Pendulum-v1）\n",
    "\n",
    "#### DDPG 算法伪代码：\n",
    "\n",
    "##### 初始化\n",
    "- 初始化 Actor 网络和 Critic 网络\n",
    "- 初始化目标网络 (Actor_target, Critic_target)\n",
    "- 初始化经验回放缓冲区\n",
    "\n",
    "##### 每个训练回合\n",
    "1. 初始化状态 `s`\n",
    "\n",
    "2. **每个时间步 `t`**：\n",
    "   - 使用 Actor 网络选择动作 `a`，并加上探索噪声：`a = actor(s) + 噪声`\n",
    "   - 执行动作 `a`，观察下一个状态 `s'` 和奖励 `r`\n",
    "   - 将经验 `(s, a, r, s')` 存储到经验回放缓冲区\n",
    "\n",
    "3. **从经验回放中采样一批数据**：\n",
    "   - 从回放缓冲区中随机采样：`(s_batch, a_batch, r_batch, s'_batch)`\n",
    "\n",
    "4. **更新 Critic 网络**：\n",
    "   - 计算目标 Q 值：`y = r_batch + gamma * critic_target(s'_batch, actor_target(s'_batch))`\n",
    "   - 最小化误差：`Q(s, a) - y`，更新 Critic 网络\n",
    "\n",
    "5. **更新 Actor 网络**：\n",
    "   - 计算策略梯度：`∇_a Q(s, a)` 的梯度\n",
    "   - 使用策略梯度更新 Actor 网络参数\n",
    "\n",
    "6. **软更新目标网络**：\n",
    "   - 使用软更新公式：`目标网络参数 = tau * 当前网络参数 + (1 - tau) * 目标网络参数`\n",
    "\n",
    "##### end\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7ccb5e2b19de641"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入相关库："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26e2eaaa5368f736"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'train_off_policy_agent' from 'utils.training' (D:\\GitHub\\SH-RL-notes\\RL_Advanced_Algorithms\\utils\\training.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mreplay_buffer\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ReplayBuffer\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mtraining\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m train_off_policy_agent\n\u001B[32m      6\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mutils\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01msmoothing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m moving_average\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# 神经网络\u001B[39;00m\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'train_off_policy_agent' from 'utils.training' (D:\\GitHub\\SH-RL-notes\\RL_Advanced_Algorithms\\utils\\training.py)"
     ]
    }
   ],
   "source": [
    "# 基本库\n",
    "import numpy as np\n",
    "\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "from utils.training import train_off_policy_agent\n",
    "from utils.smoothing import moving_average\n",
    "# 神经网络\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Gymnasium 是一个用于开发和测试强化学习算法的工具库，为 OpenAI Gym 的更新版本（2021迁移开发）\n",
    "import gymnasium as gym"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-09T09:43:09.655525800Z",
     "start_time": "2025-08-09T09:43:09.611506800Z"
    }
   },
   "id": "bdf835599ed40595"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义策略网络和价值网络:\n",
    "\n",
    "- 一层隐藏层\n",
    "- **策略网络** 的输出层用正切函数$(y=\\tanh x)$作为激活函数，值域是[-1,1]，方便按比例调整成环境可以接受的动作范围\n",
    "- **$Q$网络** 的输入是状态和动作拼接后的向量，**$Q$网络** 的输出是一个值，表示该状态动作对的价值。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "54afc16efaef12b4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.action_bound = action_bound  # action_bound是环境可以接受的动作最大值\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.tanh(self.fc2(x)) * self.action_bound\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        cat = torch.cat([x, a], dim=1) # 拼接状态和动作\n",
    "        x = F.relu(self.fc1(cat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc_out(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8689a135512a01c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义 DDPG 算法：\n",
    "> 使用 **高斯噪音**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c04671fddc58236f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device):\n",
    "        \n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n",
    "        self.critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_actor = PolicyNet(state_dim, hidden_dim, action_dim, action_bound).to(device)\n",
    "        self.target_critic = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        \n",
    "        # 初始化目标网络并设置和主网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.sigma = sigma  # 高斯噪声的标准差设为0\n",
    "        self.tau = tau  # 目标网络软更新参数\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        action = self.actor(state).item()\n",
    "        # 给动作添加噪声，增加探索\n",
    "        action = action + self.sigma * np.random.randn(self.action_dim)\n",
    "        return action\n",
    "\n",
    "    # 软更新\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        \n",
    "        states_np = np.array(transition_dict['states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        states = torch.tensor(states_np, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states_np = np.array(transition_dict['next_states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        next_states = torch.tensor(next_states_np, dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "\n",
    "        next_q_values = self.target_critic(next_states, self.target_actor(next_states))\n",
    "        q_targets = rewards + self.gamma * next_q_values * (1 - dones)\n",
    "        \n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states, actions), q_targets))\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        actor_loss = -torch.mean(self.critic(states, self.actor(states)))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.actor, self.target_actor)  # 软更新策略网络\n",
    "        self.soft_update(self.critic, self.target_critic)  # 软更新价值网络"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99a0b7e5499f2b83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 环境设置（'Pendulum-v1'）："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e2d1617e62bae6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(0)    # 设置 NumPy 的随机种子\n",
    "torch.manual_seed(0) # 设置 PyTorch CPU 随机种子\n",
    "torch.cuda.manual_seed_all(0) # 设置 PyTorch GPU 随机种子, 由于GPU并行性, 只能极大减小偏差\n",
    "\n",
    "env_name = 'Pendulum-v1'\n",
    "env = gym.make(env_name)\n",
    "env.reset(seed=0)   # 环境通常依赖于其他随机数生成器来初始化状态、进行探索(推荐位于以上随机之后)\n",
    "print(\"Environment spec:\", env.spec)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "316d5957b1e0e832"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 超参数设置："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8b78948b2e9a8c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]  # 连续动作空间\n",
    "action_bound = env.action_space.high[0]  # 动作最大值\n",
    "hidden_dim = 64\n",
    "\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "tau = 0.005  # 软更新参数\n",
    "gamma = 0.98\n",
    "sigma = 0.01  # 高斯噪声标准差\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device( \"cpu\")\n",
    "\n",
    "buffer_size = 10000\n",
    "minimal_size = 1000\n",
    "batch_size = 64\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "agent = DDPG(state_dim, hidden_dim, action_dim, action_bound, sigma, actor_lr, critic_lr, tau, gamma, device)\n",
    "\n",
    "num_episodes = 200"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9be8be1b0bd97946"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "return_list = train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b7deea2a9769176"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
