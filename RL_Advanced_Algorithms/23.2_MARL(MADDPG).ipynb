{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 23.2 MARL (MADDPG)\n",
    "> 前文已经介绍了 **多智能体强化学习的基本求解范式：完全中心化 与 完全去中心化**。本节来介绍一种比较经典且效果不错的**进阶范式：中心化训练去中心化执行（centralized training with decentralized execution，CTDE）**：\n",
    "> - **中心化训练**：在训练的时候使用一些单个智能体看不到的 **全局信息** 而以达到更好的训练效果。\n",
    "> - **去中心化执行**：在执行时不使用 **全局信息**，每个智能体完全根据自己的策略执行动作。\n",
    "\n",
    "**CTDE** 可以类比成一个足球队的训练和比赛过程：在训练时，11 个球员可以直接获得教练的指导从而完成球队的整体配合，而教练本身掌握着比赛全局信息，教练的指导也是从整支队、整场比赛的角度进行的；而训练好的 11 个球员在上场比赛时，则根据场上的实时情况直接做出决策，不再有教练的指导。\n",
    "> 所以 **CTDE** 能够在训练时有效地利用全局信息以达到 **更好且更稳定的训练效果**，同时在进行策略模型推断时可以仅利用局部信息，使得算法**具有一定的扩展性**。\n",
    "\n",
    "> **CTDE** 算法主要分为两种：\n",
    "> - 一种是基于值函数的方法，例如 **VDN，QMIX** \n",
    "> - 一种是基于 Actor-Critic 的方法，例如 **MADDPG，COMA** \n",
    "\n",
    "> 本节介绍 **MADDPG** 算法 （[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c96e6457b7d49d23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 23.2.1 MADDPG 算法原理\n",
    "- 学习本节内容之前，可以先复习之前的 [DDPG 算法](../RL_Classic_Algorithms/16.DDPG(Pendulum-v1).ipynb) ，**Actor**梯度为：\n",
    "$$\\nabla_\\theta J(\\mu_\\theta)=\\mathbb{E}_{s\\sim\\nu^{\\pi_\\beta}}\n",
    "\\begin{bmatrix}\n",
    "\\nabla_\\theta\\mu_\\theta(s)\\nabla_aQ_\\omega^{\\mu_\\theta}(s,a)|_{a=\\mu_\\theta(s)}\n",
    "\\end{bmatrix}$$\n",
    "> 与 **‘纯DDPG’** 不同，在 **MADDPG** 中 **所有智能体共享一个中心化的 Critic 网络**，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导；而执行时，**每个智能体的 Actor 网络** 则是完全独立做出行动，即去中心化地执行：\n",
    "\n",
    "> ![Overview_of_MADDPG_AC](./Illustrations/Overview_of_MADDPG_AC.png)\n",
    "\n",
    "考虑现在有$N$个连续的策略$\\mu_{\\theta_i}$，**Actor**梯度公式变为：\n",
    "$$\\nabla_{\\theta_i}J(\\mu_i)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[\\nabla_{\\theta_i}\\mu_i(o_i)\\nabla_{a_i}Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)|_{a_i=\\mu_i(o_i)}]$$\n",
    "- 对于每个智能体 $i$，其动作空间为$A_i$，观测空间为$O_i$\n",
    "- $\\mathbf{x}=(o_{1},\\ldots,o_{N})$ 包含了所有智能体的观测\n",
    "- $\\mathcal{D}$ 表示存储数据的经验回放池，它存储的每一个数据为 $(\\mathbf{x},\\mathbf{x}^{\\prime},a_1,\\ldots,a_N,r_1,\\ldots,r_N)$\n",
    "- 可见，有一个 **Critic** $Q_w$，$N$个 **Actor** $\\mu_\\theta^i$\n",
    "\n",
    "此时在 **MADDPG** 中，**中心化的动作价值网络$Q$** 更新的损失函数变为：\n",
    "$$\\mathcal{L}(\\omega_i)=\\mathbb{E}_{\\mathbf{x},a,r,\\mathbf{x}^{\\prime}}[(Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)-y)^2]$$\n",
    "$$y=r_i+\\gamma Q_i^{\\mu^{\\prime}}(\\mathbf{x}^{\\prime},a_1^{\\prime},\\ldots,a_N^{\\prime})|_{a_j^{\\prime}=\\mu_j^{\\prime}(o_j)}$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0e2be45d6cd422"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 23.2.2 MADDPG 代码实践"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818b9d2ea2d264c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入基本库"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b01965bfccf6a6"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-17T15:44:41.304150700Z",
     "start_time": "2025-09-17T15:44:41.217016200Z"
    }
   },
   "id": "a684da7d0c7b63a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 关于环境库\n",
    "同样需新建一个虚拟环境（防止软件包冲突），下载 **MPE2库**（[使用文档](https://mpe2.farama.org/environments/simple_adversary/)），使用其中的 **多智能体粒子环境（multiagent particles environment，MPE）**：\n",
    "(如果图片加载异常，请在浏览器中打开 Notebook)\n",
    "![mpe2_simple_adversary](./Illustrations/simple_adversary.gif)\n",
    "\n",
    "- 该环境中有 1 个红色的对抗智能体（adversary）、$N$ 个蓝色的正常智能体，以及 $N$ 个地点（一般 $N=2$）\n",
    "- 每个智能体的观察空间都为其他智能体的位置以及地点的位置\n",
    "- $N$ 个地点中有一个是目标地点（绿色），正常智能体知道哪一个是目标地点，但对抗智能体不知道\n",
    "- 正常智能体是合作关系：它们其中任意一个距离目标地点足够近，则每个正常智能体都能获得相同的奖励\n",
    "- 对抗智能体如果距离目标地点足够近，也能获得奖励，但它需要猜哪一个才是目标地点\n",
    "\n",
    "因此，性能好的正常智能体会趋于分散到不同的坐标点，以此欺骗对抗智能体。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdffb0c6d59d5eab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***测试：***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "178b8d59b10b413c"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from mpe2 import simple_adversary_v3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-17T15:44:41.304150700Z",
     "start_time": "2025-09-17T15:44:41.235630600Z"
    }
   },
   "id": "42a4f5c6b14d5aa0"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6535763  0.61153704 0.04455626 0.94084764 0.16397008]\n",
      "[0.68524176 0.2966745  0.31492484 0.54268736 0.19619809]\n",
      "[0.7138684  0.9722495  0.38129827 0.08445479 0.9143623 ]\n",
      "[0.64175266 0.7741596  0.71639365 0.127057   0.28875178]\n",
      "[0.9940063  0.24682511 0.75365716 0.6517841  0.52633333]\n",
      "[0.6624459  0.22186357 0.69565916 0.79886705 0.15701902]\n",
      "[0.05450406 0.37769225 0.5174158  0.9985724  0.9970568 ]\n",
      "[0.9264654  0.83103037 0.83835006 0.765284   0.36777616]\n",
      "[0.77770996 0.9939882  0.523521   0.64823985 0.21744789]\n",
      "[0.3734286  0.71590745 0.26866698 0.40528417 0.37566197]\n",
      "[0.26611984 0.5841139  0.10217889 0.8530907  0.6995098 ]\n",
      "[0.7703004  0.34502825 0.13438518 0.71954626 0.06097629]\n",
      "[0.50527596 0.5464655  0.96398354 0.8681917  0.73990345]\n",
      "[0.88088065 0.4061119  0.7330751  0.5334667  0.18965013]\n",
      "[0.50755125 0.66102797 0.5295199  0.7734291  0.48401356]\n",
      "[0.9069307  0.6686286  0.77100855 0.3754278  0.0499332 ]\n",
      "[0.38045818 0.878862   0.9640775  0.8532004  0.5001772 ]\n",
      "[0.30458593 0.96934813 0.09205842 0.8242491  0.18734011]\n",
      "[0.8347586  0.9288139  0.6861243  0.8567042  0.58143294]\n",
      "[0.73919857 0.53039205 0.80511165 0.34081045 0.8564158 ]\n",
      "[0.87822324 0.7703317  0.58766603 0.7022245  0.92260283]\n",
      "[0.21059062 0.6798494  0.9509394  0.04950696 0.134534  ]\n",
      "[0.55993664 0.6035905  0.4969339  0.82895285 0.8817835 ]\n",
      "[0.6874676  0.54833806 0.29724038 0.64328754 0.90245557]\n",
      "[0.04605446 0.6210682  0.50100577 0.6093857  0.3284171 ]\n",
      "[0.95191455 0.29181427 0.53892463 0.22199407 0.5933561 ]\n",
      "[0.98973024 0.11790718 0.00700798 0.33356583 0.56149006]\n",
      "[0.10434343 0.10334084 0.16113457 0.8077472  0.04408917]\n",
      "[0.03231256 0.501571   0.25639278 0.8607268  0.8712289 ]\n",
      "[0.08951637 0.925583   0.6915165  0.04445136 0.6549286 ]\n",
      "[0.9471194  0.59610873 0.5626778  0.3386464  0.41661063]\n",
      "[0.72119296 0.58980906 0.65471154 0.3627291  0.84764755]\n",
      "[0.08604326 0.38120484 0.03044576 0.8956392  0.87808377]\n",
      "[0.0786477  0.33633214 0.5962315  0.5960658  0.31232682]\n",
      "[0.1719347  0.9144174  0.2199383  0.7521524  0.48966196]\n",
      "[0.4256321  0.90104836 0.9748531  0.23788868 0.89185345]\n",
      "[0.8082082  0.6985741  0.9435901  0.46739686 0.5004192 ]\n",
      "[0.6796113  0.41032988 0.7224234  0.52290356 0.265189  ]\n",
      "[0.19206865 0.9103441  0.9516657  0.49297404 0.4096585 ]\n",
      "[0.18969053 0.8511925  0.11423628 0.13525185 0.37828207]\n",
      "[0.25427747 0.44810292 0.20816563 0.5603957  0.38173497]\n",
      "[0.76371264 0.14854938 0.06308445 0.96858025 0.65365505]\n",
      "[0.8236115  0.11050353 0.9115873  0.6911167  0.12446196]\n",
      "[0.22692245 0.7620262  0.10468631 0.3785253  0.48221254]\n",
      "[0.19048654 0.24905473 0.5972282  0.5569376  0.33342472]\n",
      "[0.26857564 0.6067487  0.13873492 0.300637   0.55508614]\n",
      "[0.93220615 0.5910916  0.6218855  0.74995786 0.23695965]\n",
      "[0.8689846  0.7020587  0.05164482 0.70302165 0.58449495]\n",
      "[0.40929472 0.8752557  0.2173968  0.28252953 0.52814955]\n",
      "[0.82316554 0.44259706 0.26110923 0.31092912 0.86742   ]\n",
      "[0.7113163  0.02985962 0.95704484 0.03658744 0.63910496]\n",
      "[0.41923448 0.83919114 0.38652912 0.6977121  0.41723627]\n",
      "[0.47268653 0.58201426 0.8219351  0.23509818 0.9937533 ]\n",
      "[0.34102327 0.3384363  0.37548956 0.53719145 0.48407954]\n",
      "[0.16610134 0.8105757  0.04519674 0.8426275  0.6274786 ]\n",
      "[0.82434124 0.32513237 0.832994   0.6282855  0.30319402]\n",
      "[0.14880611 0.41731966 0.7957989  0.49883232 0.31846634]\n",
      "[0.76651067 0.6605588  0.0984751  0.6573268  0.7652699 ]\n",
      "[0.07852221 0.07143591 0.3753159  0.7837536  0.9700723 ]\n",
      "[0.791217   0.99433666 0.88034976 0.23349582 0.44516638]\n",
      "[0.42984435 0.06089775 0.59782875 0.8758098  0.994464  ]\n",
      "[0.84356445 0.5328476  0.99781716 0.6312349  0.97417593]\n",
      "[0.11811497 0.9255407  0.29588073 0.26947555 0.74048376]\n",
      "[0.9891702  0.41781813 0.9970846  0.9637129  0.89934415]\n",
      "[0.45219174 0.92069143 0.8197084  0.2900855  0.7375552 ]\n",
      "[0.9560329  0.4935311  0.56939226 0.63441104 0.02921859]\n",
      "[0.97842747 0.49533576 0.9545177  0.4202296  0.8135215 ]\n",
      "[0.97825235 0.4424597  0.15683877 0.06268879 0.9619435 ]\n",
      "[0.8155624  0.66584474 0.24950151 0.15677312 0.07116331]\n",
      "[0.9015968  0.27133042 0.06881669 0.8899704  0.2351394 ]\n",
      "[0.59917736 0.30839524 0.22766723 0.64594287 0.927594  ]\n",
      "[0.03522316 0.86103415 0.6025626  0.3794236  0.23784907]\n",
      "[0.66077965 0.9475161  0.6342193  0.69136    0.81027925]\n",
      "[0.5027388  0.86338776 0.8069186  0.9243247  0.9232628 ]\n",
      "[0.67171234 0.50242907 0.8366858  0.6259849  0.37559178]\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.env(N=2, max_cycles=25, continuous_actions=True, dynamic_rescaling=True)\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "        print(action)\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-17T15:44:41.695775200Z",
     "start_time": "2025-09-17T15:44:41.249868500Z"
    }
   },
   "id": "9dd2f61a04bb0a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 采样动作不可导问题：离散采样操作不可导，这会导致无法使用反向传播来优化模型（[复习SAC中使用的重参数化技巧（reparameterization trick）](../RL_Classic_Algorithms/17.1_SAC算法原理.ipynb)）\n",
    "> **DDPG** 算法通过 **确定性策略** 针对 **连续动作空间**，使智能体的动作对于其 **策略参数$\\mu_\\theta$** 可导\n",
    "> 但 **MPE** 环境中 **默认** 每个智能体的 **动作空间是离散的**，虽然依旧是 **确定性策略**，但是需要对网络输出进行 **离散采样**，这种采样是 **不可导的**，此时就需要运用之前的 **重参数化方法（这里要用的是 Gumbel-Softmax 技巧）** 让离散分布的采样可导：\n",
    "\n",
    "**Gumbel-Softmax 变换：**\n",
    "1. Gumbel 随机变量的采样（重参数因子$g_i$）\n",
    "$$g_i=-\\log(-\\log u),u\\sim\\mathrm{Uniform}(0,1)$$\n",
    "2. 加入 Gumbel 噪声并进行 softmax\n",
    "$$\\tilde{y}_i=\\frac{\\exp\\left(\\frac{\\log(\\pi_i)+g_i}{\\tau}\\right)}{\\sum_j\\exp\\left(\\frac{\\log(\\pi_j)+g_j}{\\tau}\\right)}$$\n",
    "- $\\tau$ 是温度参数，用来控制离散度，温度越低，近似越接近离散采样\n",
    "- 引入 Gumbel 随机变量和温度参数$\\tau$，得到一个平滑的概率分布，使得模型的输出变得可微\n",
    "- 将原本的离散采样过程（不可导）转化为连续的近似，这样，就能利用反向传播来优化模型\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986381fe87eed7f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gumbel Softmax 采样的相关函数："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16139cd4f1d9db6a"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "def onehot_from_logits(logits, eps=0.01):\n",
    "    \"\"\" 独热（one-hot）离散化 \"\"\"\n",
    "    # 生成最优动作的独热形式\n",
    "    argmax_acs = (logits == logits.max(1, keepdim=True)[0]).float()\n",
    "    # 生成随机动作的独热形式\n",
    "    rand_acs = torch.autograd.Variable(torch.eye(logits.shape[1])[[\n",
    "        np.random.choice(range(logits.shape[1]), size=logits.shape[0])\n",
    "    ]], requires_grad=False).to(logits.device)\n",
    "    \n",
    "    # 通过epsilon-贪婪算法来选择用哪个动作\n",
    "    return torch.stack([\n",
    "        argmax_acs[i] if r > eps else rand_acs[i]\n",
    "        for i, r in enumerate(torch.rand(logits.shape[0]))\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):\n",
    "    \"\"\" 1.Gumbel随机变量的采样 \"\"\"\n",
    "    U = torch.autograd.Variable(tens_type(*shape).uniform_(), requires_grad=False)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" 2.加入Gumbel噪声并进行softmax \"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)).to(logits.device)\n",
    "    return F.softmax(y / temperature, dim=1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0):\n",
    "    \"\"\" 从Gumbel-Softmax分布中采样,并进行离散化 \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    y_hard = onehot_from_logits(y)\n",
    "    y = (y_hard.to(logits.device) - y).detach() + y  # 使硬选择部分y_hard与模型的训练过程断开连接,但又保证在训练中通过软选择y反传梯度对模型进行优化\n",
    "    return y  # 虽然返回的是独热量y_hard,但是它的梯度是y,既能够得到一个与环境交互的离散动作,又可以正确地反传梯度"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T02:29:10.675913700Z",
     "start_time": "2025-09-18T02:29:10.659302100Z"
    }
   },
   "id": "aa6bf15fcc8d09ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 注意： \n",
    "在最新的 **MPE** 环境中可以设置每个智能体的 **动作空间为连续**，以上仅用于拓展，以下代码实践直接使用连续动作空间"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7386a8271900609"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 每个智能体：单智能体 DDPG\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62c8ed81a793ef27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 合并定义策略网络和价值网络\n",
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\" DDPG算法 \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim, actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        \n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        \n",
    "        # 初始化目标网络并设置和主网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    # 重参数化方法\n",
    "    def take_action(self, state, explore=False):\n",
    "        action = self.actor(state)\n",
    "        if explore:\n",
    "            action = gumbel_softmax(action)\n",
    "        else:\n",
    "            action = onehot_from_logits(action)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) +\n",
    "                                    param.data * tau)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5196b8aaeb8c867d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. MADDPG 类：维护每个智能体"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8c2d5501e778e4d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 测试与训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc9c79d6485104c3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
