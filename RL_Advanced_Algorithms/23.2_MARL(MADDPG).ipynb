{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96e6457b7d49d23",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 23.2 MARL (MADDPG)\n",
    "> 前文已经介绍了 **多智能体强化学习的基本求解范式：完全中心化 与 完全去中心化**。本节来介绍一种比较经典且效果不错的**进阶范式：中心化训练去中心化执行（centralized training with decentralized execution，CTDE）**：\n",
    "> - **中心化训练**：在训练的时候使用一些单个智能体看不到的 **全局信息** 而以达到更好的训练效果。\n",
    "> - **去中心化执行**：在执行时不使用 **全局信息**，每个智能体完全根据自己的策略执行动作。\n",
    "\n",
    "**CTDE** 可以类比成一个足球队的训练和比赛过程：在训练时，11 个球员可以直接获得教练的指导从而完成球队的整体配合，而教练本身掌握着比赛全局信息，教练的指导也是从整支队、整场比赛的角度进行的；而训练好的 11 个球员在上场比赛时，则根据场上的实时情况直接做出决策，不再有教练的指导。\n",
    "> 所以 **CTDE** 能够在训练时有效地利用全局信息以达到 **更好且更稳定的训练效果**，同时在进行策略模型推断时可以仅利用局部信息，使得算法**具有一定的扩展性**。\n",
    "\n",
    "> **CTDE** 算法主要分为两种：\n",
    "> - 一种是基于值函数的方法，例如 **VDN，QMIX** \n",
    "> - 一种是基于 Actor-Critic 的方法，例如 **MADDPG，COMA** \n",
    "\n",
    "> 本节介绍 **MADDPG** 算法 （[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e2be45d6cd422",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.2.1 MADDPG 算法原理\n",
    "- 学习本节内容之前，可以先复习之前的 [DDPG 算法](../RL_Classic_Algorithms/16.DDPG(Pendulum-v1).ipynb) ，**Actor**梯度为：\n",
    "$$\\nabla_\\theta J(\\mu_\\theta)=\\mathbb{E}_{s\\sim\\nu^{\\pi_\\beta}}\n",
    "\\begin{bmatrix}\n",
    "\\nabla_\\theta\\mu_\\theta(s)\\nabla_aQ_\\omega^{\\mu_\\theta}(s,a)|_{a=\\mu_\\theta(s)}\n",
    "\\end{bmatrix}$$\n",
    "> 与 **‘纯DDPG’** 不同，在 **MADDPG** 中 **所有智能体共享一个中心化的 Critic 网络**，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导；而执行时，**每个智能体的 Actor 网络** 则是完全独立做出行动，即去中心化地执行：\n",
    "\n",
    "> ![Overview_of_MADDPG_AC](./Illustrations/Overview_of_MADDPG_AC.png)\n",
    "\n",
    "考虑现在有$N$个连续的策略$\\mu_{\\theta_i}$，**Actor**梯度公式变为：\n",
    "$$\\nabla_{\\theta_i}J(\\mu_i)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[\\nabla_{\\theta_i}\\mu_i(o_i)\\nabla_{a_i}Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)|_{a_i=\\mu_i(o_i)}]$$\n",
    "- 对于每个智能体 $i$，其动作空间为$A_i$，观测空间为$O_i$\n",
    "- $\\mathbf{x}=(o_{1},\\ldots,o_{N})$ 包含了所有智能体的观测\n",
    "- $\\mathcal{D}$ 表示存储数据的经验回放池，它存储的每一个数据为 $(\\mathbf{x},\\mathbf{x}^{\\prime},a_1,\\ldots,a_N,r_1,\\ldots,r_N)$\n",
    "- 可见，有一个 **Critic** $Q_w$，$N$个 **Actor** $\\mu_\\theta^i$\n",
    "\n",
    "此时在 **MADDPG** 中，**中心化的动作价值网络$Q$** 更新的损失函数变为：\n",
    "$$\\mathcal{L}(\\omega_i)=\\mathbb{E}_{\\mathbf{x},a,r,\\mathbf{x}^{\\prime}}[(Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)-y)^2]$$\n",
    "$$y=r_i+\\gamma Q_i^{\\mu^{\\prime}}(\\mathbf{x}^{\\prime},a_1^{\\prime},\\ldots,a_N^{\\prime})|_{a_j^{\\prime}=\\mu_j^{\\prime}(o_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b9d2ea2d264c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.2.2 MADDPG 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b01965bfccf6a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "导入基本库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a684da7d0c7b63a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:44.898404600Z",
     "start_time": "2025-09-20T18:40:44.801366900Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffb0c6d59d5eab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. 关于环境库\n",
    "同样需新建一个虚拟环境（防止软件包冲突），下载 **MPE2库**（[使用文档](https://mpe2.farama.org/environments/simple_adversary/)），使用其中的 **多智能体粒子环境（multiagent particles environment，MPE）**：\n",
    "(如果图片加载异常，请在浏览器中打开 Notebook)\n",
    "![mpe2_simple_adversary](./Illustrations/simple_adversary.gif)\n",
    "\n",
    "- 该环境中有 1 个红色的对抗智能体（adversary）、$N$ 个蓝色的正常智能体，以及 $N$ 个地点（一般 $N=2$）\n",
    "- 每个智能体的观察空间都为其他智能体的位置以及地点的位置\n",
    "- $N$ 个地点中有一个是目标地点（绿色），正常智能体知道哪一个是目标地点，但对抗智能体不知道\n",
    "- 正常智能体是合作关系：它们其中任意一个距离目标地点足够近，则每个正常智能体都能获得相同的奖励\n",
    "- 对抗智能体如果距离目标地点足够近，也能获得奖励，但它需要猜哪一个才是目标地点\n",
    "\n",
    "因此，性能好的正常智能体会趋于分散到不同的坐标点，以此欺骗对抗智能体。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b8d59b10b413c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***测试（两个版本）：***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "42a4f5c6b14d5aa0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:44.899911900Z",
     "start_time": "2025-09-20T18:40:44.808373500Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpe2 import simple_adversary_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9dd2f61a04bb0a8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:45.010523Z",
     "start_time": "2025-09-20T18:40:44.817947100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentName: adversary_0, Action: 0, Reward: 0.0\n",
      "AgentName: agent_0, Action: 1, Reward: 0.0\n",
      "AgentName: agent_1, Action: 4, Reward: 0.0\n",
      "AgentName: adversary_0, Action: 4, Reward: -1.6175850136777643\n",
      "AgentName: agent_0, Action: 1, Reward: 1.4276513321984416\n",
      "AgentName: agent_1, Action: 4, Reward: 1.4276513321984416\n"
     ]
    }
   ],
   "source": [
    "# 逐个循环智能体\n",
    "env1 = simple_adversary_v3.env(N=2, max_cycles=2, continuous_actions=False, dynamic_rescaling=True)\n",
    "env1.reset(seed=42)\n",
    "\n",
    "for agent in env1.agent_iter():  # 一轮执行完之后，3个智能体才有奖励返回\n",
    "    _, _, termination, truncation, _ = env1.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env1.action_space(agent).sample()\n",
    "        observation, reward, _, _, info = env1.last()\n",
    "        print(\"AgentName: {}, Action: {}, Reward: {}\".format(agent, action, reward))\n",
    "    env1.step(action)  # 注意，执行完动作之后，会将视窗移动到下一个智能体\n",
    "\n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "eae576faa71c1356",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:45.150523Z",
     "start_time": "2025-09-20T18:40:44.890317500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "智能体: ['adversary_0', 'agent_0', 'agent_1']\n",
      "三智能体动作: {'adversary_0': np.int64(1), 'agent_0': np.int64(2), 'agent_1': np.int64(2)}\n",
      "三智能体奖励: defaultdict(<class 'int'>, {'adversary_0': -1.6175850136777643, 'agent_0': 1.4276513321984416, 'agent_1': 1.4276513321984416})\n"
     ]
    }
   ],
   "source": [
    "# 并行执行智能体\n",
    "env2 = simple_adversary_v3.parallel_env(N=2, max_cycles=1, continuous_actions=False, dynamic_rescaling=True)\n",
    "observations, infos = env2.reset(seed=42)\n",
    "print('智能体:', env2.agents)\n",
    "\n",
    "while env2.agents:\n",
    "    \n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env2.action_space(agent).sample() for agent in env2.agents}\n",
    "    print(\"三智能体动作:\", actions)\n",
    "    observations, rewards, terminations, truncations, infos = env2.step(actions)\n",
    "    print(\"三智能体奖励:\", rewards)\n",
    "env2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986381fe87eed7f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. 采样动作不可导问题：离散动作空间需要从概率分布中进行采样来选择动作，采样操作不可导，这会导致无法使用反向传播来优化模型（[复习SAC中使用的重参数化技巧（reparameterization trick）](../RL_Classic_Algorithms/17.1_SAC算法原理.ipynb)）\n",
    "> **DDPG** 算法通过 **确定性策略** 针对 **连续动作空间**，使智能体的动作对于其 **策略参数$\\mu_\\theta$** 可导\n",
    "> 但 **MPE** 环境中 **默认** 每个智能体的 **动作空间是离散的**，虽然依旧是 **确定性策略**，但是需要对网络输出进行 **离散分布的采样**，这种采样是 **不可导的**，此时就需要运用之前的 **重参数化方法（这里要用的是 Gumbel-Softmax 技巧）** 让离散分布的采样可导：\n",
    "\n",
    "**Gumbel-Softmax 变换：**\n",
    "1. Gumbel 随机变量的采样（重参数因子$g_i$）\n",
    "$$g_i=-\\log(-\\log u),u\\sim\\mathrm{Uniform}(0,1)$$\n",
    "2. 加入 Gumbel 噪声并进行 softmax\n",
    "$$\\tilde{y}_i=\\frac{\\exp\\left(\\frac{\\log(\\pi_i)+g_i}{\\tau}\\right)}{\\sum_j\\exp\\left(\\frac{\\log(\\pi_j)+g_j}{\\tau}\\right)}$$\n",
    "- $\\tau$ 是温度参数，用来控制离散度，温度越低，近似越接近离散采样\n",
    "- 引入 Gumbel 随机变量和温度参数$\\tau$，得到一个平滑的概率分布，使得模型的输出变得可微\n",
    "- 将原本的离散采样过程（不可导）转化为连续的近似，这样，就能利用反向传播来优化模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16139cd4f1d9db6a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Gumbel Softmax 采样的相关函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "aa6bf15fcc8d09ad",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:45.153647400Z",
     "start_time": "2025-09-20T18:40:45.143404100Z"
    }
   },
   "outputs": [],
   "source": [
    "def onehot_from_logits(logits, eps=0.01):\n",
    "    \"\"\" 独热（one-hot）离散化 \"\"\"\n",
    "    # 生成最优动作的独热形式\n",
    "    argmax_acs = (logits == logits.max(1, keepdim=True)[0]).float()\n",
    "    # 生成随机动作的独热形式\n",
    "    rand_acs = (torch.eye(logits.shape[1])[\n",
    "    np.random.choice(range(logits.shape[1]), size=logits.shape[0])\n",
    "                ].to(logits.device))\n",
    "    \n",
    "    # 通过epsilon-贪婪算法来选择用哪个动作\n",
    "    return torch.stack([\n",
    "        argmax_acs[i] if r > eps else rand_acs[i]\n",
    "        for i, r in enumerate(torch.rand(logits.shape[0]))\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):\n",
    "    \"\"\" 1.Gumbel随机变量的采样 \"\"\"\n",
    "    U = torch.autograd.Variable(tens_type(*shape).uniform_(), requires_grad=False)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" 2.加入Gumbel噪声并进行softmax \"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)).to(logits.device)\n",
    "    return F.softmax(y / temperature, dim=1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0):\n",
    "    \"\"\" 从Gumbel-Softmax分布中采样,并进行离散化 \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    y_hard = onehot_from_logits(y)\n",
    "    y = (y_hard.to(logits.device) - y).detach() + y  # 使硬选择部分y_hard与模型的训练过程断开连接,但又保证在训练中通过软选择y反传梯度对模型进行优化\n",
    "    return y  # 返回的值是独热量y_hard,但是它的梯度是y,既能够得到一个与环境交互的离散动作,又可以正确地反传梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7386a8271900609",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 注意： \n",
    "在最新的 **MPE** 环境中可以设置每个智能体的 **动作空间为连续**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8ed81a793ef27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. 每个智能体：单智能体 DDPG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5196b8aaeb8c867d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:45.198371Z",
     "start_time": "2025-09-20T18:40:45.153647400Z"
    }
   },
   "outputs": [],
   "source": [
    "# 合并定义策略网络和价值网络\n",
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\" DDPG算法 \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim, actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        \n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        \n",
    "        # 初始化目标网络并设置和主网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    # 重参数化方法\n",
    "    def take_action(self, state, explore=False):\n",
    "        action = self.actor(state)\n",
    "        if explore:\n",
    "            action = gumbel_softmax(action)\n",
    "        else:\n",
    "            action = onehot_from_logits(action)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    # 软更新\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2d5501e778e4d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4. MADDPG 类：维护每个智能体\n",
    "> 每个智能体都有自己的 actor 网络，但它们的 critic 网络 是共享的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "8570b0c1b1781d0d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:45.212577200Z",
     "start_time": "2025-09-20T18:40:45.169935900Z"
    }
   },
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,\n",
    "                 state_dims, action_dims, critic_input_dim, gamma, tau):\n",
    "        self.agents = []\n",
    "        for i in range(len(env.agents)):  # 3\n",
    "            self.agents.append(\n",
    "                DDPG(state_dims[i], action_dims[i], critic_input_dim,\n",
    "                     hidden_dim, actor_lr, critic_lr, device))\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def policies(self):\n",
    "        # 获取所有智能体的 actor 网络\n",
    "        return [agt.actor for agt in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        # 获取所有智能体的 target_actor 网络\n",
    "        return [agt.target_actor for agt in self.agents]\n",
    "\n",
    "    # 多智能体执行动作\n",
    "    def take_action(self, states, explore):\n",
    "        full_order = ['adversary_0', 'agent_0', 'agent_1']\n",
    "        take_actions = {}\n",
    "        for idx, name in enumerate(full_order):\n",
    "             #if name in states:        \n",
    "                 obs_tensor = torch.tensor(\n",
    "                     np.array([states[name]]), dtype=torch.float, device=self.device\n",
    "                 )\n",
    "                 take_actions[name] = self.agents[idx].take_action(obs_tensor, explore)\n",
    "        return take_actions  # 返回字典\n",
    "\n",
    "\n",
    "    # 策略网络更新和评估网络更新\n",
    "    def update(self, sample, i_agent):\n",
    "        obs, act, rew, next_obs, done = sample\n",
    "        cur_agent = self.agents[i_agent]\n",
    "        \n",
    "        # 评估网络更新\n",
    "        cur_agent.critic_optimizer.zero_grad()\n",
    "        all_target_act = [\n",
    "            onehot_from_logits(pi(_next_obs))\n",
    "            for pi, _next_obs in zip(self.target_policies, next_obs)\n",
    "        ]  # 计算目标评估网络所选择的动作\n",
    "        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)\n",
    "        target_critic_value = (rew[i_agent].view(-1, 1) +\n",
    "                               self.gamma * cur_agent.target_critic(target_critic_input) * (1 - done[i_agent].view(-1, 1)))\n",
    "        critic_input = torch.cat((*obs, *act), dim=1)\n",
    "        critic_value = cur_agent.critic(critic_input)\n",
    "        # MSE（均方误差）损失函数，用于计算 critic 网络的损失\n",
    "        critic_loss = torch.nn.MSELoss(critic_value, target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        cur_agent.critic_optimizer.step()\n",
    "\n",
    "        # 策略网络更新\n",
    "        cur_agent.actor_optimizer.zero_grad()\n",
    "        cur_actor_out = cur_agent.actor(obs[i_agent])\n",
    "        cur_act_vf_in = gumbel_softmax(cur_actor_out)\n",
    "        all_actor_acs = []  # 包含所有智能体的动作选择\n",
    "        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):\n",
    "            if i == i_agent:\n",
    "                all_actor_acs.append(cur_act_vf_in)\n",
    "            else:\n",
    "                all_actor_acs.append(onehot_from_logits(pi(_obs)))\n",
    "        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)\n",
    "        actor_loss = -cur_agent.critic(vf_in).mean()  # 目标是最大化 critic 网络的输出\n",
    "        actor_loss += (cur_actor_out**2).mean() * 1e-3  # 为了增加稳定性，防止其输出过大或过小，加上一个 L2 正则化项\n",
    "        actor_loss.backward()\n",
    "        cur_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets(self):\n",
    "        for agt in self.agents:\n",
    "            agt.soft_update(agt.actor, agt.target_actor, self.tau)\n",
    "            agt.soft_update(agt.critic, agt.target_critic, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c79d6485104c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. 测试与训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06fe9682efba66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 功能函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c06f61bdbce80fec",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:45.213587100Z",
     "start_time": "2025-09-20T18:40:45.184712Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(maddpg, n_episode=100):\n",
    "    env4 = simple_adversary_v3.parallel_env(\n",
    "        N=2, max_cycles=25,\n",
    "        continuous_actions=False, dynamic_rescaling=True\n",
    "    )\n",
    "    returns = np.zeros(3, dtype=np.float32)\n",
    "\n",
    "    for _ in range(n_episode):\n",
    "        obs, _ = env4.reset()\n",
    "        while env4.agents:  # 25\n",
    "            actions = maddpg.take_action(obs, explore=False)\n",
    "            int_actions = {\n",
    "                name: np.int64(np.argmax(vec))\n",
    "                for name, vec in actions.items()\n",
    "            }\n",
    "\n",
    "            obs, rew, term, trunc, info = env4.step(int_actions)\n",
    "            #r = np.array([rew[name] for name in env4.agents], dtype=np.float32)\n",
    "            #returns[:len(r)] += r / n_episode\n",
    "            rew = np.array(list(rew.values())) \n",
    "            returns += rew / n_episode\n",
    "    return returns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 参数设置："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "782a0d297360999e"
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0\n",
      "agent_0\n",
      "agent_1\n",
      "[np.int64(5), np.int64(5), np.int64(5)] [8, 10, 10] 43\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.parallel_env(N=2, max_cycles=25, continuous_actions=False, dynamic_rescaling=True)\n",
    "env.reset(seed=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "actor_lr = 1e-2\n",
    "critic_lr = 1e-2\n",
    "hidden_dim = 64\n",
    "gamma = 0.95\n",
    "tau = 1e-2\n",
    "\n",
    "state_dims = []\n",
    "action_dims = []\n",
    "for agent in env.agents:\n",
    "    action_space = env.action_space(agent)\n",
    "    print(agent)\n",
    "    observation_space = env.observation_space(agent)\n",
    "    action_dims.append(action_space.n)  # 'n'表示离散动作空间的大小\n",
    "    state_dims.append(observation_space.shape[0])\n",
    "critic_input_dim = sum(state_dims) + sum(action_dims)\n",
    "print(action_dims, state_dims, critic_input_dim)\n",
    "env.reset(seed=0)\n",
    "\n",
    "maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,\n",
    "                action_dims, critic_input_dim, gamma, tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:40:45.418440500Z",
     "start_time": "2025-09-20T18:40:45.200537400Z"
    }
   },
   "id": "720afcf3c1f127a0"
  },
  {
   "cell_type": "markdown",
   "id": "85736dfde75fc773",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 训练:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e0fde686f6739e5f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-20T18:41:22.624758700Z",
     "start_time": "2025-09-20T18:40:45.410423600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, [-50.33283615112305, 28.745771408081055, 28.745771408081055]\n",
      "Episode: 100, [-46.97783279418945, 25.70140838623047, 25.70140838623047]\n",
      "([[array([ 1.1192245 ,  0.4970538 ,  1.5303763 ,  1.0415109 , -0.35422722,\n",
      "        0.935157  ,  1.9434441 ,  0.6779096 ], dtype=float32), array([ 1.8846035 ,  0.10635395,  1.4734517 , -0.43810317,  1.8846035 ,\n",
      "        0.10635395,  0.35422722, -0.935157  ,  2.2976713 , -0.2572474 ],\n",
      "      dtype=float32), array([-0.41306782,  0.36360133, -0.8242195 , -0.1808558 , -0.41306782,\n",
      "        0.36360133, -1.9434441 , -0.6779096 , -2.2976713 ,  0.2572474 ],\n",
      "      dtype=float32)]], [[array([1., 0., 0., 0., 0.], dtype=float32), array([0., 0., 0., 0., 1.], dtype=float32), array([0., 0., 0., 1., 0.], dtype=float32)]], [[-1.7560202889747076, 1.1698097573224824, 1.1698097573224824]], [[array([ 1.0192734,  0.4741161,  1.430425 ,  1.0185732, -0.5422999,\n",
      "        0.8875161,  1.8790675,  0.6412633], dtype=float32), array([ 1.972725  ,  0.13105711,  1.5615733 , -0.41340002,  1.972725  ,\n",
      "        0.13105711,  0.5422999 , -0.8875161 ,  2.4213674 , -0.24625278],\n",
      "      dtype=float32), array([-0.44864243,  0.3773099 , -0.85979414, -0.16714723, -0.44864243,\n",
      "        0.3773099 , -1.8790675 , -0.6412633 , -2.4213674 ,  0.24625278],\n",
      "      dtype=float32)]], [[False, False, False]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MSELoss' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[238], line 55\u001B[0m\n\u001B[0;32m     53\u001B[0m         sample \u001B[38;5;241m=\u001B[39m [stack_array(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m sample]\n\u001B[0;32m     54\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m a_i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(agents)):\n\u001B[1;32m---> 55\u001B[0m             \u001B[43mmaddpg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma_i\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     56\u001B[0m         maddpg\u001B[38;5;241m.\u001B[39mupdate_all_targets()\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (i_episode \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i_episode\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[235], line 54\u001B[0m, in \u001B[0;36mMADDPG.update\u001B[1;34m(self, sample, i_agent)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m# MSE（均方误差）损失函数，用于计算 critic 网络的损失\u001B[39;00m\n\u001B[0;32m     53\u001B[0m critic_loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mMSELoss(critic_value, target_critic_value\u001B[38;5;241m.\u001B[39mdetach())\n\u001B[1;32m---> 54\u001B[0m \u001B[43mcritic_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m()\n\u001B[0;32m     55\u001B[0m cur_agent\u001B[38;5;241m.\u001B[39mcritic_optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     57\u001B[0m \u001B[38;5;66;03m# 策略网络更新\u001B[39;00m\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\torch\\nn\\modules\\module.py:1940\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1938\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1939\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1940\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m   1941\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1942\u001B[0m )\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'MSELoss' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "return_list = []  # 记录每一轮的回报（return）\n",
    "num_episodes = 5000\n",
    "total_step = 0\n",
    "minimal_size = 4000\n",
    "update_interval = 100  # 更新间隔\n",
    "batch_size = 1024  # 训练样本大小\n",
    "\n",
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "env.reset(seed=0)\n",
    "agents = env.agents\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    states, _ = env.reset(seed=0)\n",
    "    if len(states)<3:\n",
    "        print(len(states))\n",
    "    while env.agents:  # 25\n",
    "        actions = maddpg.take_action(states, explore=True)\n",
    "        #print(actions)\n",
    "        int_actions = {\n",
    "            name: np.int64(np.argmax(vec))\n",
    "            for name, vec in actions.items()\n",
    "        }\n",
    "        # print(int_actions)\n",
    "        if len(int_actions)<3:\n",
    "            print(len(int_actions))\n",
    "        \n",
    "        next_states, rewards, done, truncations, infos = env.step(int_actions)\n",
    "        \n",
    "        s = [states[name] for name in agents]\n",
    "        a = [actions[name] for name in agents]\n",
    "        r = [rewards[name] for name in agents]\n",
    "        s_next = [next_states[name] for name in agents]\n",
    "        d = [done[name] for name in agents]\n",
    "        replay_buffer.add(s, a, r, s_next, d)\n",
    "        \n",
    "        states = next_states\n",
    "\n",
    "        total_step += 1\n",
    "        if replay_buffer.size() >= minimal_size and total_step % update_interval == 0:\n",
    "            sample = replay_buffer.sample1(1)  # 1024\n",
    "            print(sample)\n",
    "\n",
    "            def stack_array(x):\n",
    "                # 转置\n",
    "                transposed = list(zip(*x))\n",
    "                return [\n",
    "                    torch.as_tensor(np.vstack(group), dtype=torch.float32, device=device)\n",
    "                    for group in transposed\n",
    "                ]\n",
    "\n",
    "            sample = [stack_array(x) for x in sample]\n",
    "            for a_i in range(len(agents)):\n",
    "                maddpg.update(sample, a_i)\n",
    "            maddpg.update_all_targets()\n",
    "            \n",
    "    if (i_episode + 1) % 100 == 0 or i_episode==0:\n",
    "        ep_returns = evaluate(maddpg)\n",
    "        return_list.append(ep_returns)\n",
    "        print(f\"Episode: {i_episode+1}, {ep_returns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6566e9002dc283",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 绘图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ca0f679a0ec7e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-20T18:41:22.622753700Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 保存为CSV文件\n",
    "return_array = np.array(return_list)\n",
    "pd.DataFrame(return_array).to_csv('MADDPG_returns_data.csv', index=False, header=False)\n",
    "print(\"回报数据已保存到 MADDPG_returns_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5397c388aa6373",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-20T18:41:22.624758700Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.smoothing import moving_average\n",
    "# 从CSV文件读取数据\n",
    "loaded_data = pd.read_csv('MADDPG_returns_data.csv', header=None).values\n",
    "# 使用Plotly绘图\n",
    "agents = [\"adversary_0\", \"agent_0\", \"agent_1\"]\n",
    "\n",
    "for i, agent_name in enumerate(agents):\n",
    "    # 计算x轴数据（每100个episode一个点）\n",
    "    x_data = np.arange(loaded_data.shape[0]) * 100\n",
    "    y_data = moving_average(loaded_data[:, i], 9)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_data,\n",
    "        y=y_data,\n",
    "        mode='lines',\n",
    "        name=agent_name,\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"{agent_name} by MADDPG\",\n",
    "        xaxis_title=\"Episodes\",\n",
    "        yaxis_title=\"Returns\",\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b7fb10eaf3412",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 待解决问题：\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
