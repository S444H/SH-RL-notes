{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96e6457b7d49d23",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 23.2 MARL (MADDPG)\n",
    "> 前文已经介绍了 **多智能体强化学习的基本求解范式：完全中心化 与 完全去中心化**。本节来介绍一种比较经典且效果不错的**进阶范式：中心化训练去中心化执行（centralized training with decentralized execution，CTDE）**：\n",
    "> - **中心化训练**：在训练的时候使用一些单个智能体看不到的 **全局信息** 而以达到更好的训练效果。\n",
    "> - **去中心化执行**：在执行时不使用 **全局信息**，每个智能体完全根据自己的策略执行动作。\n",
    "\n",
    "**CTDE** 可以类比成一个足球队的训练和比赛过程：在训练时，11 个球员可以直接获得教练的指导从而完成球队的整体配合，而教练本身掌握着比赛全局信息，教练的指导也是从整支队、整场比赛的角度进行的；而训练好的 11 个球员在上场比赛时，则根据场上的实时情况直接做出决策，不再有教练的指导。\n",
    "> 所以 **CTDE** 能够在训练时有效地利用全局信息以达到 **更好且更稳定的训练效果**，同时在进行策略模型推断时可以仅利用局部信息，使得算法**具有一定的扩展性**。\n",
    "\n",
    "> **CTDE** 算法主要分为两种：\n",
    "> - 一种是基于值函数的方法，例如 **VDN，QMIX** \n",
    "> - 一种是基于 Actor-Critic 的方法，例如 **MADDPG，COMA** \n",
    "\n",
    "> 本节介绍 **MADDPG** 算法 （[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e2be45d6cd422",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.2.1 MADDPG 算法原理\n",
    "- 学习本节内容之前，可以先复习之前的 [DDPG 算法](../RL_Classic_Algorithms/16.DDPG(Pendulum-v1).ipynb) ，**Actor**梯度为：\n",
    "$$\\nabla_\\theta J(\\mu_\\theta)=\\mathbb{E}_{s\\sim\\nu^{\\pi_\\beta}}\n",
    "\\begin{bmatrix}\n",
    "\\nabla_\\theta\\mu_\\theta(s)\\nabla_aQ_\\omega^{\\mu_\\theta}(s,a)|_{a=\\mu_\\theta(s)}\n",
    "\\end{bmatrix}$$\n",
    "> 与 **‘纯DDPG’** 不同，在 **MADDPG** 中 **所有智能体共享一个中心化的 Critic 网络**，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导；而执行时，**每个智能体的 Actor 网络** 则是完全独立做出行动，即去中心化地执行：\n",
    "\n",
    "> ![Overview_of_MADDPG_AC](./Illustrations/Overview_of_MADDPG_AC.png)\n",
    "\n",
    "考虑现在有$N$个连续的策略$\\mu_{\\theta_i}$，**Actor**梯度公式变为：\n",
    "$$\\nabla_{\\theta_i}J(\\mu_i)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[\\nabla_{\\theta_i}\\mu_i(o_i)\\nabla_{a_i}Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)|_{a_i=\\mu_i(o_i)}]$$\n",
    "- 对于每个智能体 $i$，其动作空间为$A_i$，观测空间为$O_i$\n",
    "- $\\mathbf{x}=(o_{1},\\ldots,o_{N})$ 包含了所有智能体的观测\n",
    "- $\\mathcal{D}$ 表示存储数据的经验回放池，它存储的每一个数据为 $(\\mathbf{x},\\mathbf{x}^{\\prime},a_1,\\ldots,a_N,r_1,\\ldots,r_N)$\n",
    "- 可见，有一个 **Critic** $Q_w$，$N$个 **Actor** $\\mu_\\theta^i$\n",
    "\n",
    "此时在 **MADDPG** 中，**中心化的动作价值网络$Q$** 更新的损失函数变为：\n",
    "$$\\mathcal{L}(\\omega_i)=\\mathbb{E}_{\\mathbf{x},a,r,\\mathbf{x}^{\\prime}}[(Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)-y)^2]$$\n",
    "$$y=r_i+\\gamma Q_i^{\\mu^{\\prime}}(\\mathbf{x}^{\\prime},a_1^{\\prime},\\ldots,a_N^{\\prime})|_{a_j^{\\prime}=\\mu_j^{\\prime}(o_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b9d2ea2d264c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.2.2 MADDPG 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b01965bfccf6a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "导入基本库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a684da7d0c7b63a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:41.524073800Z",
     "start_time": "2025-09-21T13:04:37.339647300Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffb0c6d59d5eab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. 关于环境库\n",
    "同样需新建一个虚拟环境（防止软件包冲突），下载 **MPE2库**（[使用文档](https://mpe2.farama.org/environments/simple_adversary/)），使用其中的 **多智能体粒子环境（multiagent particles environment，MPE）**：\n",
    "(如果图片加载异常，请在浏览器中打开 Notebook)\n",
    "![mpe2_simple_adversary](./Illustrations/simple_adversary.gif)\n",
    "\n",
    "- 该环境中有 1 个红色的对抗智能体（adversary）、$N$ 个蓝色的正常智能体，以及 $N$ 个地点（一般 $N=2$）\n",
    "- 每个智能体的观察空间都为其他智能体的位置以及地点的位置\n",
    "- $N$ 个地点中有一个是目标地点（绿色），正常智能体知道哪一个是目标地点，但对抗智能体不知道\n",
    "- 正常智能体是合作关系：它们其中任意一个距离目标地点足够近，则每个正常智能体都能获得相同的奖励\n",
    "- 对抗智能体如果距离目标地点足够近，也能获得奖励，但它需要猜哪一个才是目标地点\n",
    "\n",
    "因此，性能好的正常智能体会趋于分散到不同的坐标点，以此欺骗对抗智能体。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b8d59b10b413c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***测试（两个版本）：***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42a4f5c6b14d5aa0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:42.038088600Z",
     "start_time": "2025-09-21T13:04:41.525074700Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpe2 import simple_adversary_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dd2f61a04bb0a8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:42.357149Z",
     "start_time": "2025-09-21T13:04:42.041705800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentName: adversary_0, Action: [0.8492577  0.6995638  0.47482747 0.4934049  0.10040354], Reward: 0.0\n",
      "AgentName: agent_0, Action: [0.86889106 0.72690856 0.931795   0.61236095 0.9526541 ], Reward: 0.0\n",
      "AgentName: agent_1, Action: [0.6391585  0.925156   0.1494986  0.23830062 0.6773103 ], Reward: 0.0\n",
      "AgentName: adversary_0, Action: [0.20951553 0.850252   0.50153923 0.04110396 0.7620364 ], Reward: -1.6175850136777643\n",
      "AgentName: agent_0, Action: [0.49952307 0.32639977 0.94924635 0.509019   0.38772565], Reward: 1.4276513321984416\n",
      "AgentName: agent_1, Action: [0.27510354 0.25966677 0.11764825 0.78459024 0.5443927 ], Reward: 1.4276513321984416\n"
     ]
    }
   ],
   "source": [
    "# 逐个循环智能体\n",
    "env1 = simple_adversary_v3.env(N=2, max_cycles=2, continuous_actions=True, dynamic_rescaling=True)\n",
    "env1.reset(seed=42)\n",
    "\n",
    "for agent in env1.agent_iter():  # 一轮执行完之后，3个智能体才有奖励返回\n",
    "    _, _, termination, truncation, _ = env1.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env1.action_space(agent).sample()\n",
    "        observation, reward, _, _, info = env1.last()\n",
    "        print(\"AgentName: {}, Action: {}, Reward: {}\".format(agent, action, reward))\n",
    "    env1.step(action)  # 注意，执行完动作之后，会将视窗移动到下一个智能体\n",
    "\n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eae576faa71c1356",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:42.635118300Z",
     "start_time": "2025-09-21T13:04:42.353634400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "智能体: ['adversary_0', 'agent_0', 'agent_1']\n",
      "三智能体动作: {'adversary_0': array([0.7658434 , 0.78055173, 0.79897845, 0.582395  , 0.8307444 ],\n",
      "      dtype=float32), 'agent_0': array([0.01800416, 0.42815614, 0.6513796 , 0.49666423, 0.1362664 ],\n",
      "      dtype=float32), 'agent_1': array([0.12272205, 0.5419411 , 0.4509158 , 0.6353917 , 0.11222132],\n",
      "      dtype=float32)}\n",
      "三智能体奖励: defaultdict(<class 'int'>, {'adversary_0': -1.6175850136777643, 'agent_0': 1.4276513321984416, 'agent_1': 1.4276513321984416})\n"
     ]
    }
   ],
   "source": [
    "# 并行执行智能体\n",
    "env2 = simple_adversary_v3.parallel_env(N=2, max_cycles=1, continuous_actions=True, dynamic_rescaling=True)\n",
    "observations, infos = env2.reset(seed=42)\n",
    "print('智能体:', env2.agents)\n",
    "\n",
    "while env2.agents:\n",
    "    \n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env2.action_space(agent).sample() for agent in env2.agents}\n",
    "    print(\"三智能体动作:\", actions)\n",
    "    observations, rewards, terminations, truncations, infos = env2.step(actions)\n",
    "    print(\"三智能体奖励:\", rewards)\n",
    "env2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986381fe87eed7f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. 采样动作不可导问题：离散动作空间需要从概率分布中进行采样来选择动作，采样操作不可导，这会导致无法使用反向传播来优化模型（[复习SAC中使用的重参数化技巧（reparameterization trick）](../RL_Classic_Algorithms/17.1_SAC算法原理.ipynb)）\n",
    "> **DDPG** 算法通过 **确定性策略** 针对 **连续动作空间**，使智能体的动作对于其 **策略参数$\\mu_\\theta$** 可导\n",
    "> 但 **MPE** 环境中 **默认** 每个智能体的 **动作空间是离散的**，虽然依旧是 **确定性策略**，但是需要对网络输出进行 **离散分布的采样**，这种采样是 **不可导的**，此时就需要运用之前的 **重参数化方法（这里要用的是 Gumbel-Softmax 技巧）** 让离散分布的采样可导：\n",
    "\n",
    "**Gumbel-Softmax 变换：**\n",
    "1. Gumbel 随机变量的采样（重参数因子$g_i$）\n",
    "$$g_i=-\\log(-\\log u),u\\sim\\mathrm{Uniform}(0,1)$$\n",
    "2. 加入 Gumbel 噪声并进行 softmax\n",
    "$$\\tilde{y}_i=\\frac{\\exp\\left(\\frac{\\log(\\pi_i)+g_i}{\\tau}\\right)}{\\sum_j\\exp\\left(\\frac{\\log(\\pi_j)+g_j}{\\tau}\\right)}$$\n",
    "- $\\tau$ 是温度参数，用来控制离散度，温度越低，近似越接近离散采样\n",
    "- 引入 Gumbel 随机变量和温度参数$\\tau$，得到一个平滑的概率分布，使得模型的输出变得可微\n",
    "- 将原本的离散采样过程（不可导）转化为连续的近似，这样，就能利用反向传播来优化模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7386a8271900609",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 注意： \n",
    "在最新的 **MPE** 环境中可以设置每个智能体的 **动作空间为连续**，所以本实验采用以下方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def actors_actions(actions, eps=0.01):  \n",
    "    # 生成随机动作（探索时使用）\n",
    "    random_actions = torch.rand_like(actions)\n",
    "    return torch.stack([\n",
    "        actions[i] if r > eps else random_actions[i]\n",
    "        for i, r in enumerate(torch.rand(actions.shape[0]))\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:42.677870700Z",
     "start_time": "2025-09-21T13:04:42.633023700Z"
    }
   },
   "id": "9ae4c6d8f7b3963a"
  },
  {
   "cell_type": "markdown",
   "id": "62c8ed81a793ef27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. 每个智能体：单智能体 DDPG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5196b8aaeb8c867d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:42.688615Z",
     "start_time": "2025-09-21T13:04:42.655595300Z"
    }
   },
   "outputs": [],
   "source": [
    "# 合并定义策略网络和价值网络\n",
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim, is_actor_network=True):\n",
    "        super().__init__()\n",
    "        self.is_actor_network = is_actor_network\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #-------------------------------------------------------------------\n",
    "        # 对于策略网络，使用 sigmoid 限制输出在 [0, 1] 范围内\n",
    "        if self.is_actor_network:\n",
    "            return torch.sigmoid(self.fc3(x))\n",
    "        else:\n",
    "            # 对于评估网络，直接返回输出\n",
    "            return self.fc3(x)\n",
    "        #-------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\" DDPG算法 \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim, actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        #-------------------------------------------------------------------\n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim, is_actor_network=False).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1, hidden_dim, is_actor_network=False).to(device)\n",
    "        #-------------------------------------------------------------------\n",
    "        # 初始化目标网络并设置和主网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    def take_action(self, state):\n",
    "        action = self.actor(state).detach().cpu().numpy()[0]\n",
    "        return action\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # 软更新\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2d5501e778e4d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4. MADDPG 类：维护每个智能体\n",
    "> 每个智能体都有自己的 actor 网络，但它们的 critic 网络 是共享的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8570b0c1b1781d0d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:42.699755500Z",
     "start_time": "2025-09-21T13:04:42.664631100Z"
    }
   },
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,\n",
    "                 state_dims, action_dims, critic_input_dim, gamma, tau):\n",
    "        self.agents = []\n",
    "        for i in range(len(env.agents)):  # 3\n",
    "            self.agents.append(\n",
    "                DDPG(state_dims[i], action_dims[i], critic_input_dim,\n",
    "                     hidden_dim, actor_lr, critic_lr, device))  # 评估网络输入是一样的\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "        self.critic_criterion = torch.nn.MSELoss()\n",
    "        self.env = env\n",
    "    @property\n",
    "    def policies(self):\n",
    "        # 获取所有智能体的 actor 网络\n",
    "        return [agt.actor for agt in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        # 获取所有智能体的 target_actor 网络\n",
    "        return [agt.target_actor for agt in self.agents]\n",
    "\n",
    "    # 多智能体执行动作\n",
    "    def take_actions(self, states, explore, explore_prob=0.01):\n",
    "        full_order = ['adversary_0', 'agent_0', 'agent_1']\n",
    "        e = 0\n",
    "        if explore and np.random.rand() < explore_prob:\n",
    "            explore_actions = {agent: self.env.action_space(agent).sample() for agent in full_order}\n",
    "            e = 1\n",
    "            return explore_actions, e\n",
    "        else:\n",
    "            take_actions = {}\n",
    "            for idx, name in enumerate(full_order):   \n",
    "                obs_tensor = torch.tensor(\n",
    "                     np.array([states[name]]), dtype=torch.float, device=self.device\n",
    "                )\n",
    "                take_actions[name] = self.agents[idx].take_action(obs_tensor)\n",
    "            return take_actions, e\n",
    "\n",
    "\n",
    "    # 策略网络更新和评估网络更新\n",
    "    def update(self, sample, i_agent):\n",
    "        obs, act, rew, next_obs, done = sample\n",
    "        cur_agent = self.agents[i_agent]\n",
    "        \n",
    "        # 评估网络更新\n",
    "        cur_agent.critic_optimizer.zero_grad()\n",
    "        all_target_act = [\n",
    "            actors_actions(pi(_next_obs))\n",
    "            for pi, _next_obs in zip(self.target_policies, next_obs)\n",
    "            ]  # 计算目标评估网络所选择的动作\n",
    "        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)\n",
    "        target_critic_value = (rew[i_agent].view(-1, 1) +\n",
    "                               self.gamma * cur_agent.target_critic(target_critic_input) * (1 - done[i_agent].view(-1, 1))\n",
    "                               )\n",
    "        critic_input = torch.cat((*obs, *act), dim=1)\n",
    "        critic_value = cur_agent.critic(critic_input)\n",
    "        # MSE（均方误差）损失函数，用于计算 critic 网络的损失\n",
    "        critic_loss = self.critic_criterion(critic_value,\n",
    "                                            target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        cur_agent.critic_optimizer.step()\n",
    "\n",
    "        # 策略网络更新\n",
    "        cur_agent.actor_optimizer.zero_grad()\n",
    "        cur_actor_out = cur_agent.actor(obs[i_agent])\n",
    "        #cur_act_vf_in = gumbel_softmax(cur_actor_out)\n",
    "        all_actor_acs = []  # 包含所有智能体的动作选择\n",
    "        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):\n",
    "            if i == i_agent:\n",
    "                all_actor_acs.append(cur_actor_out)\n",
    "            else:\n",
    "                all_actor_acs.append(actors_actions(pi(_obs)))\n",
    "        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)\n",
    "        actor_loss = -cur_agent.critic(vf_in).mean()  # 目标是最大化 critic 网络的输出\n",
    "        actor_loss += (cur_actor_out**2).mean() * 1e-3  # 为了增加稳定性，防止其输出过大或过小，加上一个 L2 正则化项\n",
    "        actor_loss.backward()\n",
    "        cur_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets(self):\n",
    "        for agt in self.agents:\n",
    "            agt.soft_update(agt.actor, agt.target_actor, self.tau)\n",
    "            agt.soft_update(agt.critic, agt.target_critic, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c79d6485104c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. 测试与训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06fe9682efba66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 功能函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06f61bdbce80fec",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:42.721642600Z",
     "start_time": "2025-09-21T13:04:42.694683900Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(maddpg, n_episode=100):\n",
    "    env4 = simple_adversary_v3.parallel_env(\n",
    "        N=2, max_cycles=40,\n",
    "        continuous_actions=True, dynamic_rescaling=True\n",
    "    )\n",
    "    returns = np.zeros(3, dtype=np.float32)\n",
    "\n",
    "    for _ in range(n_episode):\n",
    "        obs, _ = env4.reset(seed=42)\n",
    "        while env4.agents:  # 25\n",
    "            actions, _ = maddpg.take_actions(obs, explore=False)\n",
    "            obs, rew, term, trunc, info = env4.step(actions)\n",
    "            #r = np.array([rew[name] for name in env4.agents], dtype=np.float32)\n",
    "            #returns[:len(r)] += r / n_episode\n",
    "            rew = np.array(list(rew.values())) \n",
    "            returns += rew / n_episode\n",
    "    return returns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a0d297360999e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "720afcf3c1f127a0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:04:45.663820300Z",
     "start_time": "2025-09-21T13:04:42.709524900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0\n",
      "agent_0\n",
      "agent_1\n",
      "[5, 5, 5] [8, 10, 10] 43\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.parallel_env(N=2, max_cycles=40, continuous_actions=True, dynamic_rescaling=True)\n",
    "env.reset(seed=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "actor_lr = 1e-2\n",
    "critic_lr = 1e-2\n",
    "hidden_dim = 64\n",
    "gamma = 0.95\n",
    "tau = 1e-2\n",
    "\n",
    "state_dims = []\n",
    "action_dims = []\n",
    "for agent in env.agents:\n",
    "    action_space = env.action_space(agent)\n",
    "    print(agent)\n",
    "    observation_space = env.observation_space(agent)\n",
    "    action_dims.append(action_space.shape[0])  # 'n'表示离散动作空间的大小\n",
    "    state_dims.append(observation_space.shape[0])\n",
    "critic_input_dim = sum(state_dims) + sum(action_dims)\n",
    "print(action_dims, state_dims, critic_input_dim)\n",
    "env.reset(seed=0)\n",
    "\n",
    "maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,\n",
    "                action_dims, critic_input_dim, gamma, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85736dfde75fc773",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 训练:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0fde686f6739e5f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:08:44.413847500Z",
     "start_time": "2025-09-21T13:04:45.663820300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, [-54.354644775390625, 47.191158294677734, 47.191158294677734]\n",
      "Episode: 100, [-55.37299728393555, 45.791282653808594, 45.791282653808594]\n",
      "Episode: 200, [-64.70082092285156, 19.83726692199707, 19.83726692199707]\n",
      "Episode: 300, [-64.70082092285156, 19.808670043945312, 19.808670043945312]\n",
      "Episode: 400, [-64.70082092285156, 19.80816650390625, 19.80816650390625]\n",
      "Episode: 500, [-64.70082092285156, 19.808124542236328, 19.808124542236328]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 48\u001B[0m\n\u001B[0;32m     46\u001B[0m         \u001B[38;5;66;03m#print(sample)\u001B[39;00m\n\u001B[0;32m     47\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m a_i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(agents)):\n\u001B[1;32m---> 48\u001B[0m             \u001B[43mmaddpg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma_i\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m         maddpg\u001B[38;5;241m.\u001B[39mupdate_all_targets()\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (i_episode \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m i_episode\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[7], line 78\u001B[0m, in \u001B[0;36mMADDPG.update\u001B[1;34m(self, sample, i_agent)\u001B[0m\n\u001B[0;32m     76\u001B[0m actor_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mcur_agent\u001B[38;5;241m.\u001B[39mcritic(vf_in)\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# 目标是最大化 critic 网络的输出\u001B[39;00m\n\u001B[0;32m     77\u001B[0m actor_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (cur_actor_out\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m)\u001B[38;5;241m.\u001B[39mmean() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1e-3\u001B[39m  \u001B[38;5;66;03m# 为了增加稳定性，防止其输出过大或过小，加上一个 L2 正则化项\u001B[39;00m\n\u001B[1;32m---> 78\u001B[0m \u001B[43mactor_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m cur_agent\u001B[38;5;241m.\u001B[39mactor_optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\torch\\_tensor.py:648\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    638\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    640\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    641\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    646\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    647\u001B[0m     )\n\u001B[1;32m--> 648\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    649\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    650\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    348\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    350\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    351\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 353\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    361\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    822\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 824\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Variable\u001B[38;5;241m.\u001B[39m_execution_engine\u001B[38;5;241m.\u001B[39mrun_backward(  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    825\u001B[0m         t_outputs, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m    826\u001B[0m     )  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    827\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    828\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "return_list = []  # 记录每一轮的回报（return）\n",
    "num_episodes = 5000\n",
    "total_step = 0\n",
    "minimal_size = 4000\n",
    "update_interval = 100  # 更新间隔\n",
    "batch_size = 1024  # 训练样本大小\n",
    "\n",
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "env.reset(seed=0)\n",
    "agents = env.agents\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    states, _ = env.reset(seed=0)\n",
    "    if len(states)<3:\n",
    "        print(len(states))\n",
    "    while env.agents:  # 25\n",
    "        actions, e = maddpg.take_actions(states, explore=True)\n",
    "        #print(actions, e)\n",
    "        next_states, rewards, done, truncations, infos = env.step(actions)\n",
    "        \n",
    "        s = [states[name] for name in agents]\n",
    "        a = [actions[name] for name in agents]\n",
    "        r = [rewards[name] for name in agents]\n",
    "        s_next = [next_states[name] for name in agents]\n",
    "        d = [done[name] for name in agents]\n",
    "        replay_buffer.add(s, a, r, s_next, d)\n",
    "        \n",
    "        states = next_states\n",
    "\n",
    "        total_step += 1\n",
    "        if replay_buffer.size() >= minimal_size and total_step % update_interval == 0:\n",
    "            sample = replay_buffer.sample1(batch_size)  # 1024\n",
    "            #print(sample)\n",
    "            def stack_array(x):\n",
    "                # 转置\n",
    "                rearranged = [[sub_x[i] for sub_x in x]\n",
    "                              for i in range(len(x[0]))]\n",
    "                return [\n",
    "                    torch.FloatTensor(np.vstack(aa)).to(device)\n",
    "                    for aa in rearranged\n",
    "                ]\n",
    "\n",
    "            sample = [stack_array(x) for x in sample]\n",
    "            #print(sample)\n",
    "            for a_i in range(len(agents)):\n",
    "                maddpg.update(sample, a_i)\n",
    "            maddpg.update_all_targets()\n",
    "            \n",
    "    if (i_episode + 1) % 100 == 0 or i_episode==0:\n",
    "        ep_returns = evaluate(maddpg)\n",
    "        return_list.append(ep_returns)\n",
    "        print(f\"Episode: {i_episode+1}, {ep_returns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6566e9002dc283",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 绘图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ca0f679a0ec7e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-21T13:08:44.416848400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 保存为CSV文件\n",
    "return_array = np.array(return_list)\n",
    "pd.DataFrame(return_array).to_csv('MADDPG_returns_data.csv', index=False, header=False)\n",
    "print(\"回报数据已保存到 MADDPG_returns_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5397c388aa6373",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-21T13:08:44.417846Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.smoothing import moving_average\n",
    "# 从CSV文件读取数据\n",
    "loaded_data = pd.read_csv('MADDPG_returns_data.csv', header=None).values\n",
    "# 使用Plotly绘图\n",
    "agents = [\"adversary_0\", \"agent_0\", \"agent_1\"]\n",
    "\n",
    "for i, agent_name in enumerate(agents):\n",
    "    # 计算x轴数据（每100个episode一个点）\n",
    "    x_data = np.arange(loaded_data.shape[0]) * 100\n",
    "    y_data = moving_average(loaded_data[:, i], 9)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_data,\n",
    "        y=y_data,\n",
    "        mode='lines',\n",
    "        name=agent_name,\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"{agent_name} by MADDPG\",\n",
    "        xaxis_title=\"Episodes\",\n",
    "        yaxis_title=\"Returns\",\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 可视化界面展示："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed239261e871717f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m         actions, e \u001B[38;5;241m=\u001B[39m maddpg\u001B[38;5;241m.\u001B[39mtake_actions(states, explore\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m      6\u001B[0m         \u001B[38;5;66;03m#print(actions)\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m         states, rewards, done, truncations, infos \u001B[38;5;241m=\u001B[39m \u001B[43menv5\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m env5\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\pettingzoo\\utils\\conversions.py:207\u001B[0m, in \u001B[0;36maec_to_parallel_wrapper.step\u001B[1;34m(self, actions)\u001B[0m\n\u001B[0;32m    203\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    204\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpected agent \u001B[39m\u001B[38;5;132;01m{\u001B[39;00magent\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m got agent \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39magent_selection\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Parallel environment wrapper expects agents to step in a cycle.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    205\u001B[0m         )\n\u001B[0;32m    206\u001B[0m obs, rew, termination, truncation, info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39mlast()\n\u001B[1;32m--> 207\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maec_env\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactions\u001B[49m\u001B[43m[\u001B[49m\u001B[43magent\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m agent \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39magents:\n\u001B[0;32m    209\u001B[0m     rewards[agent] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maec_env\u001B[38;5;241m.\u001B[39mrewards[agent]\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\order_enforcing.py:70\u001B[0m, in \u001B[0;36mOrderEnforcingWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_updated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:47\u001B[0m, in \u001B[0;36mBaseWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: ActionType) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\clip_out_of_bounds.py:52\u001B[0m, in \u001B[0;36mClipOutOfBoundsWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     43\u001B[0m     EnvLogger\u001B[38;5;241m.\u001B[39mwarn_action_out_of_bound(\n\u001B[0;32m     44\u001B[0m         action\u001B[38;5;241m=\u001B[39maction, action_space\u001B[38;5;241m=\u001B[39mspace, backup_policy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclipping to space\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     45\u001B[0m     )\n\u001B[0;32m     46\u001B[0m     action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mclip(\n\u001B[0;32m     47\u001B[0m         action,  \u001B[38;5;66;03m# pyright: ignore[reportGeneralTypeIssues]\u001B[39;00m\n\u001B[0;32m     48\u001B[0m         space\u001B[38;5;241m.\u001B[39mlow,  \u001B[38;5;66;03m# pyright: ignore[reportGeneralTypeIssues]\u001B[39;00m\n\u001B[0;32m     49\u001B[0m         space\u001B[38;5;241m.\u001B[39mhigh,  \u001B[38;5;66;03m# pyright: ignore[reportGeneralTypeIssues]\u001B[39;00m\n\u001B[0;32m     50\u001B[0m     )\n\u001B[1;32m---> 52\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\pettingzoo\\utils\\wrappers\\base.py:47\u001B[0m, in \u001B[0;36mBaseWrapper.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: ActionType) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 47\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\mpe2\\_mpe_utils\\simple_env.py:271\u001B[0m, in \u001B[0;36mSimpleEnv.step\u001B[1;34m(self, action)\u001B[0m\n\u001B[0;32m    268\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accumulate_rewards()\n\u001B[0;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m--> 271\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrender\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\python\\Anaconda3\\envs\\mpe\\lib\\site-packages\\mpe2\\_mpe_utils\\simple_env.py:294\u001B[0m, in \u001B[0;36mSimpleEnv.render\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrender_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhuman\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    293\u001B[0m     pygame\u001B[38;5;241m.\u001B[39mdisplay\u001B[38;5;241m.\u001B[39mflip()\n\u001B[1;32m--> 294\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtick\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrender_fps\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    295\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "env5 = simple_adversary_v3.parallel_env(N=2, max_cycles=40, continuous_actions=True, render_mode=\"human\", dynamic_rescaling=True)\n",
    "for i_episode in range(3):\n",
    "    states, _ = env5.reset()\n",
    "    while env5.agents:  # 25\n",
    "        actions, e = maddpg.take_actions(states, explore=False)\n",
    "        #print(actions)\n",
    "        states, rewards, done, truncations, infos = env5.step(actions)\n",
    "env5.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T13:09:03.176773700Z",
     "start_time": "2025-09-21T13:08:51.672088200Z"
    }
   },
   "id": "77ac178abed37e3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 待解决的问题：\n",
    "- 智能体不动了\n",
    "- 动作处理阶段有问题"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fed6e2e8fce6d22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
