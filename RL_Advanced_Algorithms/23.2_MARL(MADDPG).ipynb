{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c96e6457b7d49d23",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 23.2 MARL (MADDPG)\n",
    "> 前文已经介绍了 **多智能体强化学习的基本求解范式：完全中心化 与 完全去中心化**。本节来介绍一种比较经典且效果不错的**进阶范式：中心化训练去中心化执行（centralized training with decentralized execution，CTDE）**：\n",
    "> - **中心化训练**：在训练的时候使用一些单个智能体看不到的 **全局信息** 而以达到更好的训练效果。\n",
    "> - **去中心化执行**：在执行时不使用 **全局信息**，每个智能体完全根据自己的策略执行动作。\n",
    "\n",
    "**CTDE** 可以类比成一个足球队的训练和比赛过程：在训练时，11 个球员可以直接获得教练的指导从而完成球队的整体配合，而教练本身掌握着比赛全局信息，教练的指导也是从整支队、整场比赛的角度进行的；而训练好的 11 个球员在上场比赛时，则根据场上的实时情况直接做出决策，不再有教练的指导。\n",
    "> 所以 **CTDE** 能够在训练时有效地利用全局信息以达到 **更好且更稳定的训练效果**，同时在进行策略模型推断时可以仅利用局部信息，使得算法**具有一定的扩展性**。\n",
    "\n",
    "> **CTDE** 算法主要分为两种：\n",
    "> - 一种是基于值函数的方法，例如 **VDN，QMIX** \n",
    "> - 一种是基于 Actor-Critic 的方法，例如 **MADDPG，COMA** \n",
    "\n",
    "> 本节介绍 **MADDPG** 算法 （[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0e2be45d6cd422",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.2.1 MADDPG 算法原理\n",
    "- 学习本节内容之前，可以先复习之前的 [DDPG 算法](../RL_Classic_Algorithms/16.DDPG(Pendulum-v1).ipynb) ，**Actor**梯度为：\n",
    "$$\\nabla_\\theta J(\\mu_\\theta)=\\mathbb{E}_{s\\sim\\nu^{\\pi_\\beta}}\n",
    "\\begin{bmatrix}\n",
    "\\nabla_\\theta\\mu_\\theta(s)\\nabla_aQ_\\omega^{\\mu_\\theta}(s,a)|_{a=\\mu_\\theta(s)}\n",
    "\\end{bmatrix}$$\n",
    "> 与 **‘纯DDPG’** 不同，在 **MADDPG** 中 **所有智能体共享一个中心化的 Critic 网络**，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导；而执行时，**每个智能体的 Actor 网络** 则是完全独立做出行动，即去中心化地执行：\n",
    "\n",
    "> ![Overview_of_MADDPG_AC](./Illustrations/Overview_of_MADDPG_AC.png)\n",
    "\n",
    "考虑现在有$N$个连续的策略$\\mu_{\\theta_i}$，**Actor**梯度公式变为：\n",
    "$$\\nabla_{\\theta_i}J(\\mu_i)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[\\nabla_{\\theta_i}\\mu_i(o_i)\\nabla_{a_i}Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)|_{a_i=\\mu_i(o_i)}]$$\n",
    "- 对于每个智能体 $i$，其动作空间为$A_i$，观测空间为$O_i$\n",
    "- $\\mathbf{x}=(o_{1},\\ldots,o_{N})$ 包含了所有智能体的观测\n",
    "- $\\mathcal{D}$ 表示存储数据的经验回放池，它存储的每一个数据为 $(\\mathbf{x},\\mathbf{x}^{\\prime},a_1,\\ldots,a_N,r_1,\\ldots,r_N)$\n",
    "- 可见，有一个 **Critic** $Q_w$，$N$个 **Actor** $\\mu_\\theta^i$\n",
    "\n",
    "此时在 **MADDPG** 中，**中心化的动作价值网络$Q$** 更新的损失函数变为：\n",
    "$$\\mathcal{L}(\\omega_i)=\\mathbb{E}_{\\mathbf{x},a,r,\\mathbf{x}^{\\prime}}[(Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)-y)^2]$$\n",
    "$$y=r_i+\\gamma Q_i^{\\mu^{\\prime}}(\\mathbf{x}^{\\prime},a_1^{\\prime},\\ldots,a_N^{\\prime})|_{a_j^{\\prime}=\\mu_j^{\\prime}(o_j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b9d2ea2d264c4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.2.2 MADDPG 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b01965bfccf6a6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "导入基本库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a684da7d0c7b63a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:05.812637800Z",
     "start_time": "2025-09-21T10:46:05.725498700Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdffb0c6d59d5eab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1. 关于环境库\n",
    "同样需新建一个虚拟环境（防止软件包冲突），下载 **MPE2库**（[使用文档](https://mpe2.farama.org/environments/simple_adversary/)），使用其中的 **多智能体粒子环境（multiagent particles environment，MPE）**：\n",
    "(如果图片加载异常，请在浏览器中打开 Notebook)\n",
    "![mpe2_simple_adversary](./Illustrations/simple_adversary.gif)\n",
    "\n",
    "- 该环境中有 1 个红色的对抗智能体（adversary）、$N$ 个蓝色的正常智能体，以及 $N$ 个地点（一般 $N=2$）\n",
    "- 每个智能体的观察空间都为其他智能体的位置以及地点的位置\n",
    "- $N$ 个地点中有一个是目标地点（绿色），正常智能体知道哪一个是目标地点，但对抗智能体不知道\n",
    "- 正常智能体是合作关系：它们其中任意一个距离目标地点足够近，则每个正常智能体都能获得相同的奖励\n",
    "- 对抗智能体如果距离目标地点足够近，也能获得奖励，但它需要猜哪一个才是目标地点\n",
    "\n",
    "因此，性能好的正常智能体会趋于分散到不同的坐标点，以此欺骗对抗智能体。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b8d59b10b413c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***测试（两个版本）：***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "42a4f5c6b14d5aa0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:05.832477500Z",
     "start_time": "2025-09-21T10:46:05.735620600Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpe2 import simple_adversary_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9dd2f61a04bb0a8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:05.957251200Z",
     "start_time": "2025-09-21T10:46:05.741416500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentName: adversary_0, Action: [0.40669394 0.9366993  0.17366584 0.40559247 0.06674325], Reward: 0.0\n",
      "AgentName: agent_0, Action: [0.05778387 0.6406837  0.31650752 0.21499564 0.57346207], Reward: 0.0\n",
      "AgentName: agent_1, Action: [0.5929536  0.6518198  0.4570186  0.1431223  0.21338588], Reward: 0.0\n",
      "AgentName: adversary_0, Action: [0.25103202 0.9565129  0.8445674  0.6802933  0.7212755 ], Reward: -1.6175850136777643\n",
      "AgentName: agent_0, Action: [0.89899516 0.9659684  0.3988493  0.39673197 0.46944836], Reward: 1.4276513321984416\n",
      "AgentName: agent_1, Action: [0.8739639  0.6578781  0.11482245 0.82699007 0.15310134], Reward: 1.4276513321984416\n"
     ]
    }
   ],
   "source": [
    "# 逐个循环智能体\n",
    "env1 = simple_adversary_v3.env(N=2, max_cycles=2, continuous_actions=True, dynamic_rescaling=True)\n",
    "env1.reset(seed=42)\n",
    "\n",
    "for agent in env1.agent_iter():  # 一轮执行完之后，3个智能体才有奖励返回\n",
    "    _, _, termination, truncation, _ = env1.last()\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env1.action_space(agent).sample()\n",
    "        observation, reward, _, _, info = env1.last()\n",
    "        print(\"AgentName: {}, Action: {}, Reward: {}\".format(agent, action, reward))\n",
    "    env1.step(action)  # 注意，执行完动作之后，会将视窗移动到下一个智能体\n",
    "\n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "eae576faa71c1356",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:06.103972400Z",
     "start_time": "2025-09-21T10:46:05.832477500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "智能体: ['adversary_0', 'agent_0', 'agent_1']\n",
      "三智能体动作: {'adversary_0': array([0.04028012, 0.34433487, 0.47131336, 0.58480334, 0.44477195],\n",
      "      dtype=float32), 'agent_0': array([0.57984185, 0.33126107, 0.35894918, 0.78700763, 0.10439193],\n",
      "      dtype=float32), 'agent_1': array([0.7839657 , 0.6180009 , 0.50998306, 0.52536434, 0.9825897 ],\n",
      "      dtype=float32)}\n",
      "三智能体奖励: defaultdict(<class 'int'>, {'adversary_0': -1.6175850136777643, 'agent_0': 1.4276513321984416, 'agent_1': 1.4276513321984416})\n"
     ]
    }
   ],
   "source": [
    "# 并行执行智能体\n",
    "env2 = simple_adversary_v3.parallel_env(N=2, max_cycles=1, continuous_actions=True, dynamic_rescaling=True)\n",
    "observations, infos = env2.reset(seed=42)\n",
    "print('智能体:', env2.agents)\n",
    "\n",
    "while env2.agents:\n",
    "    \n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env2.action_space(agent).sample() for agent in env2.agents}\n",
    "    print(\"三智能体动作:\", actions)\n",
    "    observations, rewards, terminations, truncations, infos = env2.step(actions)\n",
    "    print(\"三智能体奖励:\", rewards)\n",
    "env2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986381fe87eed7f0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2. 采样动作不可导问题：离散动作空间需要从概率分布中进行采样来选择动作，采样操作不可导，这会导致无法使用反向传播来优化模型（[复习SAC中使用的重参数化技巧（reparameterization trick）](../RL_Classic_Algorithms/17.1_SAC算法原理.ipynb)）\n",
    "> **DDPG** 算法通过 **确定性策略** 针对 **连续动作空间**，使智能体的动作对于其 **策略参数$\\mu_\\theta$** 可导\n",
    "> 但 **MPE** 环境中 **默认** 每个智能体的 **动作空间是离散的**，虽然依旧是 **确定性策略**，但是需要对网络输出进行 **离散分布的采样**，这种采样是 **不可导的**，此时就需要运用之前的 **重参数化方法（这里要用的是 Gumbel-Softmax 技巧）** 让离散分布的采样可导：\n",
    "\n",
    "**Gumbel-Softmax 变换：**\n",
    "1. Gumbel 随机变量的采样（重参数因子$g_i$）\n",
    "$$g_i=-\\log(-\\log u),u\\sim\\mathrm{Uniform}(0,1)$$\n",
    "2. 加入 Gumbel 噪声并进行 softmax\n",
    "$$\\tilde{y}_i=\\frac{\\exp\\left(\\frac{\\log(\\pi_i)+g_i}{\\tau}\\right)}{\\sum_j\\exp\\left(\\frac{\\log(\\pi_j)+g_j}{\\tau}\\right)}$$\n",
    "- $\\tau$ 是温度参数，用来控制离散度，温度越低，近似越接近离散采样\n",
    "- 引入 Gumbel 随机变量和温度参数$\\tau$，得到一个平滑的概率分布，使得模型的输出变得可微\n",
    "- 将原本的离散采样过程（不可导）转化为连续的近似，这样，就能利用反向传播来优化模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7386a8271900609",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 注意： \n",
    "在最新的 **MPE** 环境中可以设置每个智能体的 **动作空间为连续**，所以本实验采用以下方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "def actors_actions(actions, eps=0.01):  \n",
    "    # 生成随机动作（探索时使用）\n",
    "    random_actions = torch.rand_like(actions)\n",
    "    return torch.stack([\n",
    "        actions[i] if r > eps else random_actions[i]\n",
    "        for i, r in enumerate(torch.rand(actions.shape[0]))\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:06.120128Z",
     "start_time": "2025-09-21T10:46:06.097889200Z"
    }
   },
   "id": "9ae4c6d8f7b3963a"
  },
  {
   "cell_type": "markdown",
   "id": "62c8ed81a793ef27",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3. 每个智能体：单智能体 DDPG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "5196b8aaeb8c867d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:06.172298600Z",
     "start_time": "2025-09-21T10:46:06.120128Z"
    }
   },
   "outputs": [],
   "source": [
    "# 合并定义策略网络和价值网络\n",
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim, is_actor_network=True):\n",
    "        super().__init__()\n",
    "        self.is_actor_network = is_actor_network\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #-------------------------------------------------------------------\n",
    "        # 对于策略网络，使用 sigmoid 限制输出在 [0, 1] 范围内\n",
    "        if self.is_actor_network:\n",
    "            return torch.sigmoid(self.fc3(x))\n",
    "        else:\n",
    "            # 对于评估网络，直接返回输出\n",
    "            return self.fc3(x)\n",
    "        #-------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\" DDPG算法 \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim, actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        #-------------------------------------------------------------------\n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim, is_actor_network=False).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1, hidden_dim, is_actor_network=False).to(device)\n",
    "        #-------------------------------------------------------------------\n",
    "        # 初始化目标网络并设置和主网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    def take_action(self, state, explore=False, explore_prob=0.01):\n",
    "        action = self.actor(state).detach().cpu().numpy()[0]\n",
    "        if explore and np.random.rand() < explore_prob:\n",
    "             # 如果探索开启且概率小于 explore_prob，生成随机动作\n",
    "             action = np.random.rand(self.action_dim)  # 随机生成 [0, 1] 范围内的动作\n",
    "        action = np.clip(action, 0.01, 0.99)\n",
    "        return action\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # 软更新\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c2d5501e778e4d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4. MADDPG 类：维护每个智能体\n",
    "> 每个智能体都有自己的 actor 网络，但它们的 critic 网络 是共享的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8570b0c1b1781d0d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:06.186044100Z",
     "start_time": "2025-09-21T10:46:06.129282300Z"
    }
   },
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,\n",
    "                 state_dims, action_dims, critic_input_dim, gamma, tau):\n",
    "        self.agents = []\n",
    "        for i in range(len(env.agents)):  # 3\n",
    "            self.agents.append(\n",
    "                DDPG(state_dims[i], action_dims[i], critic_input_dim,\n",
    "                     hidden_dim, actor_lr, critic_lr, device))  # 评估网络输入是一样的\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "        self.critic_criterion = torch.nn.MSELoss()\n",
    "\n",
    "    @property\n",
    "    def policies(self):\n",
    "        # 获取所有智能体的 actor 网络\n",
    "        return [agt.actor for agt in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        # 获取所有智能体的 target_actor 网络\n",
    "        return [agt.target_actor for agt in self.agents]\n",
    "\n",
    "    # 多智能体执行动作\n",
    "    def take_action(self, states, explore):\n",
    "        full_order = ['adversary_0', 'agent_0', 'agent_1']\n",
    "        take_actions = {}\n",
    "        for idx, name in enumerate(full_order):\n",
    "             #if name in states:        \n",
    "                 obs_tensor = torch.tensor(\n",
    "                     np.array([states[name]]), dtype=torch.float, device=self.device\n",
    "                 )\n",
    "                 take_actions[name] = self.agents[idx].take_action(obs_tensor, explore)\n",
    "        return take_actions  # 返回字典\n",
    "\n",
    "\n",
    "    # 策略网络更新和评估网络更新\n",
    "    def update(self, sample, i_agent):\n",
    "        obs, act, rew, next_obs, done = sample\n",
    "        cur_agent = self.agents[i_agent]\n",
    "        \n",
    "        # 评估网络更新\n",
    "        cur_agent.critic_optimizer.zero_grad()\n",
    "        all_target_act = [\n",
    "            actors_actions(pi(_next_obs))\n",
    "            for pi, _next_obs in zip(self.target_policies, next_obs)\n",
    "        ]  # 计算目标评估网络所选择的动作\n",
    "        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)\n",
    "        target_critic_value = (rew[i_agent].view(-1, 1) +\n",
    "                               self.gamma * cur_agent.target_critic(target_critic_input) * (1 - done[i_agent].view(-1, 1)))\n",
    "        critic_input = torch.cat((*obs, *act), dim=1)\n",
    "        critic_value = cur_agent.critic(critic_input)\n",
    "        #return critic_value,target_critic_value.detach()\n",
    "        # MSE（均方误差）损失函数，用于计算 critic 网络的损失\n",
    "        critic_loss = self.critic_criterion(critic_value,\n",
    "                                            target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        cur_agent.critic_optimizer.step()\n",
    "\n",
    "        # 策略网络更新\n",
    "        cur_agent.actor_optimizer.zero_grad()\n",
    "        cur_actor_out = cur_agent.actor(obs[i_agent])\n",
    "        #cur_act_vf_in = gumbel_softmax(cur_actor_out)\n",
    "        all_actor_acs = []  # 包含所有智能体的动作选择\n",
    "        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):\n",
    "            if i == i_agent:\n",
    "                all_actor_acs.append(cur_actor_out)\n",
    "            else:\n",
    "                all_actor_acs.append(actors_actions(pi(_obs)))\n",
    "        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)\n",
    "        actor_loss = -cur_agent.critic(vf_in).mean()  # 目标是最大化 critic 网络的输出\n",
    "        actor_loss += (cur_actor_out**2).mean() * 1e-3  # 为了增加稳定性，防止其输出过大或过小，加上一个 L2 正则化项\n",
    "        actor_loss.backward()\n",
    "        cur_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets(self):\n",
    "        for agt in self.agents:\n",
    "            agt.soft_update(agt.actor, agt.target_actor, self.tau)\n",
    "            agt.soft_update(agt.critic, agt.target_critic, self.tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9c79d6485104c3",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5. 测试与训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06fe9682efba66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 功能函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c06f61bdbce80fec",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:06.186044100Z",
     "start_time": "2025-09-21T10:46:06.145254600Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(maddpg, n_episode=100):\n",
    "    env4 = simple_adversary_v3.parallel_env(\n",
    "        N=2, max_cycles=25,\n",
    "        continuous_actions=True, dynamic_rescaling=True\n",
    "    )\n",
    "    returns = np.zeros(3, dtype=np.float32)\n",
    "\n",
    "    for _ in range(n_episode):\n",
    "        obs, _ = env4.reset()\n",
    "        while env4.agents:  # 25\n",
    "            actions = maddpg.take_action(obs, explore=False)\n",
    "            obs, rew, term, trunc, info = env4.step(actions)\n",
    "            #r = np.array([rew[name] for name in env4.agents], dtype=np.float32)\n",
    "            #returns[:len(r)] += r / n_episode\n",
    "            rew = np.array(list(rew.values())) \n",
    "            returns += rew / n_episode\n",
    "    return returns.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a0d297360999e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "720afcf3c1f127a0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:06.390268800Z",
     "start_time": "2025-09-21T10:46:06.163501600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adversary_0\n",
      "agent_0\n",
      "agent_1\n",
      "[5, 5, 5] [8, 10, 10] 43\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.parallel_env(N=2, max_cycles=25, continuous_actions=True, dynamic_rescaling=True)\n",
    "env.reset(seed=0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "actor_lr = 1e-2\n",
    "critic_lr = 1e-2\n",
    "hidden_dim = 64\n",
    "gamma = 0.95\n",
    "tau = 1e-2\n",
    "\n",
    "state_dims = []\n",
    "action_dims = []\n",
    "for agent in env.agents:\n",
    "    action_space = env.action_space(agent)\n",
    "    print(agent)\n",
    "    observation_space = env.observation_space(agent)\n",
    "    action_dims.append(action_space.shape[0])  # 'n'表示离散动作空间的大小\n",
    "    state_dims.append(observation_space.shape[0])\n",
    "critic_input_dim = sum(state_dims) + sum(action_dims)\n",
    "print(action_dims, state_dims, critic_input_dim)\n",
    "env.reset(seed=0)\n",
    "\n",
    "maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,\n",
    "                action_dims, critic_input_dim, gamma, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85736dfde75fc773",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 训练:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e0fde686f6739e5f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-21T10:46:16.755851300Z",
     "start_time": "2025-09-21T10:46:06.376261400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Received an action [0.32412374 0.65535299 0.96245361 0.44496168 0.66224882] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "Episode: 1, [-26.393068313598633, 5.956729888916016, 5.956729888916016]\n",
      "[WARNING]: Received an action [0.20528763 0.52109343 0.95229311 0.09883702 0.85185525] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.82872991 0.69567284 0.917539   0.97256464 0.8554428 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.48317987 0.09415894 0.78082277 0.62005522 0.97953549] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.09471956 0.34057258 0.77907842 0.20949728 0.97870114] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.34633166 0.72221267 0.40459212 0.52995034 0.52673928] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.31564595 0.92003039 0.46750752 0.44440526 0.73765536] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.52351493 0.64675752 0.14858143 0.10123545 0.14371556] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.51644433 0.84449758 0.42800712 0.47549822 0.74105474] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.21279864 0.37749721 0.06705479 0.23030744 0.71641294] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.48780367 0.87678993 0.41603167 0.62730259 0.83623322] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.44278141 0.12302168 0.18970126 0.63716498 0.92732546] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.57006088 0.72866749 0.63614154 0.92225567 0.22450698] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.93730817 0.02660573 0.38318284 0.07280713 0.40201983] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.91675414 0.2052971  0.09266452 0.65413516 0.1742614 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.12903335 0.695902   0.22276478 0.87353548 0.15639286] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.73633016 0.42031665 0.05348755 0.78486422 0.06917947] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.37889511 0.50482284 0.17214296 0.57348086 0.50617692] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.34374116 0.05912046 0.75424334 0.40137408 0.54871488] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.20783271 0.98197595 0.5087839  0.66893202 0.38812374] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.01       0.31163876 0.63694135 0.53391723 0.46671144] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.24146625 0.24887742 0.59232168 0.46249342 0.87743144] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.07820139 0.79176194 0.24675789 0.66424755 0.67004177] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.07276137 0.79393449 0.65257766 0.2892177  0.46106549] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.29928801 0.84294128 0.5232027  0.96188866 0.95114447] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.68059026 0.61470558 0.3746051  0.65122333 0.0918958 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.67498082 0.01018439 0.04731733 0.397692   0.88851394] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.77725909 0.15215804 0.03140568 0.94688491 0.89035846] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.87067303 0.17414217 0.62171756 0.73131034 0.75970827] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.29464242 0.69452324 0.27278524 0.34046129 0.48401606] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.89967138 0.75611034 0.15483428 0.0875112  0.01596408] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.93388858 0.20174233 0.39989949 0.92707768 0.72427462] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.41612135 0.92503798 0.76873379 0.77208385 0.49456517] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.34198377 0.8966369  0.95103126 0.38264521 0.39113189] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.38942019 0.53940709 0.57515487 0.01583324 0.54161968] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.9857423  0.62807611 0.45631383 0.97765404 0.13224767] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.08085663 0.07178713 0.49983756 0.40243978 0.71277842] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.86235975 0.85354701 0.46097963 0.82848196 0.5449946 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.09164337 0.12841872 0.7039563  0.34171898 0.38822196] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.06859595 0.6795125  0.15745622 0.01       0.11699963] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.20970535 0.75950173 0.73480176 0.44083872 0.84647365] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.47945695 0.39371407 0.0941972  0.91376845 0.5553193 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.35183555 0.94349102 0.03688018 0.27827659 0.96958181] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.40776349 0.82359686 0.32805153 0.47214163 0.03335306] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.32939824 0.15187181 0.28339862 0.81045123 0.0501095 ] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.77779526 0.82299435 0.6999025  0.32737106 0.01993549] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n",
      "[WARNING]: Received an action [0.92968742 0.37806484 0.92482582 0.83524856 0.46689106] that was outside action space Box(0.0, 1.0, (5,), float32). Environment is clipping to space\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[292], line 19\u001B[0m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(states))\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m env\u001B[38;5;241m.\u001B[39magents:  \u001B[38;5;66;03m# 25\u001B[39;00m\n\u001B[1;32m---> 19\u001B[0m     actions \u001B[38;5;241m=\u001B[39m \u001B[43mmaddpg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstates\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexplore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;66;03m#print(actions)\u001B[39;00m\n\u001B[0;32m     21\u001B[0m     next_states, rewards, done, truncations, infos \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(actions)\n",
      "Cell \u001B[1;32mIn[289], line 33\u001B[0m, in \u001B[0;36mMADDPG.take_action\u001B[1;34m(self, states, explore)\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m idx, name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(full_order):\n\u001B[0;32m     29\u001B[0m      \u001B[38;5;66;03m#if name in states:        \u001B[39;00m\n\u001B[0;32m     30\u001B[0m          obs_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\n\u001B[0;32m     31\u001B[0m              np\u001B[38;5;241m.\u001B[39marray([states[name]]), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\n\u001B[0;32m     32\u001B[0m          )\n\u001B[1;32m---> 33\u001B[0m          take_actions[name] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magents\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexplore\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m take_actions\n",
      "Cell \u001B[1;32mIn[288], line 42\u001B[0m, in \u001B[0;36mDDPG.take_action\u001B[1;34m(self, state, explore, explore_prob)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mtake_action\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, explore\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, explore_prob\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m):\n\u001B[1;32m---> 42\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m explore \u001B[38;5;129;01mand\u001B[39;00m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrand() \u001B[38;5;241m<\u001B[39m explore_prob:\n\u001B[0;32m     44\u001B[0m          \u001B[38;5;66;03m# 如果探索开启且概率小于 explore_prob，生成随机动作\u001B[39;00m\n\u001B[0;32m     45\u001B[0m          action \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maction_dim)  \u001B[38;5;66;03m# 随机生成 [0, 1] 范围内的动作\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "return_list = []  # 记录每一轮的回报（return）\n",
    "num_episodes = 5000\n",
    "total_step = 0\n",
    "minimal_size = 4000\n",
    "update_interval = 100  # 更新间隔\n",
    "batch_size = 1024  # 训练样本大小\n",
    "\n",
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "env.reset(seed=0)\n",
    "agents = env.agents\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    states, _ = env.reset(seed=0)\n",
    "    if len(states)<3:\n",
    "        print(len(states))\n",
    "    while env.agents:  # 25\n",
    "        actions = maddpg.take_action(states, explore=True)\n",
    "        #print(actions)\n",
    "        next_states, rewards, done, truncations, infos = env.step(actions)\n",
    "        \n",
    "        s = [states[name] for name in agents]\n",
    "        a = [actions[name] for name in agents]\n",
    "        r = [rewards[name] for name in agents]\n",
    "        s_next = [next_states[name] for name in agents]\n",
    "        d = [done[name] for name in agents]\n",
    "        replay_buffer.add(s, a, r, s_next, d)\n",
    "        \n",
    "        states = next_states\n",
    "\n",
    "        total_step += 1\n",
    "        if replay_buffer.size() >= minimal_size and total_step % update_interval == 0:\n",
    "            sample = replay_buffer.sample1(batch_size)  # 1024\n",
    "            #print(sample)\n",
    "            def stack_array(x):\n",
    "                # 转置\n",
    "                rearranged = [[sub_x[i] for sub_x in x]\n",
    "                              for i in range(len(x[0]))]\n",
    "                return [\n",
    "                    torch.FloatTensor(np.vstack(aa)).to(device)\n",
    "                    for aa in rearranged\n",
    "                ]\n",
    "\n",
    "            sample = [stack_array(x) for x in sample]\n",
    "            #print(sample)\n",
    "            for a_i in range(len(agents)):\n",
    "                maddpg.update(sample, a_i)\n",
    "                #print(m,n)\n",
    "            maddpg.update_all_targets()\n",
    "            \n",
    "    if (i_episode + 1) % 100 == 0 or i_episode==0:\n",
    "        ep_returns = evaluate(maddpg)\n",
    "        return_list.append(ep_returns)\n",
    "        print(f\"Episode: {i_episode+1}, {ep_returns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6566e9002dc283",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 绘图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971ca0f679a0ec7e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-21T10:46:16.752336Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 保存为CSV文件\n",
    "return_array = np.array(return_list)\n",
    "pd.DataFrame(return_array).to_csv('MADDPG_returns_data.csv', index=False, header=False)\n",
    "print(\"回报数据已保存到 MADDPG_returns_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5397c388aa6373",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-21T10:46:16.754852900Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utils.smoothing import moving_average\n",
    "# 从CSV文件读取数据\n",
    "loaded_data = pd.read_csv('MADDPG_returns_data.csv', header=None).values\n",
    "# 使用Plotly绘图\n",
    "agents = [\"adversary_0\", \"agent_0\", \"agent_1\"]\n",
    "\n",
    "for i, agent_name in enumerate(agents):\n",
    "    # 计算x轴数据（每100个episode一个点）\n",
    "    x_data = np.arange(loaded_data.shape[0]) * 100\n",
    "    y_data = moving_average(loaded_data[:, i], 9)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=x_data,\n",
    "        y=y_data,\n",
    "        mode='lines',\n",
    "        name=agent_name,\n",
    "        line=dict(width=2)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f\"{agent_name} by MADDPG\",\n",
    "        xaxis_title=\"Episodes\",\n",
    "        yaxis_title=\"Returns\",\n",
    "        hovermode=\"x unified\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 可视化界面展示："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed239261e871717f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env5 = simple_adversary_v3.parallel_env(N=2, max_cycles=25, continuous_actions=True, render_mode=\"human\", dynamic_rescaling=True)\n",
    "for i_episode in range(3):\n",
    "    states, _ = env5.reset()\n",
    "    while env5.agents:  # 25\n",
    "        actions = maddpg.take_action(states, explore=False)\n",
    "        int_actions = {\n",
    "            name: np.int64(np.argmax(vec))\n",
    "            for name, vec in actions.items()\n",
    "        }\n",
    "        states, rewards, done, truncations, infos = env5.step(int_actions)\n",
    "env5.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-09-21T10:46:16.755851300Z"
    }
   },
   "id": "77ac178abed37e3b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 待解决的问题：\n",
    "- "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5fed6e2e8fce6d22"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
