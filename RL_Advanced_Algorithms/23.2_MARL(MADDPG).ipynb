{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 23.2 MARL (MADDPG)\n",
    "> 前文已经介绍了 **多智能体强化学习的基本求解范式：完全中心化 与 完全去中心化**。本节来介绍一种比较经典且效果不错的**进阶范式：中心化训练去中心化执行（centralized training with decentralized execution，CTDE）**：\n",
    "> - **中心化训练**：在训练的时候使用一些单个智能体看不到的 **全局信息** 而以达到更好的训练效果。\n",
    "> - **去中心化执行**：在执行时不使用 **全局信息**，每个智能体完全根据自己的策略执行动作。\n",
    "\n",
    "**CTDE** 可以类比成一个足球队的训练和比赛过程：在训练时，11 个球员可以直接获得教练的指导从而完成球队的整体配合，而教练本身掌握着比赛全局信息，教练的指导也是从整支队、整场比赛的角度进行的；而训练好的 11 个球员在上场比赛时，则根据场上的实时情况直接做出决策，不再有教练的指导。\n",
    "> 所以 **CTDE** 能够在训练时有效地利用全局信息以达到 **更好且更稳定的训练效果**，同时在进行策略模型推断时可以仅利用局部信息，使得算法**具有一定的扩展性**。\n",
    "\n",
    "> **CTDE** 算法主要分为两种：\n",
    "> - 一种是基于值函数的方法，例如 **VDN，QMIX** \n",
    "> - 一种是基于 Actor-Critic 的方法，例如 **MADDPG，COMA** \n",
    "\n",
    "> 本节介绍 **MADDPG** 算法 （[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c96e6457b7d49d23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 23.2.1 MADDPG 算法原理\n",
    "- 学习本节内容之前，可以先复习之前的 [DDPG 算法](../RL_Classic_Algorithms/16.DDPG(Pendulum-v1).ipynb) ，**Actor**梯度为：\n",
    "$$\\nabla_\\theta J(\\mu_\\theta)=\\mathbb{E}_{s\\sim\\nu^{\\pi_\\beta}}\n",
    "\\begin{bmatrix}\n",
    "\\nabla_\\theta\\mu_\\theta(s)\\nabla_aQ_\\omega^{\\mu_\\theta}(s,a)|_{a=\\mu_\\theta(s)}\n",
    "\\end{bmatrix}$$\n",
    "> 与 **‘纯DDPG’** 不同，在 **MADDPG** 中 **所有智能体共享一个中心化的 Critic 网络**，该 Critic 网络在训练的过程中同时对每个智能体的 Actor 网络给出指导；而执行时，**每个智能体的 Actor 网络** 则是完全独立做出行动，即去中心化地执行：\n",
    "\n",
    "> ![Overview_of_MADDPG_AC](./Illustrations/Overview_of_MADDPG_AC.png)\n",
    "\n",
    "考虑现在有$N$个连续的策略$\\mu_{\\theta_i}$，**Actor**梯度公式变为：\n",
    "$$\\nabla_{\\theta_i}J(\\mu_i)=\\mathbb{E}_{\\mathbf{x}\\sim\\mathcal{D}}[\\nabla_{\\theta_i}\\mu_i(o_i)\\nabla_{a_i}Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)|_{a_i=\\mu_i(o_i)}]$$\n",
    "- 对于每个智能体 $i$，其动作空间为$A_i$，观测空间为$O_i$\n",
    "- $\\mathbf{x}=(o_{1},\\ldots,o_{N})$ 包含了所有智能体的观测\n",
    "- $\\mathcal{D}$ 表示存储数据的经验回放池，它存储的每一个数据为 $(\\mathbf{x},\\mathbf{x}^{\\prime},a_1,\\ldots,a_N,r_1,\\ldots,r_N)$\n",
    "- 可见，有一个 **Critic** $Q_w$，$N$个 **Actor** $\\mu_\\theta^i$\n",
    "\n",
    "此时在 **MADDPG** 中，**中心化的动作价值网络$Q$** 更新的损失函数变为：\n",
    "$$\\mathcal{L}(\\omega_i)=\\mathbb{E}_{\\mathbf{x},a,r,\\mathbf{x}^{\\prime}}[(Q_i^\\mu(\\mathbf{x},a_1,\\ldots,a_N)-y)^2]$$\n",
    "$$y=r_i+\\gamma Q_i^{\\mu^{\\prime}}(\\mathbf{x}^{\\prime},a_1^{\\prime},\\ldots,a_N^{\\prime})|_{a_j^{\\prime}=\\mu_j^{\\prime}(o_j)}$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b0e2be45d6cd422"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 23.2.2 MADDPG 代码实践"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818b9d2ea2d264c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入基本库"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27b01965bfccf6a6"
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import replay_buffer\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:45.084072600Z",
     "start_time": "2025-09-18T16:20:44.867442900Z"
    }
   },
   "id": "a684da7d0c7b63a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. 关于环境库\n",
    "同样需新建一个虚拟环境（防止软件包冲突），下载 **MPE2库**（[使用文档](https://mpe2.farama.org/environments/simple_adversary/)），使用其中的 **多智能体粒子环境（multiagent particles environment，MPE）**：\n",
    "(如果图片加载异常，请在浏览器中打开 Notebook)\n",
    "![mpe2_simple_adversary](./Illustrations/simple_adversary.gif)\n",
    "\n",
    "- 该环境中有 1 个红色的对抗智能体（adversary）、$N$ 个蓝色的正常智能体，以及 $N$ 个地点（一般 $N=2$）\n",
    "- 每个智能体的观察空间都为其他智能体的位置以及地点的位置\n",
    "- $N$ 个地点中有一个是目标地点（绿色），正常智能体知道哪一个是目标地点，但对抗智能体不知道\n",
    "- 正常智能体是合作关系：它们其中任意一个距离目标地点足够近，则每个正常智能体都能获得相同的奖励\n",
    "- 对抗智能体如果距离目标地点足够近，也能获得奖励，但它需要猜哪一个才是目标地点\n",
    "\n",
    "因此，性能好的正常智能体会趋于分散到不同的坐标点，以此欺骗对抗智能体。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdffb0c6d59d5eab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***测试（两个版本）：***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "178b8d59b10b413c"
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "from mpe2 import simple_adversary_v3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:45.189342600Z",
     "start_time": "2025-09-18T16:20:44.878502400Z"
    }
   },
   "id": "42a4f5c6b14d5aa0"
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "智能体数量: 3\n",
      "[ 0.69437176 -1.4609686   0.023015   -0.97559977  0.51697916 -1.5288411\n",
      "  1.0734879  -0.19491644] 0.0 False False {}\n",
      "AgentName: adversary_0, 动作空间大小: 5\n",
      "AgentName: adversary_0, 观察空间大小: (8,)\n",
      "AgentName: adversary_0, Action: 1\n",
      "[ 0.17739256  0.06787257  0.17739256  0.06787257 -0.4939642   0.5532414\n",
      " -0.51697916  1.5288411   0.55650866  1.3339247 ] 0.0 False False {}\n",
      "AgentName: agent_0, 动作空间大小: 5\n",
      "AgentName: agent_0, 观察空间大小: (10,)\n",
      "AgentName: agent_0, Action: 3\n",
      "[-0.3791161  -1.2660521  -0.3791161  -1.2660521  -1.0504729  -0.78068334\n",
      " -1.0734879   0.19491644 -0.55650866 -1.3339247 ] 0.0 False False {}\n",
      "AgentName: agent_1, 动作空间大小: 5\n",
      "AgentName: agent_1, 观察空间大小: (10,)\n",
      "AgentName: agent_1, Action: 2\n",
      "[np.int64(5), np.int64(5), np.int64(5)] [8, 10, 10] 43\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.env(N=2, max_cycles=1, continuous_actions=False, render_mode=\"human\", dynamic_rescaling=True)\n",
    "env.reset(seed=42)\n",
    "print('智能体数量:', len(env.agents))\n",
    "state_dims = []\n",
    "action_dims = []\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "    \n",
    "    if termination or truncation:\n",
    "        break\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "        \n",
    "        action_space = env.action_space(agent)\n",
    "        observation_space = env.observation_space(agent)\n",
    "        print(observation, reward, termination, truncation, info)\n",
    "        print(\"AgentName: {}, 动作空间大小: {}\".format(agent, action_space.n))\n",
    "        print(\"AgentName: {}, 观察空间大小: {}\".format(agent, observation_space.shape))\n",
    "        print(\"AgentName: {}, Action: {}\".format(agent, action))\n",
    "        action_dims.append(action_space.n)  # 'n'表示离散动作空间的大小\n",
    "        state_dims.append(observation_space.shape[0])\n",
    "        env.step(action)\n",
    "\n",
    "\n",
    "critic_input_dim = sum(state_dims) + sum(action_dims)\n",
    "print(action_dims, state_dims, critic_input_dim)\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:45.463782900Z",
     "start_time": "2025-09-18T16:20:44.890581100Z"
    }
   },
   "id": "9dd2f61a04bb0a8"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adversary_0': np.int64(1), 'agent_0': np.int64(4), 'agent_1': np.int64(1)}\n",
      "{'adversary_0': array([-0.89341164,  0.38111728,  0.5119528 ,  0.4140471 ,  0.27848193,\n",
      "        0.17952721, -0.904031  ,  0.92039686], dtype=float32), 'agent_0': array([ 0.23347086,  0.2345199 , -1.1718936 ,  0.20159009,  0.23347086,\n",
      "        0.2345199 , -0.27848193, -0.17952721, -1.1825129 ,  0.74086964],\n",
      "      dtype=float32), 'agent_1': array([ 1.4159838 , -0.50634974,  0.01061934, -0.5392796 ,  1.4159838 ,\n",
      "       -0.50634974,  0.904031  , -0.92039686,  1.1825129 , -0.74086964],\n",
      "      dtype=float32)} defaultdict(<class 'int'>, {'adversary_0': -0.6584304529750572, 'agent_0': 0.3275101821483954, 'agent_1': 0.3275101821483954}) {'adversary_0': False, 'agent_0': False, 'agent_1': False} {'adversary_0': True, 'agent_0': True, 'agent_1': True} {'adversary_0': {}, 'agent_0': {}, 'agent_1': {}}\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.parallel_env(N=2, max_cycles=1, continuous_actions=False, render_mode=\"human\", dynamic_rescaling=True)\n",
    "observations, infos = env.reset()\n",
    "\n",
    "while env.agents:\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "    print(actions)\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    print(observations, rewards, terminations, truncations, infos)\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:46.027886800Z",
     "start_time": "2025-09-18T16:20:45.329338500Z"
    }
   },
   "id": "eae576faa71c1356"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. 采样动作不可导问题：离散动作空间需要从概率分布中进行采样来选择动作，采样操作不可导，这会导致无法使用反向传播来优化模型（[复习SAC中使用的重参数化技巧（reparameterization trick）](../RL_Classic_Algorithms/17.1_SAC算法原理.ipynb)）\n",
    "> **DDPG** 算法通过 **确定性策略** 针对 **连续动作空间**，使智能体的动作对于其 **策略参数$\\mu_\\theta$** 可导\n",
    "> 但 **MPE** 环境中 **默认** 每个智能体的 **动作空间是离散的**，虽然依旧是 **确定性策略**，但是需要对网络输出进行 **离散分布的采样**，这种采样是 **不可导的**，此时就需要运用之前的 **重参数化方法（这里要用的是 Gumbel-Softmax 技巧）** 让离散分布的采样可导：\n",
    "\n",
    "**Gumbel-Softmax 变换：**\n",
    "1. Gumbel 随机变量的采样（重参数因子$g_i$）\n",
    "$$g_i=-\\log(-\\log u),u\\sim\\mathrm{Uniform}(0,1)$$\n",
    "2. 加入 Gumbel 噪声并进行 softmax\n",
    "$$\\tilde{y}_i=\\frac{\\exp\\left(\\frac{\\log(\\pi_i)+g_i}{\\tau}\\right)}{\\sum_j\\exp\\left(\\frac{\\log(\\pi_j)+g_j}{\\tau}\\right)}$$\n",
    "- $\\tau$ 是温度参数，用来控制离散度，温度越低，近似越接近离散采样\n",
    "- 引入 Gumbel 随机变量和温度参数$\\tau$，得到一个平滑的概率分布，使得模型的输出变得可微\n",
    "- 将原本的离散采样过程（不可导）转化为连续的近似，这样，就能利用反向传播来优化模型\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "986381fe87eed7f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Gumbel Softmax 采样的相关函数："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16139cd4f1d9db6a"
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "def onehot_from_logits(logits, eps=0.01):\n",
    "    \"\"\" 独热（one-hot）离散化 \"\"\"\n",
    "    # 生成最优动作的独热形式\n",
    "    argmax_acs = (logits == logits.max(1, keepdim=True)[0]).float()\n",
    "    # 生成随机动作的独热形式\n",
    "    rand_acs = torch.autograd.Variable(torch.eye(logits.shape[1])[[\n",
    "        np.random.choice(range(logits.shape[1]), size=logits.shape[0])\n",
    "    ]], requires_grad=False).to(logits.device)\n",
    "    \n",
    "    # 通过epsilon-贪婪算法来选择用哪个动作\n",
    "    return torch.stack([\n",
    "        argmax_acs[i] if r > eps else rand_acs[i]\n",
    "        for i, r in enumerate(torch.rand(logits.shape[0]))\n",
    "    ])\n",
    "\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20, tens_type=torch.FloatTensor):\n",
    "    \"\"\" 1.Gumbel随机变量的采样 \"\"\"\n",
    "    U = torch.autograd.Variable(tens_type(*shape).uniform_(), requires_grad=False)\n",
    "    return -torch.log(-torch.log(U + eps) + eps)\n",
    "\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    \"\"\" 2.加入Gumbel噪声并进行softmax \"\"\"\n",
    "    y = logits + sample_gumbel(logits.shape, tens_type=type(logits.data)).to(logits.device)\n",
    "    return F.softmax(y / temperature, dim=1)\n",
    "\n",
    "\n",
    "def gumbel_softmax(logits, temperature=1.0):\n",
    "    \"\"\" 从Gumbel-Softmax分布中采样,并进行离散化 \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    y_hard = onehot_from_logits(y)\n",
    "    y = (y_hard.to(logits.device) - y).detach() + y  # 使硬选择部分y_hard与模型的训练过程断开连接,但又保证在训练中通过软选择y反传梯度对模型进行优化\n",
    "    return y  # 返回的值是独热量y_hard,但是它的梯度是y,既能够得到一个与环境交互的离散动作,又可以正确地反传梯度"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:46.062188200Z",
     "start_time": "2025-09-18T16:20:46.036428300Z"
    }
   },
   "id": "aa6bf15fcc8d09ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 注意： \n",
    "在最新的 **MPE** 环境中可以设置每个智能体的 **动作空间为连续**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7386a8271900609"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. 每个智能体：单智能体 DDPG\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62c8ed81a793ef27"
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "# 合并定义策略网络和价值网络\n",
    "class TwoLayerFC(torch.nn.Module):\n",
    "    def __init__(self, num_in, num_out, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(num_in, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, num_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "    \"\"\" DDPG算法 \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, critic_input_dim, hidden_dim, actor_lr, critic_lr, device):\n",
    "        self.actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_actor = TwoLayerFC(state_dim, action_dim, hidden_dim).to(device)\n",
    "        \n",
    "        self.critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        self.target_critic = TwoLayerFC(critic_input_dim, 1, hidden_dim).to(device)\n",
    "        \n",
    "        # 初始化目标网络并设置和主网络相同的参数\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    # 重参数化方法\n",
    "    def take_action(self, state, explore=False):\n",
    "        action = self.actor(state)\n",
    "        if explore:\n",
    "            action = gumbel_softmax(action)\n",
    "        else:\n",
    "            action = onehot_from_logits(action)\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "    \n",
    "    # 软更新\n",
    "    def soft_update(self, net, target_net, tau):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - tau) + param.data * tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:46.100784900Z",
     "start_time": "2025-09-18T16:20:46.058865900Z"
    }
   },
   "id": "5196b8aaeb8c867d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. MADDPG 类：维护每个智能体\n",
    "> 每个智能体都有自己的 actor 网络，但它们的 critic 网络 是共享的"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8c2d5501e778e4d"
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "class MADDPG:\n",
    "    def __init__(self, env, device, actor_lr, critic_lr, hidden_dim,\n",
    "                 state_dims, action_dims, critic_input_dim, gamma, tau):\n",
    "        self.agents = []\n",
    "        for i in range(len(env.agents)):  # 3\n",
    "            self.agents.append(\n",
    "                DDPG(state_dims[i], action_dims[i], critic_input_dim,\n",
    "                     hidden_dim, actor_lr, critic_lr, device))\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "    @property\n",
    "    def policies(self):\n",
    "        # 获取所有智能体的 actor 网络\n",
    "        return [agt.actor for agt in self.agents]\n",
    "\n",
    "    @property\n",
    "    def target_policies(self):\n",
    "        # 获取所有智能体的 target_actor 网络\n",
    "        return [agt.target_actor for agt in self.agents]\n",
    "\n",
    "    # 多智能体执行动作\n",
    "    def take_action(self, states, explore):\n",
    "        tensor_states = {\n",
    "            name: torch.tensor(\n",
    "                np.array([states[name]]),\n",
    "                dtype=torch.float,\n",
    "                device=self.device\n",
    "            )\n",
    "            for name in env.agents\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            name: agent.take_action(tensor_states[name], explore)\n",
    "            for name, agent in zip(env.agents, self.agents)\n",
    "        }\n",
    "\n",
    "    # 策略网络更新和评估网络更新\n",
    "    def update(self, sample, i_agent):\n",
    "        obs, act, rew, next_obs, done = sample\n",
    "        cur_agent = self.agents[i_agent]\n",
    "        \n",
    "        # 评估网络更新\n",
    "        cur_agent.critic_optimizer.zero_grad()\n",
    "        all_target_act = [\n",
    "            onehot_from_logits(pi(_next_obs))\n",
    "            for pi, _next_obs in zip(self.target_policies, next_obs)\n",
    "        ]  # 计算目标评估网络所选择的动作\n",
    "        target_critic_input = torch.cat((*next_obs, *all_target_act), dim=1)\n",
    "        target_critic_value = (rew[i_agent].view(-1, 1) +\n",
    "                               self.gamma * cur_agent.target_critic(target_critic_input) * (1 - done[i_agent].view(-1, 1)))\n",
    "        critic_input = torch.cat((*obs, *act), dim=1)\n",
    "        critic_value = cur_agent.critic(critic_input)\n",
    "        # MSE（均方误差）损失函数，用于计算 critic 网络的损失\n",
    "        critic_loss = torch.nn.MSELoss(critic_value, target_critic_value.detach())\n",
    "        critic_loss.backward()\n",
    "        cur_agent.critic_optimizer.step()\n",
    "\n",
    "        # 策略网络更新\n",
    "        cur_agent.actor_optimizer.zero_grad()\n",
    "        cur_actor_out = cur_agent.actor(obs[i_agent])\n",
    "        cur_act_vf_in = gumbel_softmax(cur_actor_out)\n",
    "        all_actor_acs = []  # 包含所有智能体的动作选择\n",
    "        for i, (pi, _obs) in enumerate(zip(self.policies, obs)):\n",
    "            if i == i_agent:\n",
    "                all_actor_acs.append(cur_act_vf_in)\n",
    "            else:\n",
    "                all_actor_acs.append(onehot_from_logits(pi(_obs)))\n",
    "        vf_in = torch.cat((*obs, *all_actor_acs), dim=1)\n",
    "        actor_loss = -cur_agent.critic(vf_in).mean()  # 目标是最大化 critic 网络的输出\n",
    "        actor_loss += (cur_actor_out**2).mean() * 1e-3  # 为了增加稳定性，防止其输出过大或过小，加上一个 L2 正则化项\n",
    "        actor_loss.backward()\n",
    "        cur_agent.actor_optimizer.step()\n",
    "\n",
    "    def update_all_targets(self):\n",
    "        for agt in self.agents:\n",
    "            agt.soft_update(agt.actor, agt.target_actor, self.tau)\n",
    "            agt.soft_update(agt.critic, agt.target_critic, self.tau)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:46.120787Z",
     "start_time": "2025-09-18T16:20:46.089712Z"
    }
   },
   "id": "8570b0c1b1781d0d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. 测试与训练"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc9c79d6485104c3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 参数设置："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faa4f6db3f599ba7"
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'adversary_0': array([ 0.69437176, -1.4609686 ,  0.023015  , -0.97559977,  0.51697916,\n",
      "       -1.5288411 ,  1.0734879 , -0.19491644], dtype=float32), 'agent_0': array([ 0.17739256,  0.06787257,  0.17739256,  0.06787257, -0.4939642 ,\n",
      "        0.5532414 , -0.51697916,  1.5288411 ,  0.55650866,  1.3339247 ],\n",
      "      dtype=float32), 'agent_1': array([-0.3791161 , -1.2660521 , -0.3791161 , -1.2660521 , -1.0504729 ,\n",
      "       -0.78068334, -1.0734879 ,  0.19491644, -0.55650866, -1.3339247 ],\n",
      "      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "env = simple_adversary_v3.parallel_env(N=2, max_cycles=25, continuous_actions=False, dynamic_rescaling=True)\n",
    "observations, infos = env.reset(seed=42)\n",
    "print(observations)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "actor_lr = 1e-2\n",
    "critic_lr = 1e-2\n",
    "hidden_dim = 64\n",
    "gamma = 0.95\n",
    "tau = 1e-2\n",
    "\n",
    "buffer_size = 100000\n",
    "replay_buffer = replay_buffer.ReplayBuffer(buffer_size)\n",
    "\n",
    "maddpg = MADDPG(env, device, actor_lr, critic_lr, hidden_dim, state_dims,\n",
    "                action_dims, critic_input_dim, gamma, tau)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:46.400938800Z",
     "start_time": "2025-09-18T16:20:46.121295Z"
    }
   },
   "id": "ca09b4dd359de539"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 功能函数："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc06fe9682efba66"
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "def evaluate(maddpg, n_episode=100):\n",
    "    \"\"\"\n",
    "    评估学习好的策略，不进行探索\n",
    "    \"\"\"\n",
    "    env4 = simple_adversary_v3.parallel_env(\n",
    "        N=2, max_cycles=25, continuous_actions=False, dynamic_rescaling=True\n",
    "    )\n",
    "    returns = np.zeros(3, dtype=np.float32)\n",
    "\n",
    "    for _ in range(n_episode):\n",
    "        obs, _ = env4.reset()\n",
    "        while env.agents:  # 25\n",
    "            actions = maddpg.take_action(obs, explore=False)\n",
    "            int_actions = {\n",
    "                name: np.int64(np.argmax(vec))\n",
    "                for name, vec in actions.items()\n",
    "            }\n",
    "            _, rew, _, _, _ = env4.step(int_actions)\n",
    "\n",
    "            # 按照 env4.agents 的顺序取奖励\n",
    "            rew = np.array([rew[name] for name in env4.agents], dtype=np.float32)\n",
    "            returns += rew / n_episode\n",
    "\n",
    "    return returns.tolist()  # 转成 Python 普通列表\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:20:46.411831500Z",
     "start_time": "2025-09-18T16:20:46.406308400Z"
    }
   },
   "id": "c06f61bdbce80fec"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 训练:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "85736dfde75fc773"
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100, [0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ReplayBuffer' object has no attribute 'sample1'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[157], line 29\u001B[0m\n\u001B[0;32m     27\u001B[0m total_step \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m replay_buffer\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m minimal_size \u001B[38;5;129;01mand\u001B[39;00m total_step \u001B[38;5;241m%\u001B[39m update_interval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m---> 29\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[43mreplay_buffer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample1\u001B[49m(batch_size)  \u001B[38;5;66;03m# 1024\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstack_array\u001B[39m(x):\n\u001B[0;32m     32\u001B[0m         \u001B[38;5;66;03m# 转置\u001B[39;00m\n\u001B[0;32m     33\u001B[0m         transposed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mx))\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ReplayBuffer' object has no attribute 'sample1'"
     ]
    }
   ],
   "source": [
    "return_list = []  # 记录每一轮的回报（return）\n",
    "total_step = 0\n",
    "num_episodes = 5000\n",
    "minimal_size = 4000\n",
    "update_interval = 100  # 更新间隔\n",
    "batch_size = 1024  # 训练样本大小\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    states, _ = env.reset()\n",
    "    while env.agents:  # 25\n",
    "        actions = maddpg.take_action(states, explore=True)\n",
    "        int_actions = {\n",
    "            name: np.int64(np.argmax(vec))\n",
    "            for name, vec in actions.items()\n",
    "        }\n",
    "        next_states, rewards, done, truncations, infos = env.step(int_actions)\n",
    "        \n",
    "        s = [states[name] for name in env.agents]\n",
    "        a = [actions[name] for name in env.agents]\n",
    "        r = [rewards[name] for name in env.agents]\n",
    "        s_next = [next_states[name] for name in env.agents]\n",
    "        d = [done[name] for name in env.agents]\n",
    "        replay_buffer.add(s, a, r, s_next, d)\n",
    "        \n",
    "        states = next_states\n",
    "\n",
    "        total_step += 1\n",
    "        if replay_buffer.size() >= minimal_size and total_step % update_interval == 0:\n",
    "            sample = replay_buffer.sample1(batch_size)  # 1024\n",
    "\n",
    "            def stack_array(x):\n",
    "                # 转置\n",
    "                transposed = list(zip(*x))\n",
    "                return [\n",
    "                    torch.as_tensor(np.vstack(group), dtype=torch.float32, device=device)\n",
    "                    for group in transposed\n",
    "                ]\n",
    "\n",
    "            sample = [stack_array(x) for x in sample]\n",
    "            for a_i in range(len(env.agents)):\n",
    "                maddpg.update(sample, a_i)\n",
    "            maddpg.update_all_targets()\n",
    "            \n",
    "    if (i_episode + 1) % 100 == 0:\n",
    "        ep_returns = evaluate(maddpg)\n",
    "        return_list.append(ep_returns)\n",
    "        print(f\"Episode: {i_episode+1}, {ep_returns}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-18T16:21:12.543581Z",
     "start_time": "2025-09-18T16:20:46.418394Z"
    }
   },
   "id": "e0fde686f6739e5f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
