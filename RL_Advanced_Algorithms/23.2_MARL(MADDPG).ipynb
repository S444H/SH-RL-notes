{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 23.2 MARL (MADDPG)\n",
    "> 前文已经介绍了 **多智能体强化学习的基本求解范式：完全中心化 与 完全去中心化**。本节来介绍一种比较经典且效果不错的**进阶范式：中心化训练去中心化执行（centralized training with decentralized execution，CTDE）**：\n",
    "> - **中心化训练**：在训练的时候使用一些单个智能体看不到的 **全局信息** 而以达到更好的训练效果。\n",
    "> - **去中心化执行**：在执行时不使用 **全局信息**，每个智能体完全根据自己的策略执行动作。\n",
    "\n",
    "**CTDE** 可以类比成一个足球队的训练和比赛过程：在训练时，11 个球员可以直接获得教练的指导从而完成球队的整体配合，而教练本身掌握着比赛全局信息，教练的指导也是从整支队、整场比赛的角度进行的；而训练好的 11 个球员在上场比赛时，则根据场上的实时情况直接做出决策，不再有教练的指导。\n",
    "> 所以 **CTDE** 能够在训练时有效地利用全局信息以达到 **更好且更稳定的训练效果**，同时在进行策略模型推断时可以仅利用局部信息，使得算法**具有一定的扩展性**。\n",
    "\n",
    "> **CTDE** 算法主要分为两种：\n",
    "> - 一种是基于值函数的方法，例如 **VDN，QMIX** \n",
    "> - 一种是基于 Actor-Critic 的方法，例如 **MADDPG，COMA** \n",
    "\n",
    "> 本节介绍 **MADDPG** 算法 （[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275)）"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c96e6457b7d49d23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 23.2.1 MADDPG 算法原理"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80dd650cbb2f7cc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 23.2.2 MADDPG 代码实践"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "818b9d2ea2d264c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 关于环境库：\n",
    "同样需新建一个虚拟环境（防止软件包冲突），下载 **MPE2库**（[使用文档](https://mpe2.farama.org/environments/simple_adversary/)），使用其中的 **多智能体粒子环境（multiagent particles environment，MPE）**：\n",
    "\n",
    "![mpe2_simple_adversary](./Illustrations/mpe2_simple_adversary.gif)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdffb0c6d59d5eab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "***测试：***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "178b8d59b10b413c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from mpe2 import simple_adversary_v3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-16T14:48:33.589562700Z",
     "start_time": "2025-09-16T14:48:32.477803400Z"
    }
   },
   "id": "42a4f5c6b14d5aa0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "env = simple_adversary_v3.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-16T14:08:01.762956700Z",
     "start_time": "2025-09-16T14:07:53.855666800Z"
    }
   },
   "id": "9dd2f61a04bb0a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 导入其他库："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be667ef596df92f4"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-16T14:08:13.063757900Z",
     "start_time": "2025-09-16T14:08:08.923024Z"
    }
   },
   "id": "100141c5e29116fe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
