{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a9b858fb1d554d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 12.REINFORCE算法\n",
    "> 1992 年，Ronald J. Williams 的论文[《Simple statistical gradient-following algorithms for connectionist reinforcement learning》](https://link.springer.com/article/10.1007/BF00992696)首次系统地提出了 **基于蒙特卡洛** 回报的策略梯度更新方法：**REINFORCE**，它是现代策略梯度方法的起点，是第一个系统性提出并形式化了策略梯度更新规则的算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239890d14a0851",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**REINFORCE** 算法采用 **蒙特卡洛方法** ，以 **实际的样本回报** $G_t=\\sum_{k=t}^T\\gamma^{k-t}r_k$ 来代替公式推导中的 **长期预期回报** $Q^{\\pi_{\\theta}}(s,a)$ ，对于一个有限步数的环境来说，REINFORCE 算法中的策略梯度表示为：\n",
    "$$\\nabla_\\theta J(\\theta)=\\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=0}^TG_t\\nabla_\\theta\\log\\pi_\\theta(a_t|s_t)\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c036ce4c96d5eb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## REINFORCE 代码实践："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866b99486615cf8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "导入相关库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79add9ba12b04702",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:33:00.063762600Z",
     "start_time": "2025-07-24T11:33:00.023422300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 基本库\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils.smoothing import moving_average\n",
    "# 神经网络\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Gymnasium 是一个用于开发和测试强化学习算法的工具库，为 OpenAI Gym 的更新版本（2021迁移开发）\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f87c760d5a4ff",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "定义策略网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781807bf1a62559c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:33:00.075281400Z",
     "start_time": "2025-07-24T11:33:00.030120800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)  # 输出通过 softmax 激活函数转化为一个概率分布，表示每个动作的选择概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92aa56c24d4572c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "定义 REINFORCE 算法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93dea439835d5029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:33:00.109362200Z",
     "start_time": "2025-07-24T11:33:00.039086500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, device):\n",
    "        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)  # 使用Adam优化器\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):  # 根据动作概率分布随机采样\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        probs = self.policy_net(state)  # 输出一个概率分布\n",
    "        action_dist = torch.distributions.Categorical(probs)  # 创建一个 Categorical 分布，方便从中采样\n",
    "        action = action_dist.sample()  # 从概率分布中随机采样一个动作的索引\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        reward_list = transition_dict['rewards']\n",
    "        state_list = transition_dict['states']\n",
    "        action_list = transition_dict['actions']\n",
    "        G = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in reversed(range(len(reward_list))):  # 从最后一步算起\n",
    "            reward = reward_list[i]\n",
    "            state = torch.tensor(np.array([state_list[i]]), dtype=torch.float).to(self.device)\n",
    "            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)\n",
    "            log_prob = torch.log(self.policy_net(state).gather(1, action))\n",
    "            G = self.gamma * G + reward\n",
    "            loss = -log_prob * G  # 每一步的损失函数\n",
    "            loss.backward()  # 反向传播计算梯度\n",
    "        self.optimizer.step()  # 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dafc9269cd9a78",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "环境设置（'CartPole-v1'）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "433ba5e39a6b57ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:33:00.111362Z",
     "start_time": "2025-07-24T11:33:00.047908800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment spec: EnvSpec(id='CartPole-v1', entry_point='gymnasium.envs.classic_control.cartpole:CartPoleEnv', reward_threshold=475.0, nondeterministic=False, max_episode_steps=500, order_enforce=True, disable_env_checker=False, kwargs={}, namespace=None, name='CartPole', version=1, additional_wrappers=(), vector_entry_point='gymnasium.envs.classic_control.cartpole:CartPoleVectorEnv')\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)    # 设置 NumPy 的随机种子\n",
    "torch.manual_seed(0) # 设置 PyTorch CPU 随机种子\n",
    "torch.cuda.manual_seed_all(0) # 设置 PyTorch GPU 随机种子, 由于GPU并行性, 只能极大减小偏差\n",
    "\n",
    "env = gym.make('CartPole-v1')  # CartPole-v1 最大回合步数修改到了500步(v0为200)\n",
    "#env = env.unwrapped # 获取原始环境（绕过 TimeLimit 包装器）解除最大步数500限制\n",
    "env.reset(seed=0)   # 环境通常依赖于其他随机数生成器来初始化状态、进行探索(推荐位于以上随机之后)\n",
    "print(\"Environment spec:\", env.spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369c3c4026b7ca69",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "超参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "614215f8910f90fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T11:33:02.856936200Z",
     "start_time": "2025-07-24T11:33:00.058754400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "hidden_dim = 128\n",
    "action_dim = env.action_space.n\n",
    "learning_rate = 1e-3\n",
    "gamma = 0.98\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device( \"cpu\")\n",
    "agent = REINFORCE(state_dim, hidden_dim, action_dim, learning_rate, gamma, device)\n",
    "\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b399e409d0606",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "测试与训练:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d31d56910e60f9da",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86191\\AppData\\Local\\Temp\\ipykernel_18196\\591963734.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:257.)\n",
      "  state = torch.tensor([state_list[i]],\n",
      "Iteration 0: 100%|██████████| 100/100 [00:09<00:00, 10.61it/s, episode=100, return=39.500]\n",
      "Iteration 1: 100%|██████████| 100/100 [00:20<00:00,  4.88it/s, episode=200, return=94.000]\n",
      "Iteration 2: 100%|██████████| 100/100 [00:42<00:00,  2.34it/s, episode=300, return=55.300]\n",
      "Iteration 3: 100%|██████████| 100/100 [00:47<00:00,  2.11it/s, episode=400, return=129.600]\n",
      "Iteration 4:  26%|██▌       | 26/100 [00:19<00:56,  1.30it/s, episode=420, return=260.800]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m     15\u001B[39m truncated = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     16\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (done \u001B[38;5;129;01mor\u001B[39;00m truncated):  \u001B[38;5;66;03m# 杆子倒下或达到最大步数\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m     action = agent.take_action(state)\n\u001B[32m     18\u001B[39m     next_state, reward, done, truncated, _ = env.step(action)  \u001B[38;5;66;03m# Gymnasium返回值不一样\u001B[39;00m\n\u001B[32m     19\u001B[39m     transition_dict[\u001B[33m'\u001B[39m\u001B[33mstates\u001B[39m\u001B[33m'\u001B[39m].append(state)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 12\u001B[39m, in \u001B[36mREINFORCE.take_action\u001B[39m\u001B[34m(self, state)\u001B[39m\n\u001B[32m     10\u001B[39m probs = \u001B[38;5;28mself\u001B[39m.policy_net(state)  \u001B[38;5;66;03m# 输出一个概率分布\u001B[39;00m\n\u001B[32m     11\u001B[39m action_dist = torch.distributions.Categorical(probs)  \u001B[38;5;66;03m# 创建一个 Categorical 分布，方便从中采样\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m12\u001B[39m action = action_dist.sample()  \u001B[38;5;66;03m# 从概率分布中随机采样一个动作的索引\u001B[39;00m\n\u001B[32m     13\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m action.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\python\\Anaconda3\\envs\\HRL2\\Lib\\site-packages\\torch\\distributions\\categorical.py:135\u001B[39m, in \u001B[36mCategorical.sample\u001B[39m\u001B[34m(self, sample_shape)\u001B[39m\n\u001B[32m    133\u001B[39m     sample_shape = torch.Size(sample_shape)\n\u001B[32m    134\u001B[39m probs_2d = \u001B[38;5;28mself\u001B[39m.probs.reshape(-\u001B[32m1\u001B[39m, \u001B[38;5;28mself\u001B[39m._num_events)\n\u001B[32m--> \u001B[39m\u001B[32m135\u001B[39m samples_2d = torch.multinomial(probs_2d, sample_shape.numel(), \u001B[38;5;28;01mTrue\u001B[39;00m).T\n\u001B[32m    136\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m samples_2d.reshape(\u001B[38;5;28mself\u001B[39m._extended_shape(sample_shape))\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            episode_return = 0\n",
    "            transition_dict = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            state, info = env.reset()  # 测试阶段(调整参数与对比算法)种子应固定; 训练阶段不固定，提高泛化能力\n",
    "            done = False\n",
    "            truncated = False\n",
    "            while not (done or truncated):  # 杆子倒下或达到最大步数\n",
    "                action = agent.take_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)  # Gymnasium返回值不一样\n",
    "                transition_dict['states'].append(state)\n",
    "                transition_dict['actions'].append(action)\n",
    "                transition_dict['next_states'].append(next_state)\n",
    "                transition_dict['rewards'].append(reward)\n",
    "                transition_dict['dones'].append(done)\n",
    "                state = next_state\n",
    "                episode_return += reward\n",
    "            return_list.append(episode_return)\n",
    "            agent.update(transition_dict)\n",
    "            \n",
    "            if (i_episode+1) % 10 == 0:\n",
    "                pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f521f8936b43ad21",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "绘图："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab73405fb17e498",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "episodes_list = list(range(len(return_list)))\n",
    "mv_return = moving_average(return_list, 9)\n",
    "# 创建 DataFrame\n",
    "df1 = pd.DataFrame({'Episodes': episodes_list, 'Returns': return_list})\n",
    "df2 = pd.DataFrame({'Episodes': episodes_list, 'Returns': mv_return})\n",
    "# 保存为 CSV 文件\n",
    "df1.to_csv('REINFORCE_returns_data.csv', index=False)\n",
    "df2.to_csv('REINFORCE_mv_returns_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390d6a0093f4b8a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "df = pd.read_csv('REINFORCE_returns_data.csv')  # 从 CSV 文件中读取数据\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['Episodes'], y=df['Returns'], mode='lines', name='Returns'))\n",
    "fig.update_layout(\n",
    "    title='REINFORCE on CartPole-v1',\n",
    "    xaxis_title='Episodes',\n",
    "    yaxis_title='Returns',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd735feede9f21c3",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "df = pd.read_csv('REINFORCE_mv_returns_data.csv')  # 从 CSV 文件中读取数据\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['Episodes'], y=df['Returns'], mode='lines', name='Returns'))\n",
    "fig.update_layout(\n",
    "    title='REINFORCE on CartPole-v1',\n",
    "    xaxis_title='Episodes',\n",
    "    yaxis_title='Returns',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f3a6bb45b9410",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
