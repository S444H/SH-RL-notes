{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 12.REINFORCE算法\n",
    "> 1992 年，Ronald J. Williams 的论文[《Simple statistical gradient-following algorithms for connectionist reinforcement learning》](https://link.springer.com/article/10.1007/BF00992696)首次系统地提出了 **基于蒙特卡洛** 回报的策略梯度更新方法：**REINFORCE**，它是现代策略梯度方法的起点，是第一个系统性提出并形式化了策略梯度更新规则的算法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68a9b858fb1d554d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**REINFORCE** 算法采用 **蒙特卡洛方法** 来估计 $Q^{\\pi_{\\theta}}(s,a)$ ，对于一个有限步数的环境来说，REINFORCE 算法中的策略梯度为："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2239890d14a0851"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## REINFORCE 代码实践："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9c036ce4c96d5eb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入相关库："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e866b99486615cf8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 基本库\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\"\"\"在绘图时引入,防止绘图时失效\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "# 神经网络\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Gymnasium 是一个用于开发和测试强化学习算法的工具库，为 OpenAI Gym 的更新版本（2021迁移开发）\n",
    "import gymnasium as gym"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79add9ba12b04702"
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义策略网络："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c48f87c760d5a4ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)  # 输出通过 softmax 激活函数转化为一个概率分布，表示每个动作的选择概率"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "781807bf1a62559c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义 REINFORCE 算法："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f92aa56c24d4572c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, device):\n",
    "        self.policy_net = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=learning_rate)  # 使用Adam优化器\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):  # 根据动作概率分布随机采样\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        probs = self.policy_net(state)  # 输出一个概率分布\n",
    "        action_dist = torch.distributions.Categorical(probs)  # 创建一个 Categorical 分布，方便从中采样\n",
    "        action = action_dist.sample()  # 从概率分布中随机采样一个动作的索引\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        reward_list = transition_dict['rewards']\n",
    "        state_list = transition_dict['states']\n",
    "        action_list = transition_dict['actions']\n",
    "        G = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in reversed(range(len(reward_list))):  # 从最后一步算起\n",
    "            reward = reward_list[i]\n",
    "            state = torch.tensor([state_list[i]],\n",
    "                                 dtype=torch.float).to(self.device)\n",
    "            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.device)\n",
    "            log_prob = torch.log(self.policy_net(state).gather(1, action))\n",
    "            G = self.gamma * G + reward\n",
    "            loss = -log_prob * G  # 每一步的损失函数\n",
    "            loss.backward()  # 反向传播计算梯度\n",
    "        self.optimizer.step()  # 梯度下降"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93dea439835d5029"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "902f3a6bb45b9410"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
