{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 14.TRPO （Trust Region Policy Optimization）：信任区域策略优化\n",
    "> 即使 **Actor-Critic** 在一定程度上能够降低训练过程中的方差，在实际应用过程中仍与传统策略梯度算法一样存在着 **训练不稳定** 的情况。根本原因在于**策略更新过大**：网络参数对样本传递的环境信息 **“过度拟合”**，导致训练时全局策略变差，造成训练的不稳定。特别是当面临情况复杂，策略网络是深度模型时，沿着策略梯度更新参数，步长太长，在一段时间内陷入对部分样本的过分信任，会导致全局策略突然显著变差。\n",
    "\n",
    "> **TRPO（Trust Region Policy Optimization）算法** 最初由 John Schulman 等人于2015年在 [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) 中提出。\n",
    "> 论文引入了 **信任区域（trust region）和KL散度约束** 的概念。在 **信任区域** 上更新策略时能够得到某种策略性能的安全性保证，是其主要思想。**TRPO** 描述了一种优化策略的迭代过程：在理论上能够保证策略学习的 **性能单调性**，并在实际应用中取得了比策略梯度算法更好的效果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14f3a280b466199d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 14.1 TRPO原理\n",
    "![TRPO原理示意图](Illustrations/TRPO原理示意图.png)\n",
    "- 左图表示当**完全不设置信任区域**时，传统策略的梯度更新可能导致策略的性能骤降\n",
    "- 右图表示当**设置了信任区域** 时，（理论上）可以保证每次策略的梯度更新（尽可能）带来性能的提升"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "809d775d79827f92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14.1.1 目标函数\n",
    "> **TRPO** 对目标函数进行了变化："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d1b38dc041a542f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 无论策略为什么，采样一条轨迹的**初始状态$s_0$的分布$p(s_0)$固定和策略$\\pi$无关**（实际实验中也规定如此），也就是说:\n",
    "$$\\begin{aligned}\n",
    "J(\\theta) & =\\mathbb{E}_{\\pi}[V^{\\pi_\\theta}(s_0)] =\\sum_{s_0}p(s_0)V^{\\pi_\\theta}(s_0) \\\\\n",
    " & =\\mathbb{E}_{\\pi_{\\theta^{\\prime}}}[V^{\\pi_\\theta}(s_0)] \\\\\n",
    " & =\\mathbb{E}_{\\pi_{\\theta^{\\prime}}}\\left[\\sum_{t=0}^\\infty\\gamma^tV^{\\pi_\\theta}(s_t)-\\sum_{t=1}^\\infty\\gamma^tV^{\\pi_\\theta}(s_t)\\right] \\\\\n",
    " & =-\\mathbb{E}_{\\pi_{\\theta^{\\prime}}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}\\left(\\gamma V^{\\pi_{\\theta}}(s_{t+1})-V^{\\pi_{\\theta}}(s_{t})\\right)\\right]\n",
    "\\end{aligned}$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2474877200ec1f2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 基于以上等式，可以推导新旧策略的目标函数之间的差距：\n",
    "$$\\begin{aligned}\n",
    "J(\\theta^{\\prime})-J(\\theta) & =\\mathbb{E}_{\\pi}\\left[V^{\\pi_{\\theta^{\\prime}}}(s_0)\\right]-\\mathbb{E}_{\\pi}\\left[V^{\\pi_\\theta}(s_0)\\right] \\\\\n",
    " & =\\mathbb{E}_{\\pi_{\\theta^{\\prime}}}\\left[\\sum_{t=0}^\\infty\\gamma^tr(s_t,a_t)\\right]+\\mathbb{E}_{\\pi_{\\theta^{\\prime}}}\\left[\\sum_{t=0}^\\infty\\gamma^t\\left(\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_t)\\right)\\right] \\\\\n",
    " & =\\mathbb{E}_{\\pi_{\\theta^{\\prime}}}\\left[\\sum_{t=0}^\\infty\\gamma^t\\left[r(s_t,a_t)+\\gamma V^{\\pi_\\theta}(s_{t+1})-V^{\\pi_\\theta}(s_t)\\right]\\right]\n",
    "\\end{aligned}$$\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23d72c29cf3cc7cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 可见上述公式存在 **时序差分残差** 形式，以下转换成**优势函数A**：\n",
    "$$J(\\theta^{\\prime})-J(\\theta)=\\mathbb{E}_{\\pi_{\\theta^{\\prime}}}\\left[\\sum_{t=0}^\\infty\\gamma^tA^{\\pi_\\theta}(s_t,a_t)\\right]$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e90f984f9e66950"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 展开期望形式：\n",
    "$$J(\\theta^{\\prime})-J(\\theta)=\\sum_\\tau\\left[p(\\tau|\\pi_{\\theta^{\\prime}})\\sum_{t=0}^\\infty\\gamma^tA^{\\pi_\\theta}(s_t,a_t)\\right]$$\n",
    "- 轨迹概率：$p(\\tau|\\theta)=p(s_0)\\prod_{t=0}^{T}[\\pi_\\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)]$ \n",
    "> 又状态访问分布:$\\nu^\\pi(s)=(1-\\gamma)\\sum_{t=0}^\\infty\\gamma^tP(s_t = s, a_t = a \\| \\pi)$，转换为状态访问分布表达形式：\n",
    "$$\n",
    "J(\\theta^{\\prime})-J(\\theta) = \\frac{1}{1 - \\gamma} \\sum_{s} \\left[ \\nu^{\\pi_{\\theta'}}(s) \\sum_{a} \\left[ \\pi_{\\theta'}(a | s)   A^{\\pi_\\theta}(s, a)  \\right] \\right]\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7059c1b4bb7f3f3d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 所以只要保证:\n",
    "$$\\sum_{s} \\left[ \\nu^{\\pi_{\\theta'}}(s) \\sum_{a} \\left[ \\pi_{\\theta'}(a | s)   A^{\\pi_\\theta}(s, a)  \\right] \\right] \\geq 0$$\n",
    "> 就能保证策略性能的单调递增。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db3205a678e993c7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "*但是把所有可能的**新策略**都拿来采样数据去**获取状态访问分布**，然后判断哪个**新策略**满足上述条件的做法显然是**不现实**的*\n",
    "> **TRPO** 做了一步近似操作，**忽略**两个策略之间的状态访问分布变化，直接**采用旧的策略的状态分布$\\nu^{\\pi_{\\theta}}(s)$**：\n",
    "$$J(\\theta^{\\prime})-J(\\theta) = \\frac{1}{1 - \\gamma} \\sum_{s} \\left[ \\nu^{\\pi_{\\theta}}(s) \\sum_{a} \\left[ \\pi_{\\theta'}(a | s)   A^{\\pi_\\theta}(s, a)  \\right] \\right]$$\n",
    "\n",
    "- 当新旧策略非常接近时，状态访问分布变化很小，这么近似是合理的\n",
    "> 如此再定义**优化目标**:\n",
    "$$\\begin{aligned}\n",
    "L_\\theta(\\theta^{\\prime})& =J(\\theta)+\\frac{1}{1 - \\gamma} \\sum_{s} \\left[ \\nu^{\\pi_{\\theta}}(s) \\sum_{a} \\left[ \\pi_{\\theta'}(a | s)   A^{\\pi_\\theta}(s, a)  \\right] \\right] \\\\\n",
    "& =J(\\theta)+\\frac{1}{1 - \\gamma} \\sum_{s} \\left[ \\nu^{\\pi_{\\theta}}(s) \\sum_{a}\\pi_{\\theta}(a | s) \\left[    \\frac{\\pi_{\\theta^{\\prime}}(a|s)}{\\pi_\\theta(a|s)}A^{\\pi_\\theta}(s,a) \\right] \\right] \\\\\n",
    "& =J(\\theta)+\\mathbb{E}_{s\\sim\\nu^{\\pi_\\theta}}\\mathbb{E}_{a\\sim\\pi_\\theta(\\cdot|s)}\\left[\\frac{\\pi_{\\theta^{\\prime}}(a|s)}{\\pi_\\theta(a|s)}A^{\\pi_\\theta}(s,a)\\right]\n",
    "\\end{aligned}$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afddce9bef9804b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14.1.2 约束"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "286f87dc4c9d7c0e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Kullback-Leibler (KL) 散度：用于衡量一个概率分布相对于另一个分布的信息丧失\n",
    "> 给定两个概率分布 $\\( P \\)$ 和 $\\( Q \\)$，其中 $\\( P \\)$ 通常代表真实分布（或“目标分布”），$\\( Q \\)$ 代表近似分布（或“预测分布”）\n",
    "**KL 散度**衡量了从分布 $\\( Q \\)$ 生成数据时，丢失多少来自真实分布 $\\( P \\)$ 的信息。KL 散度越小，表示两个分布越相似；如果 $\\( P \\)$ 和 $\\( Q \\)$ 完全相同，则 KL 散度为零。\n",
    "\n",
    "> 对于离散型概率分布：\n",
    "$$\n",
    "\\[\n",
    "\\text{KL}(P || Q) = \\sum_{x} P(x) \\log \\left( \\frac{P(x)}{Q(x)} \\right)\n",
    "\\]\n",
    "$$\n",
    "> 对于连续型概率分布：\n",
    "$$\n",
    "\\[\n",
    "\\text{KL}(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\left( \\frac{p(x)}{q(x)} \\right) dx\n",
    "\\]\n",
    "$$\n",
    "- $\\( P(x) \\)$ 是真实分布的概率密度函数；\n",
    "- $\\( Q(x) \\)$ 是近似分布的概率密度函数；\n",
    "\n",
    "##### 主要性质：\n",
    "1. 非负性：KL 散度总是非负的：只有当 $\\( P = Q \\)$ 时，KL 散度才为零。\n",
    "   $$\\[\n",
    "   \\text{KL}(P || Q) \\geq 0\n",
    "   \\]$$\n",
    "2. 非对称性：KL 散度是非对称的：这意味着 KL 散度不能当作一个真正的“距离”度量。\n",
    "   $$\\[\n",
    "   \\text{KL}(P || Q) \\neq \\text{KL}(Q || P)\n",
    "   \\]$$\n",
    "3. 可加性：如果将一个复杂问题分解为多个子问题，KL 散度是可加的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef903f676df3cb21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "在 **TRPO** 中，**KL 散度** 用于限制每次策略更新的幅度，确保新的策略与旧策略之间的差异不会太大，从而保持优化过程的稳定性。具体来说，**TRPO** 在每次更新策略参数时加入一个基于 **KL 散度** 的 **约束** ：\n",
    "$$\n",
    "\\[\n",
    "\\sum_s\\nu^{\\pi_\\theta}(s)\\mathrm{KL}\\left[\\pi_\\theta(\\cdot|s)||\\pi_{\\theta^{\\prime}}(\\cdot|s)\\right]=\\mathbb{E}_{s\\sim\\nu^{\\pi_\\theta}} \\left[ \\text{KL} \\left( \\pi_\\theta(\\cdot | s) || \\pi_{\\theta'}(\\cdot | s) \\right) \\right] \\leq \\delta\n",
    "\\]\n",
    "$$\n",
    "\n",
    "> 这里的**不等式约束** 定义了策略空间中的一个 **\"KL 球\"**，被称为 **信任区域**。在这个区域中，可以认为当前学习策略和环境交互的状态分布与上一轮策略最后采样的状态 **分布一致**，进而可以使当前学习策略稳定提升。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b5c18dbcc710a71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14.1.3 近似求解\n",
    "> 直接求解上式**带约束的优化问题**比较麻烦，**TRPO** 在其具体实现中做了一步近似操作来快速求解："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d063a75beb00550"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 用$\\theta_{k}$代替之前的$\\theta$，表示这是第k次迭代之后的策略，然后在 **$\\theta_{k}$处** 对上述的 **目标函数、约束** 进行泰勒展开:\n",
    "- 一阶（**梯度Gradient**，给出斜率信息，指导更新的方向）：$\\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_k}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta^{\\prime}}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]\\approx grad^T(\\theta^{\\prime}-\\theta_k)$\n",
    "- 二阶（**海森矩阵Hessian Matrix**，给出曲率信息，指导更新的稳定）：$\\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_k}}}[D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta^{\\prime}}(\\cdot|s))]\\approx\\frac{1}{2}(\\theta^{\\prime}-\\theta_k)^TH(\\theta^{\\prime}-\\theta_k)$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78ef060e596fb622"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 于是 **优化目标** 变为:\n",
    "$$\\theta_{k+1}=\\arg\\max_{\\theta^{\\prime}}g^T(\\theta^{\\prime}-\\theta_k)$$\n",
    "$$\\frac{1}{2}(\\theta^{\\prime}-\\theta_k)^TH(\\theta^{\\prime}-\\theta_k)\\leq\\delta$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5941f1d310b7cf81"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 再根据 **（Karush-Kuhn-Tucker，KKT）条件**（[详见](https://github.com/S444H/Theta/blob/main/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95%E4%B8%8EKKT%E6%9D%A1%E4%BB%B6.ipynb)）：提供了一种求解最优解的有效途径。可直接导出上述问题的解：\n",
    "$$\\theta_{k+1}=\\theta_k+\\sqrt{\\frac{2\\delta}{g^TH^{-1}g}}H^{-1}g$$\n",
    "- 其中$H^{-1}g$为 **自然梯度**，最初来源于Shun-Ichiro Amari在 1998 年的论文[Natural Gradient Works Efficiently in Learning](https://ieeexplore.ieee.org/document/6790500)中提出的 **自然梯度法（Natural Gradient Method）**\n",
    "> 设$x=H^{-1}g$，根据矩阵性质可得 **参数更新方式**：\n",
    "$$\\theta_{k+1}=\\theta_k+\\sqrt{\\frac{2\\delta}{x^THx}}x$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e35b02170d1abb19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 另一种角度：\n",
    "> 为了更快到达参数最优，假设满足 KL 距离约束的参数更新时的 **最大步长为$\\beta$**，则有：\n",
    "$$\\frac{1}{2}(\\beta x)^{T}H(\\beta x)=\\delta$$\n",
    "> 求解\n",
    "$$\\beta=\\sqrt{\\frac{2\\delta}{x^{T}Hx}}$$\n",
    "> 则 **参数更新方式** 亦为：\n",
    "$$\\theta_{k+1}=\\theta_k+\\sqrt{\\frac{2\\delta}{x^THx}}x$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e958f1d99a121908"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 共轭梯度法（Conjugate Gradient Method, CG）（[详见](https://github.com/S444H/Theta/blob/main/%E5%85%B1%E8%BD%AD%E6%A2%AF%E5%BA%A6%E6%B3%95(Conjugate%20Gradient%20Method).ipynb)）：用于高效求解对称正定矩阵 A 下的线性方程组：$Ax=b$\n",
    "- 之所以叫做 **共轭梯度法**，是因为其梯度方向更新和普通梯度下降不同，它保留了“在函数空间中与之前梯度下降方向共轭”的性质，能避免重复探索，每一步都不会干扰之前已经优化过的方向（即：每步都在“新的维度”上优化）从而加速收敛。\n",
    "> **KL 散度** 在旧策略处达到最小值，且关于新策略参数是局部严格凸的，因此它的 Hessian 是 **正定矩阵**。这个正定性保证了 **共轭梯度法** 可用，方向唯一、收敛稳定。\n",
    "> **TRPO** 中神经网络表示的策略函数的参数数量都是成千上万的，计算和存储海森矩阵H的逆矩阵会 **耗费大量的内存资源和时间**，通过 **共轭梯度法**直接计算：$x=H^{-1}g$，可以解决资源消耗问题。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78e9919ced623214"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 共轭梯度法流程（求解 $\\(Hx = g\\)$，$\\(H\\)$ 对称正定）:\n",
    "> **初始化**  \n",
    "   - $\\(x_0\\)$：初始解（一般取零向量）  \n",
    "   - $r_0 = g - H x_0$：初始残差（梯度）  \n",
    "   - $p_0 = r_0$：初始搜索方向\n",
    "\n",
    "> **迭代更新**（$k=0,1,2,\\ldots$）\n",
    "\n",
    ">计算步长：  \n",
    "$$\\alpha_k = \\frac{r_k^T r_k}{p_k^T H p_k}$$  \n",
    ">更新解：  \n",
    "$$x_{k+1} = x_k + \\alpha_k p_k$$  \n",
    ">更新残差：  \n",
    "$$r_{k+1} = r_k - \\alpha_k H p_k$$  \n",
    ">判断收敛：若 $\\|r_{k+1}\\|$ 足够小，停止迭代  \n",
    ">计算比例因子（**Fletcher–Reeves 形式**）：  \n",
    "$$\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$$  \n",
    ">更新搜索方向：  \n",
    "$$p_{k+1} = r_{k+1} + \\beta_k p_k$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ef65e2a112c7be7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 其中，为了避免**显式构造 Hessian 矩阵**，节省内存和计算：\n",
    "$$Hp=\\nabla_\\theta\\left(\\left(\\nabla_\\theta(D_{KL}^{\\nu_k}(\\pi_{\\theta_k},\\pi_{\\theta^{\\prime}}))\\right)^T\\right)p=\\nabla_\\theta\\left(\\left(\\nabla_\\theta(D_{KL}^{\\nu_{k}}(\\pi_{\\theta_k},\\pi_{\\theta^{\\prime}}))\\right)^Tp\\right)$$\n",
    "> 即先用 **梯度和向量p点乘** 后再计算梯度"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bee4cde93e06c7de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 此时参数优化公式进化为:\n",
    "$$\\theta_{k+1}=\\theta_k+\\sqrt{\\frac{2\\delta}{x^THx}}x$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b6cbe55f4e168ed"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14.1.4 线性搜索（Line Search）\n",
    "> **近似求解**中有许多的的近似与逼近，求出的**理论最大步长$\\beta=\\sqrt{\\frac{2\\delta}{x^{T}Hx}}$不总是准确**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7be334cd1eb71d68"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 引入线性搜索，是为了从理论最大步长开始，逐步缩小步长系数，$(\\alpha^0,\\alpha^1,\\alpha^2,\\ldots)$，直到同时满足两个条件：\n",
    "- 代理的策略性能提升了\n",
    "- KL 散度没超过阈值\n",
    "\n",
    "> 此时参数优化公式又进化为:\n",
    "$$\\theta_{k+1}=\\theta_k+\\alpha^i\\sqrt{\\frac{2\\delta}{x^THx}}x$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a131e9a02c7a79fd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 14.1.5 广义优势估计\n",
    "> 计算梯度grad，需要估计 **优势函数A**，但\n",
    "- 实际中$Q^{\\pi}$很难直接得到，需要使用 Bootstrapping 和 TD 误差 构造它\n",
    "- **单步的时序差分误差：** $\\delta_t=r_t+\\gamma V(s_{t+1})-V(s_t)$，虽然低方差，但**高偏差**，只考虑一步\n",
    "> 目前比较常用的一种方法为 **广义优势估计（Generalized Advantage Estimation，GAE）**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f0f43e47e0fd3f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 根据多步时序差分的思想，有：\n",
    "$$\\begin{aligned}\n",
    "A_{t}^{(1)} & =\\delta_{t} & & =-V(s_t)+r_t+\\gamma V(s_{t+1}) \\\\\n",
    "A_{t}^{(2)} & =\\delta_t+\\gamma\\delta_{t+1} & & =-V(s_t)+r_t+\\gamma r_{t+1}+\\gamma^2V(s_{t+2}) \\\\\n",
    "A_{t}^{(3)} & =\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2} & & =-V(s_t)+r_t+\\gamma r_{t+1}+\\gamma^2r_{t+2}+\\gamma^3V(s_{t+3}) \\\\\n",
    "A_{t}^{(k)} & =\\sum_{l=0}^{k-1}\\gamma^l\\delta_{t+l} & & =-V(s_t)+r_t+\\gamma r_{t+1}+\\ldots+\\gamma^{k-1}r_{t+k-1}+\\gamma^kV(s_{t+k})\n",
    "\\end{aligned}$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ef17ea96d547f8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **GAE** 便是将这些不同步数的优势估计进行指数加权平均：\n",
    "$$\\begin{aligned}\n",
    "A_{t}^{GAE} & =(1-\\lambda)(A_t^{(1)}+\\lambda A_t^{(2)}+\\lambda^2A_t^{(3)}+\\cdots) \\\\\n",
    " & =(1-\\lambda)(\\delta_t+\\lambda(\\delta_t+\\gamma\\delta_{t+1})+\\lambda^2(\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2})+\\cdots) \\\\\n",
    " & =(1-\\lambda)(\\delta(1+\\lambda+\\lambda^2+\\cdots)+\\gamma\\delta_{t+1}(\\lambda+\\lambda^2+\\lambda^3+\\cdots)+\\gamma^2 \\\\\n",
    " & =(1-\\lambda)\\left(\\delta_t\\frac{1}{1-\\lambda}+\\gamma\\delta_{t+1}\\frac{\\lambda}{1-\\lambda}+\\gamma^2\\delta_{t+2}\\frac{\\lambda^2}{1-\\lambda}+\\cdots\\right) \\\\\n",
    " & =\\sum_{l=0}^\\infty(\\gamma\\lambda)^l\\delta_{t+l}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- $\\lambda$ : **权衡偏差（bias）与方差（variance）**, 取值范围为 [0, 1]\n",
    "\n",
    "| λ 值   | 方法类型     | 偏差-方差特性 | 数学公式表达                                                                                   | 说明                             |\n",
    "|-------|----------|---------|------------------------------------------------------------------------------------------|--------------------------------|\n",
    "| 0     | 一步 TD    | 高偏差、低方差 | $\\hat{A}_t = \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$                                | 只看一步，误差积累小，估计稳定但可能不准确          |\n",
    "| 1     | 蒙特卡洛优势估计 | 低偏差、高方差 | $\\hat{A}_t = \\sum_{l=0}^\\infty \\gamma^l r_{t+l} - V(s_t)$                                | 回顾整条轨迹，误差积累大，但估计更准确            |\n",
    "| (0,1) | GAE      | 偏差-方差折中 | $\\hat{A}^{GAE(\\gamma, \\lambda)}_t = \\sum_{l=0}^{\\infty} (\\gamma \\lambda)^l \\delta_{t+l}$ | λ 越大越靠近蒙特卡洛，越小越靠近一步 TD，平衡偏差与方差 |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b4408d8ba04ef71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 给定 $\\gamma,\\lambda$以及每个时间步的$\\delta_{t}$, 以下是**每一步的优势估计函数**(已添加到utils中)："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0ad45cbdfa5840e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def compute_advantage(gamma, lmbda, td_delta):\n",
    "    td_delta = td_delta.detach().numpy()\n",
    "    advantage_list = []\n",
    "    advantage = 0.0\n",
    "    for delta in reversed(td_delta):\n",
    "        advantage = gamma * lmbda * advantage + delta\n",
    "        advantage_list.append(advantage)\n",
    "    advantage_list.reverse()\n",
    "    return torch.tensor(np.array(advantage_list), dtype=torch.float)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-03T03:10:59.591066600Z",
     "start_time": "2025-08-03T03:10:59.543503800Z"
    }
   },
   "id": "ac3793830d395bd1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "以上，将 **TRPO算法的基本原理** 分**五点**递推讲解，去学习其中的**本质**，相关概念涉及到许多其它理论，相较于之前的算法更为复杂\n",
    "\n",
    "代码实验分开在下一节中进行"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c37eaca932eed7dd"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d19de85a1c733a5a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
