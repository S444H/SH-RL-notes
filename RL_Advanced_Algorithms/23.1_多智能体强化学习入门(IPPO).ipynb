{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 23.1 多智能体强化学习入门\n",
    "- 基本概念\n",
    "- IPPO 算法"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c2b7c3acfea1c6b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 之前介绍的算法都是 **单智能体** 强化学习算法，其基本假设是动态环境是 **稳态的（stationary）**，即 **状态转移概率** 和 **奖励函数** 不变\n",
    "\n",
    "> 但在现实中，许多系统并非由单一主体组成，而是由 **多个智能体** 构成。在一些任务中，智能体可能需要合作以共同完成任务（如机器人协作、团队对抗游戏），也可能需要竞争以实现对抗模拟（如博弈论中的零和游戏）此时就要求环境中有多个智能体进行交互和学习，这样的任务被称为 **多智能体强化学习（multi-agent reinforcement learning，MARL）**\n",
    "\n",
    "> **MARL** 中的每个智能体在和环境交互的同时也在和 **其他智能体** 进行直接或者间接的**交互**，因此在每个智能体的视角下，环境是 **非稳态的（non-stationary）**：对于一个智能体而言，即使在 **相同的状态** 下采取 **相同的动作**，得到的状态转移和奖励信号的分布可能在**不断改变**\n",
    "> 所以多智能体的协作，本身会造就：\n",
    "- 训练的环境更为复杂\n",
    "> 其次:\n",
    "- 智能体目标可能是不同的，不同智能体需要最大化自己的利益\n",
    "- 训练难度更高，可能需要大规模分布式训练来提高效率\n",
    "\n",
    "***参考文献：***\n",
    "[多智能体深度强化学习的若干关键科学问题]()\n",
    "[多智能体深度强化学习的调查与批判](https://arxiv.org/abs/1810.05587)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1478d1a2354f954b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
