{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 10. Double DQN\n",
    "\n",
    "本节学习 **Double DQN** --- [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e3ead17d05d0c5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "同理，先导入以下库："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b1d94bf9382f23b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-20T20:51:30.126066400Z",
     "start_time": "2025-07-20T20:51:30.088846800Z"
    }
   },
   "id": "4698b09c58ab7272"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from utils.replay_buffer import ReplayBuffer\n",
    "from utils.smoothing import moving_average"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-20T20:51:30.127099600Z",
     "start_time": "2025-07-20T20:51:30.093033700Z"
    }
   },
   "id": "ae7b0deb34d74e33"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.1 Double DQN 原理\n",
    "> 在之前介绍原始DQN算法的 **目标网络** 时，就说到需要注意Q值更新中 **下一状态最优动作的选择** 以及 **对应的最大Q值** 都是由 **目标网络** 计算：\n",
    "$$r+\\gamma Q_{\\omega^-}\\left(s^{\\prime},\\arg\\max_{a^{\\prime}}Q_{\\omega^-}\\left(s^{\\prime},a^{\\prime}\\right)\\right)$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "109be0698970aecc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 这里有一个问题: \n",
    "神经网络在拟合的过程中本就会导致估算的Q值有正向或负向的误差，又因为每次利用的是下一状态$s^{\\prime}$的最大动作Q值，就会导致累积正向误差(当目标网络也更新时):\n",
    "$$Q_{\\mathrm{target}}=r+\\gamma\\cdot\\max_{a^{\\prime}}Q(s^{\\prime},a^{\\prime};\\omega^-)$$\n",
    "使单个目标网络中，某个状态$s_{}$的动作Q值会因为下一个状态$s^{\\prime}$的最大Q值正向增大，同时还会将本身Q值的过高估计,影响传递给上一个状态$s_{^-}$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92b08ddefcaac46f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 对于动作空间较大的任务，DQN 中的过高估计问题会非常严重，造成 DQN 无法有效工作的后果，以下为证明（概率论）：\n",
    "\n",
    "将估算误差记为：$\\epsilon_a=Q_{\\omega^-}(s,a)-\\max_{a^{\\prime}}Q^*(s,a^{\\prime})$ ，动作有m个\n",
    "又估算误差对于不同的动作是独立的：$P\\left(\\max_a\\epsilon_a\\leq x\\right)=\\prod_{a=1}^mP\\left(\\epsilon_a\\leq x\\right)$\n",
    "为简化实际环境，且已知估算误差越小概率越小，所以假设估算误差服从$[-1,1]$的均匀分布，此时有：\n",
    "$$\\begin{aligned}\n",
    "P\\left(\\max_{a}\\epsilon_{a}\\leq x\\right) & =\\prod_{a=1}^mP(\\epsilon_a\\leq x) \\\\\n",
    " & =\n",
    "\\begin{cases}\n",
    "0 & \\mathrm{if}x\\leq-1 \\\\\n",
    "\\left(\\frac{1+x}{2}\\right)^m & \\mathrm{if}x\\in(-1,1) \\\\\n",
    "1 & \\mathrm{if}x\\geq1 & \n",
    "\\end{cases}\n",
    "\\end{aligned}$$\n",
    "从而得到最大估算误差的期望为：\n",
    "$$\\begin{aligned}\n",
    "\\mathbb{E}\\left[\\max_{a}\\epsilon_{a}\\right] & =\\int_{-1}^{1}x\\frac{\\mathrm{d}}{\\mathrm{d}x}P\\left(\\max_{a}\\epsilon_{a}\\leq x\\right)\\mathrm{d}x \\\\\n",
    " & =\\left[\\left(\\frac{x+1}{2}\\right)^m\\frac{mx-1}{m+1}\\right]_{-1}^1 \\\\\n",
    " & =\\frac{m-1}{m+1}\n",
    "\\end{aligned}$$\n",
    "- m越大误差越大"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "352473041c6afff2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Double DQN 的解决方案：\n",
    "为解决这一问题，Double DQN 算法提出利用两个独立训练的神经网络估算：\n",
    "$$r+\\gamma Q_{\\omega^-}\\left(s^{\\prime},\\arg\\max_{a^{\\prime}}Q_\\omega\\left(s^{\\prime},a^{\\prime}\\right)\\right)$$\n",
    "此时传统 DQN 算法当中的训练网络，负责起对下个状态所采取的动作进行选择\n",
    "> 采用 **动作的选择** 与 **Q值的评估** 分离的方式，可以在一定程度上减少“更新滞后的目标网络愈发高估之前作出的错误的动作”的现象，通过评估实时更新的训练网络所选择的动作，增添新的可能，两网络共同作用，最终可以显著减少 Q 值估计的高估问题"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4f4ae1e00cf54df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 10.2 Double DQN 代码实现\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "534f7376087d8a39"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Q网络："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe4eee7d62062556"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a8bb1b5622a17e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Double DQN算法："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6e9788e6612b986e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DoubleDQN:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma, epsilon, target_update, device):\n",
    "        self.action_dim = action_dim \n",
    "        # Q 网络\n",
    "        self.q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device) \n",
    "        # 目标网络\n",
    "        self.target_q_net = Qnet(state_dim, hidden_dim, self.action_dim).to(device)\n",
    "        # 使用Adam优化器\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma  # 折扣因子（未来）\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略\n",
    "        self.target_update = target_update  # 目标网络更新频率\n",
    "        self.count = 0  # 计数器,记录更新次数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):  # epsilon-贪婪策略采取动作\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            # 将列表转换为(batch_size, state_dim)的结构与网络传播兼容\n",
    "            state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "    \n",
    "    def max_q_value(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        return self.q_net(state).max().item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        # .view(-1, 1)转换为一个2D张量，以便与其他维度匹配并符合模型的输入要求\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)#转换为一个2D张量，以便与其他维度匹配并符合模型的输入要求\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)  # 当前Q值\n",
    "        \n",
    "        #---------------------------------------------\n",
    "        max_action = self.q_net(next_states).max(1)[1].view(-1, 1)\n",
    "        max_next_q_values = self.target_q_net(next_states).gather(1, max_action)\n",
    "        #max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1)  # DQN的情况\n",
    "        #----------------------------------------------\n",
    "        \n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)  # TD误差目标\n",
    "        # dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  # 均方误差损失函数\n",
    "        loss = F.mse_loss(q_values, q_targets)  # 默认已经是 mean\n",
    "        self.optimizer.zero_grad()  # PyTorch中默认梯度会累积, 非批量累积梯度任务，这里需要显式将梯度置为0\n",
    "        loss.backward()  # 反向传播更新参数\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update == 0:\n",
    "            self.target_q_net.load_state_dict(\n",
    "                self.q_net.state_dict())  # 更新目标网络\n",
    "        self.count += 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec1ece6282ce43b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Env设置:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "605d4b6027f324ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2cf3703fccdb2ea5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
