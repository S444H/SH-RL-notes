{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 11.策略梯度算法\n",
    ">Q-learning、DQN 及 DQN 改进算法都是 **基于价值（value-based）** 的方法，在强化学习中，除了基于值函数的方法，还有一支非常经典的方法：**基于策略（policy-based）** 的方法。\n",
    "\n",
    "- **基于值函数的方法** 主要是学习值函数，然后根据值函数导出一个策略，学习过程中并不存在一个显式的策略\n",
    "- **基于策略的方法** 则是直接显式地学习一个目标策略\n",
    "\n",
    "其中 **策略梯度** 是基于策略的方法的基础\n",
    "它的核心思想是：通过对 **策略的参数化** 表示，利用梯度上升法直接优化 **期望回报**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ede999e98a94b844"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 11.1 策略梯度\n",
    "> 在强化学习中，策略定义了智能体在某个状态下选择动作的概率分布。假设一个策略是通过参数化的形式表示的，通常写作：\n",
    "\n",
    "$$\\pi_\\theta(a|s)$$\n",
    "\n",
    "- 表示在状态 s 下选择动作 a 的概率，而 θ 是策略的参数（如神经网络的权重）\n",
    "- 通过调整参数 θ，使得智能体的长期回报（即累积奖励）最大化，即 **最大化期望回报**\n",
    "\n",
    "> 在某个策略$\\pi_\\theta(a|s)$下，**期望回报** 可以表示为：\n",
    "\n",
    "$$J(\\theta)=\\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=0}^T\\gamma^tr_t\\right]$$\n",
    "\n",
    "\n",
    "- $r_t$是时间步$t$的即时奖励，$\\gamma$是折扣因子，$T$是时间步数\n",
    "- 这其中的整个序列动作的选择都是基于策略$\\pi_\\theta(a|s)$\n",
    "\n",
    "> 若熟知 [3.马尔可夫决策过程](../RL_Fundamentals/3.马尔可夫决策过程(Markov_decision_process,MDP).ipynb) 中的一系列定义与公式推导，可知 **期望回报** 就是:\n",
    "\n",
    "$$J(\\theta)=\\mathbb{E}_{s_0}[V^{\\pi_\\theta}(s_0)]$$\n",
    "\n",
    "- $s_0$表示初始状态\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7c9b005e86f5efb"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a8284d828868e263"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
