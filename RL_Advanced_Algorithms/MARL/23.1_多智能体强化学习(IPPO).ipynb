{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2b7c3acfea1c6b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 23.1 多智能体强化学习（IPPO）\n",
    "- 基本概念\n",
    "- IPPO 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478d1a2354f954b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> 之前介绍的算法都是 **单智能体** 强化学习算法，其基本假设是动态环境是 **稳态的（stationary）**，即 **状态转移概率** 和 **奖励函数** 不变\n",
    "\n",
    "> 但在现实中，许多系统并非由单一主体组成，而是由 **多个智能体** 构成。在一些任务中，智能体可能需要合作以共同完成任务（如机器人协作、团队对抗游戏），也可能需要竞争以实现对抗模拟（如博弈论中的零和游戏）此时就要求环境中有多个智能体进行交互和学习，这样的任务被称为 **多智能体强化学习（multi-agent reinforcement learning，MARL）**\n",
    "\n",
    "> **MARL** 中的每个智能体在和环境交互的同时也在和 **其他智能体** 进行直接或者间接的**交互**，因此在每个智能体的视角下，环境是 **非稳态的（non-stationary）**：对于一个智能体而言，即使在 **相同的状态** 下采取 **相同的动作**，得到的状态转移和奖励信号的分布可能在**不断改变**\n",
    "> 所以多智能体的协作，本身会造就：\n",
    "- 训练的环境更为复杂\n",
    "> 其次:\n",
    "- 智能体目标可能是不同的，不同智能体需要最大化自己的利益\n",
    "- 训练难度更高，可能需要大规模分布式训练来提高效率\n",
    "\n",
    "***参考文献：***\n",
    "1. [多智能体深度强化学习的若干关键科学问题](https://kns.cnki.net/kcms2/article/abstract?v=hEVkP-djbGzQFZYseuDP5znRnCKidSEjzwYYgqH7xXiRLM90zl4RCyS7NqBJ9ADgnVnqy7EFO_RTQU-IvB4q7kgt0X64dkA9SwmnGm5AUMOwU9B38ZnNOfeVxU0gDSmgGyTGkNXHoPWIJwW0nmBKe-o19dokZVne9ebzRraoD1_TqMx5EDX-xQ==&uniplatform=NZKPT&language=CHS)\n",
    "2. [A Survey and Critique of Multiagent Deep Reinforcement Learning](https://arxiv.org/abs/1810.05587)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344c5e7bf346bfc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***多智能体环境表示：***\n",
    "> 可以用 **一个元组$(N,\\mathcal{S},\\mathcal{A},\\mathcal{R},P)$** 来表示 **一个多智能体环境**\n",
    "- 其中$N$是智能体的数目\n",
    "- $\\mathcal{S}=S_1\\times\\cdots\\times S_N$ 是 所有智能体的状态集合\n",
    "- $\\mathcal{A}=A_1\\times\\cdots\\times A_N$ 是 所有智能体的动作集合\n",
    "- $\\mathcal{R}=r_1\\times\\cdots\\times r_N$ 是 所有智能体奖励函数的集合\n",
    "- $P$是环境的状态转移概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea309dbf274bf86e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.1.1 多智能体强化学习的基本求解范式\n",
    "> 如果只是基于之前已经熟悉的 **单智能体算法**，拓展多智能体的概念，那么 **多智能体强化学习算法** 主要分两种思路：\n",
    "\n",
    "> 1. **完全中心化（fully centralized）方法** ：将多个智能体进行决策当作一个 **超级智能体** 在进行决策（*所有智能体的状态聚合在一起当作一个全局的超级状态；所有智能体的动作连起来作为一个联合动作*）。\n",
    "- 优点：由于已经知道了所有智能体的状态和动作，因此对这个 **超级智能体** 来说，**环境依旧是稳态**的，一些**单智能体的算法的收敛性**依旧可以得到保证。\n",
    "- 缺点：样的做法不能很好地扩展到智能体数量很多或者环境很大的情况，因为这时候将所有的信息简单暴力地拼在一起会**导致维度爆炸**，**训练复杂度巨幅提升**的问题往往不可解决。\n",
    "\n",
    "> 2. **完全去中心化（fully decentralized）方法** ：与完全中心化方法相反的范式便是假设 **每个智能体都在自身的环境中独立地进行学习，不考虑其他智能体的改变**。完全去中心化方法直接对 **每个智能体用一个单智能体强化学习算法来学习**。\n",
    "- 缺点：环境是非稳态的，智能体之间没有信息共享，训练的收敛性不能得到保证。\n",
    "- 优点：随着智能体数量的增加有比较好的扩展性，不会遇到维度灾难而导致训练不能进行下去。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb33398eb7ff0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.1.2 完全去中心化 算法 IPPO\n",
    "> 此类算法被称为 **独立学习（Independent Learning）**。由于对于每个智能体使用单智能体算法 **PPO（PPO-截断版本）** 进行训练，因此这个算法叫作 **独立 PPO（Independent PPO，IPPO）算法**。\n",
    "\n",
    "***算法流程：***\n",
    "\n",
    "对于$N$个智能体，**初始化** 每个智能体各自的策略以及价值函数\n",
    "for 训练轮数 $k= 0, 1, 2\\ldots$ do\n",
    "\n",
    "- 所有智能体在环境中交互 **分别获得各自的一条轨迹数据**\n",
    "- 对每个智能体，基于当前的价值函数用 GAE 计算优势函数的估计\n",
    "- 对每个智能体，通过最大化其 PPO-截断的目标来 **更新其策略**\n",
    "- 对每个智能体，通过均方误差损失函数 **优化其价值函数**\n",
    "\n",
    "end for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e123e8128dfd60",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.1.3 Combat 环境说明\n",
    "> **Combat 环境** 是 **ma_gym库**（[详细下载以及环境说明](https://github.com/koulanurag/ma-gym)） 中的一个多智能体环境：\n",
    "（由于该库所依赖的一些库的版本与之前算法使用的库版本不同，为了 **防止依赖冲突**，这里可以新建一个虚拟环境使用 **ma_gym库**）\n",
    "![Combat-v0](Illustrations/Combat-v0.gif)\n",
    "\n",
    "> 在一个二维的格子世界上，有两个队伍进行对战模拟游戏：\n",
    "> - 每个团队由 $m$ 个智能体组成，他们的初始位置以 $5 × 5$ 均匀采样 围绕团队中心形成方形\n",
    "> - 每个智能体的动作集合为：向四周移动 $1$ 格（离散动作：0，1，2，3），攻击周围（$3 × 3$）格范围内的敌对智能体，或者不采取任何行动\n",
    "> - 每个智能体攻击后可有冷却时间\n",
    "> - 起初每个智能体有 $3$ 点生命值，如果智能体在敌人的攻击范围内被攻击到了，则会扣 $1$ 生命值，生命值掉为 $0$ 则死亡\n",
    "> - 如果一支队伍中的所有智能体都死亡，另一支队伍将获胜（当前版本默认最大步长为 100）\n",
    "\n",
    "**己方队伍（红色）** 移动策略由算法训练；**敌方队伍（蓝色）** 默认使用固定的算法：攻击在范围内最近的敌人，如果攻击范围内没有敌人，则向敌人靠近。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2094d0991a8ba4ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T14:11:47.317079200Z",
     "start_time": "2025-09-08T14:11:46.534153600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ma_gym.envs.combat.combat import Combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2bba5b1bb4d945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.134766600Z",
     "start_time": "2025-09-08T09:16:30.024317100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.advantage import compute_advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390435480c3d3f5b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.1.4 IPPO 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef93f29fa5a733",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PPO（PPO-截断版本）部分\n",
    "> 与之前的 **离散动作版本** 基本一致：\n",
    "1. 网络层数略微不同\n",
    "2. 一次更新中，只训练一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebe50dedff2a547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.138772300Z",
     "start_time": "2025-09-08T09:16:30.040865200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bdf12e7b8d49f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.140779600Z",
     "start_time": "2025-09-08T09:16:30.062014400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\" PPO算法,采用截断方式 \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps, gamma, device):\n",
    "        \n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        \n",
    "        states_np = np.array(transition_dict['states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        states = torch.tensor(states_np, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states_np = np.array(transition_dict['next_states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        next_states = torch.tensor(next_states_np, dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        \n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)  # 时序差分目标\n",
    "        td_delta = td_target - self.critic(states)                                 # 时序差分误差\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
    "        \n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach()\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        \n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage  # 截断\n",
    "        actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        #-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecc4e5cc7fa3e7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  IPPO 部分\n",
    "> 训练时使用 **参数共享（parameter sharing）** 的技巧：对于**所有智能体**使用 **同一套策略参数** 。\n",
    "> - 这样做的**好处**是，能使得模型训练数据更多，同时训练更稳定\n",
    "> - 这样做的**前提**是，两个智能体是同质的（homogeneous），即它们的状态空间和动作空间是完全一致的，并且它们的优化目标也完全一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76182016714ed01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 环境设置："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf94e91a43dc59",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 演示:\n",
    "> 可通过该演示，观察环境的各个字段设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df826c80ae015e7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T15:34:54.206101600Z",
     "start_time": "2025-09-08T15:34:52.503995600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实际上为冷却(step_cool+1 = 0)步后再攻击\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 1\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 2\n",
      "[0, -1] {'health': {0: 3, 1: 2}} [False, False] dict_items([(0, 3), (1, 3)]) 3\n",
      "[0, -1] {'health': {0: 3, 1: 1}} [False, False] dict_items([(0, 3), (1, 3)]) 4\n",
      "[0, -1] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 5\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 6\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 7\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 8\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 9\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 10\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 11\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 12\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 13\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 14\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 15\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 16\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 17\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 18\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 19\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 20\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 21\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 22\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 23\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 24\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 25\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 26\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 27\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 28\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 29\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 30\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 31\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 32\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 33\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 34\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 35\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 36\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 37\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 38\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 39\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 40\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 41\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 42\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 43\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 44\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 45\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 46\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 47\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 48\n",
      "[0, 0] {'health': {0: 3, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 49\n",
      "[-1, 0] {'health': {0: 2, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 50\n",
      "[-1, 0] {'health': {0: 1, 1: 0}} [False, False] dict_items([(0, 3), (1, 3)]) 51\n",
      "[-1, 0] {'health': {0: 0, 1: 0}} [True, True] dict_items([(0, 3), (1, 3)]) 52\n"
     ]
    }
   ],
   "source": [
    "from ma_gym.envs.combat.combat import Combat\n",
    "import time\n",
    "\n",
    "# 创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2\n",
    "env1 = Combat(grid_shape=(15, 15), n_agents=2, n_opponents=2, step_cool=-1)\n",
    "print('实际上为冷却(step_cool+1 = %d)步后再攻击' % env1._step_cool)\n",
    "\n",
    "# 开启渲染（mode=\"human\"）\n",
    "obs = env1.reset()\n",
    "done_n = [False] * env1.n_agents\n",
    "\n",
    "while not all(done_n):\n",
    "    actions = [env1.action_space[i].sample() for i in range(env1.n_agents)]\n",
    "    obs, reward_n, done_n, info = env1.step(actions)\n",
    "    print(reward_n, info, done_n, env1.opp_health.items(), env1._step_count)\n",
    "    env1.render(mode=\"human\")   # 可视化\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10a5128f5dae43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:46:11.738191300Z",
     "start_time": "2025-09-08T13:46:11.725157400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "env = Combat(grid_shape=(15, 15), n_agents=2, n_opponents=2, step_cool=-1, step_cost=-0.1)\n",
    "s = env.reset()\n",
    "print(env._max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106116513c9e07f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99b0b0a981df7b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:33.425803400Z",
     "start_time": "2025-09-08T09:16:33.384802800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actor_lr = 3e-4\n",
    "critic_lr = 1e-3\n",
    "lmbda = 0.97\n",
    "eps = 0.2\n",
    "gamma = 0.99\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "state_dim = env.observation_space[0].shape[0]\n",
    "hidden_dim = 64\n",
    "action_dim = env.action_space[0].n\n",
    "#两个智能体共享同一个策略\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, \n",
    "            lmbda, eps, gamma, device)\n",
    "\n",
    "\n",
    "num_episodes = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fcdc69abdf03cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 测试与训练："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675ef6ae8b6d084",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***判断某一动作执行之后，己方智能体团队是否获胜：***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74e595f57120c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:17:48.753380600Z",
     "start_time": "2025-09-08T09:16:33.416780800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10000/10000 [23:44<00:00,  7.02it/s, episode=10000, win_rate=0.240]\n",
      "Iteration 1: 100%|██████████| 10000/10000 [24:08<00:00,  6.90it/s, episode=20000, win_rate=0.230]\n",
      "Iteration 2: 100%|██████████| 10000/10000 [24:28<00:00,  6.81it/s, episode=30000, win_rate=0.200]\n",
      "Iteration 3: 100%|██████████| 10000/10000 [25:07<00:00,  6.63it/s, episode=40000, win_rate=0.270]\n",
      "Iteration 4: 100%|██████████| 10000/10000 [25:01<00:00,  6.66it/s, episode=50000, win_rate=0.290]\n",
      "Iteration 5: 100%|██████████| 10000/10000 [24:45<00:00,  6.73it/s, episode=60000, win_rate=0.440]\n",
      "Iteration 6: 100%|██████████| 10000/10000 [24:47<00:00,  6.72it/s, episode=70000, win_rate=0.250]\n",
      "Iteration 7: 100%|██████████| 10000/10000 [24:50<00:00,  6.71it/s, episode=80000, win_rate=0.420]\n",
      "Iteration 8: 100%|██████████| 10000/10000 [24:39<00:00,  6.76it/s, episode=90000, win_rate=0.350]\n",
      "Iteration 9: 100%|██████████| 10000/10000 [24:44<00:00,  6.73it/s, episode=100000, win_rate=0.320]\n"
     ]
    }
   ],
   "source": [
    "win_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            transition_dict_1 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            transition_dict_2 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            s = env.reset()  # 返回 m 组状态\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                a_1 = agent.take_action(s[0])\n",
    "                a_2 = agent.take_action(s[1])\n",
    "                next_s, r, done, info = env.step([a_1, a_2])\n",
    "                \n",
    "                transition_dict_1['states'].append(s[0])\n",
    "                transition_dict_1['actions'].append(a_1)\n",
    "                transition_dict_1['next_states'].append(next_s[0])\n",
    "                # 如果敌方血量总和为 0，则认为赢了\n",
    "                transition_dict_1['rewards'].append(\n",
    "                    r[0] + 100 if (sum([v for k, v in env.opp_health.items()]) == 0) else r[0])\n",
    "                transition_dict_1['dones'].append(False)\n",
    "                \n",
    "                transition_dict_2['states'].append(s[1])\n",
    "                transition_dict_2['actions'].append(a_2)\n",
    "                transition_dict_2['next_states'].append(next_s[1])\n",
    "                transition_dict_2['rewards'].append(\n",
    "                    r[1] + 100 if (sum([v for k, v in env.opp_health.items()]) == 0) else r[1])\n",
    "                transition_dict_2['dones'].append(False)\n",
    "                \n",
    "                s = next_s\n",
    "                terminal = all(done)  # 到达 最大步长 或者 一方智能体全部没有血了\n",
    "                \n",
    "            win_list.append(1 if (sum([v for k, v in env.opp_health.items()]) == 0) else 0)\n",
    "            agent.update(transition_dict_1)\n",
    "            agent.update(transition_dict_2)\n",
    "            \n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'win_rate':\n",
    "                    '%.3f' % np.mean(win_list[-100:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226c7e30b84a483",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 效果展示：\n",
    "> 与之前的实验 **追踪训练过程中的回报** 不同，这里将 IPPO 训练的 **智能体团队的胜率** 作为算法指标，展示效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab39844f13aa0ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:50:25.838350200Z",
     "start_time": "2025-09-08T09:50:25.787717600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "win_array = np.array(win_list)\n",
    "# 每100条轨迹的获胜率取一次平均，用于绘图\n",
    "win_array = np.mean(win_array.reshape(-1, 100), axis=1)\n",
    "episodes_list = np.arange(win_array.shape[0]) * 100\n",
    "\n",
    "# 创建 DataFrame\n",
    "df1 = pd.DataFrame({'Episodes': episodes_list, 'Returns': win_array})\n",
    "# 保存为 CSV 文件\n",
    "df1.to_csv('IPPO_Clip_Combat_win-rate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d940e454b13b6b09",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-25T10:32:20.916879Z",
     "start_time": "2025-09-25T10:32:20.404065Z"
    }
   },
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./Data/IPPO_Clip_Combat_win-rate_data.csv')  # 从 CSV 文件中读取数据\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['Episodes'], y=df['Returns'], mode='lines', name='WinRate'))\n",
    "fig.update_layout(\n",
    "    title='IPPO_Clip on Combat',\n",
    "    xaxis_title='Episodes',\n",
    "    yaxis_title='WinRate',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ],
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "mode": "lines",
         "name": "WinRate",
         "x": {
          "dtype": "i4",
          "bdata": "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"
         },
         "y": {
          "dtype": "f8",
          "bdata": "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"
         },
         "type": "scatter"
        }
       ],
       "layout": {
        "template": {
         "data": {
          "histogram2dcontour": [
           {
            "type": "histogram2dcontour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "choropleth": [
           {
            "type": "choropleth",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "histogram2d": [
           {
            "type": "histogram2d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "heatmap": [
           {
            "type": "heatmap",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "contourcarpet": [
           {
            "type": "contourcarpet",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "contour": [
           {
            "type": "contour",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "surface": [
           {
            "type": "surface",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0.0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1.0,
              "#f0f921"
             ]
            ]
           }
          ],
          "mesh3d": [
           {
            "type": "mesh3d",
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            }
           }
          ],
          "scatter": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scatter"
           }
          ],
          "parcoords": [
           {
            "type": "parcoords",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolargl": [
           {
            "type": "scatterpolargl",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "bar": [
           {
            "error_x": {
             "color": "#f2f5fa"
            },
            "error_y": {
             "color": "#f2f5fa"
            },
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "scattergeo": [
           {
            "type": "scattergeo",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterpolar": [
           {
            "type": "scatterpolar",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "line": {
              "color": "#283442"
             }
            },
            "type": "scattergl"
           }
          ],
          "scatter3d": [
           {
            "type": "scatter3d",
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermap": [
           {
            "type": "scattermap",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattermapbox": [
           {
            "type": "scattermapbox",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scatterternary": [
           {
            "type": "scatterternary",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "scattercarpet": [
           {
            "type": "scattercarpet",
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            }
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "baxis": {
             "endlinecolor": "#A2B1C6",
             "gridcolor": "#506784",
             "linecolor": "#506784",
             "minorgridcolor": "#506784",
             "startlinecolor": "#A2B1C6"
            },
            "type": "carpet"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#506784"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "header": {
             "fill": {
              "color": "#2a3f5f"
             },
             "line": {
              "color": "rgb(17,17,17)"
             }
            },
            "type": "table"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "rgb(17,17,17)",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ]
         },
         "layout": {
          "autotypenumbers": "strict",
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#f2f5fa"
          },
          "hovermode": "closest",
          "hoverlabel": {
           "align": "left"
          },
          "paper_bgcolor": "rgb(17,17,17)",
          "plot_bgcolor": "rgb(17,17,17)",
          "polar": {
           "bgcolor": "rgb(17,17,17)",
           "angularaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "radialaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "ternary": {
           "bgcolor": "rgb(17,17,17)",
           "aaxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           },
           "caxis": {
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "ticks": ""
           }
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "sequential": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0.0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1.0,
             "#f0f921"
            ]
           ],
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ]
          },
          "xaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "yaxis": {
           "gridcolor": "#283442",
           "linecolor": "#506784",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "#283442",
           "automargin": true,
           "zerolinewidth": 2
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "yaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           },
           "zaxis": {
            "backgroundcolor": "rgb(17,17,17)",
            "gridcolor": "#506784",
            "linecolor": "#506784",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "#C8D4E3",
            "gridwidth": 2
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#f2f5fa"
           }
          },
          "annotationdefaults": {
           "arrowcolor": "#f2f5fa",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "geo": {
           "bgcolor": "rgb(17,17,17)",
           "landcolor": "rgb(17,17,17)",
           "subunitcolor": "#506784",
           "showland": true,
           "showlakes": true,
           "lakecolor": "rgb(17,17,17)"
          },
          "title": {
           "x": 0.05
          },
          "updatemenudefaults": {
           "bgcolor": "#506784",
           "borderwidth": 0
          },
          "sliderdefaults": {
           "bgcolor": "#C8D4E3",
           "borderwidth": 1,
           "bordercolor": "rgb(17,17,17)",
           "tickwidth": 0
          },
          "mapbox": {
           "style": "dark"
          }
         }
        },
        "title": {
         "text": "IPPO_Clip on Combat"
        },
        "xaxis": {
         "title": {
          "text": "Episodes"
         }
        },
        "yaxis": {
         "title": {
          "text": "WinRate"
         }
        },
        "showlegend": true
       },
       "config": {
        "plotlyServerURL": "https://plot.ly"
       }
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "274520958079b8ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看出，在 **智能体数量较少** 的时候，**IPPO** 这种 **完全去中心化学习** 可以提升一定的胜率，**但达到的胜率也比较有限**。可能有以下几点原因：\n",
    "- 奖励设置依然稀疏，难以促进团队协同\n",
    "- 只依赖个体局部观测和局部奖励，智能体之间没有共享信息，难以形成协助\n",
    "- 环境对每个智能体来说是非平稳的（其他智能体在不断变化策略），训练容易震荡或收敛到局部最优，难以做到最优策略\n",
    "\n",
    "种种这些都会限制学习效果..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
