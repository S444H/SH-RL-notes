{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2b7c3acfea1c6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 23.1 多智能体强化学习入门\n",
    "- 基本概念\n",
    "- IPPO 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478d1a2354f954b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "> 之前介绍的算法都是 **单智能体** 强化学习算法，其基本假设是动态环境是 **稳态的（stationary）**，即 **状态转移概率** 和 **奖励函数** 不变\n",
    "\n",
    "> 但在现实中，许多系统并非由单一主体组成，而是由 **多个智能体** 构成。在一些任务中，智能体可能需要合作以共同完成任务（如机器人协作、团队对抗游戏），也可能需要竞争以实现对抗模拟（如博弈论中的零和游戏）此时就要求环境中有多个智能体进行交互和学习，这样的任务被称为 **多智能体强化学习（multi-agent reinforcement learning，MARL）**\n",
    "\n",
    "> **MARL** 中的每个智能体在和环境交互的同时也在和 **其他智能体** 进行直接或者间接的**交互**，因此在每个智能体的视角下，环境是 **非稳态的（non-stationary）**：对于一个智能体而言，即使在 **相同的状态** 下采取 **相同的动作**，得到的状态转移和奖励信号的分布可能在**不断改变**\n",
    "> 所以多智能体的协作，本身会造就：\n",
    "- 训练的环境更为复杂\n",
    "> 其次:\n",
    "- 智能体目标可能是不同的，不同智能体需要最大化自己的利益\n",
    "- 训练难度更高，可能需要大规模分布式训练来提高效率\n",
    "\n",
    "***参考文献：***\n",
    "1. [多智能体深度强化学习的若干关键科学问题](https://kns.cnki.net/kcms2/article/abstract?v=hEVkP-djbGzQFZYseuDP5znRnCKidSEjzwYYgqH7xXiRLM90zl4RCyS7NqBJ9ADgnVnqy7EFO_RTQU-IvB4q7kgt0X64dkA9SwmnGm5AUMOwU9B38ZnNOfeVxU0gDSmgGyTGkNXHoPWIJwW0nmBKe-o19dokZVne9ebzRraoD1_TqMx5EDX-xQ==&uniplatform=NZKPT&language=CHS)\n",
    "2. [A Survey and Critique of Multiagent Deep Reinforcement Learning](https://arxiv.org/abs/1810.05587)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344c5e7bf346bfc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***多智能体环境表示：***\n",
    "> 可以用 **一个元组$(N,\\mathcal{S},\\mathcal{A},\\mathcal{R},P)$** 来表示 **一个多智能体环境**\n",
    "- 其中$N$是智能体的数目\n",
    "- $\\mathcal{S}=S_1\\times\\cdots\\times S_N$ 是 所有智能体的状态集合\n",
    "- $\\mathcal{A}=A_1\\times\\cdots\\times A_N$ 是 所有智能体的动作集合\n",
    "- $\\mathcal{R}=r_1\\times\\cdots\\times r_N$ 是 所有智能体奖励函数的集合\n",
    "- $P$是环境的状态转移概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea309dbf274bf86e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 23.1.1 多智能体强化学习的基本求解范式\n",
    "> 如果只是基于之前已经熟悉的 **单智能体算法**，拓展多智能体的概念，那么 **多智能体强化学习算法** 主要分两种思路：\n",
    "\n",
    "> 1. **完全中心化（fully centralized）方法** ：将多个智能体进行决策当作一个 **超级智能体** 在进行决策（*所有智能体的状态聚合在一起当作一个全局的超级状态；所有智能体的动作连起来作为一个联合动作*）。\n",
    "- 优点：由于已经知道了所有智能体的状态和动作，因此对这个 **超级智能体** 来说，**环境依旧是稳态**的，一些**单智能体的算法的收敛性**依旧可以得到保证。\n",
    "- 缺点：样的做法不能很好地扩展到智能体数量很多或者环境很大的情况，因为这时候将所有的信息简单暴力地拼在一起会**导致维度爆炸**，**训练复杂度巨幅提升**的问题往往不可解决。\n",
    "\n",
    "> 2. **完全去中心化（fully decentralized）方法** ：与完全中心化方法相反的范式便是假设 **每个智能体都在自身的环境中独立地进行学习，不考虑其他智能体的改变**。完全去中心化方法直接对 **每个智能体用一个单智能体强化学习算法来学习**。\n",
    "- 缺点：环境是非稳态的，智能体之间没有信息共享，训练的收敛性不能得到保证。\n",
    "- 优点：随着智能体数量的增加有比较好的扩展性，不会遇到维度灾难而导致训练不能进行下去。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb33398eb7ff0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 23.1.2 完全去中心化 算法 IPPO\n",
    "> 此类算法被称为 **独立学习（Independent Learning）**。由于对于每个智能体使用单智能体算法 **PPO（PPO-截断版本）** 进行训练，因此这个算法叫作 **独立 PPO（Independent PPO，IPPO）算法**。\n",
    "\n",
    "***算法流程：***\n",
    "\n",
    "对于$N$个智能体，**初始化** 每个智能体各自的策略以及价值函数\n",
    "for 训练轮数 $k= 0, 1, 2\\ldots$ do\n",
    "\n",
    "- 所有智能体在环境中交互 **分别获得各自的一条轨迹数据**\n",
    "- 对每个智能体，基于当前的价值函数用 GAE 计算优势函数的估计\n",
    "- 对每个智能体，通过最大化其 PPO-截断的目标来 **更新其策略**\n",
    "- 对每个智能体，通过均方误差损失函数 **优化其价值函数**\n",
    "\n",
    "end for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e123e8128dfd60",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 23.1.3 Combat 环境说明\n",
    "> **Combat 环境** 是 **ma_gym库**（[详细下载以及环境说明](https://github.com/koulanurag/ma-gym)） 中的一个多智能体环境：\n",
    "（由于该库所依赖的一些库的版本与之前算法使用的库版本不同，为了 **防止依赖冲突**，这里可以新建一个虚拟环境使用 **ma_gym库**）\n",
    "![Combat-v0](Illustrations/Combat-v0.gif)\n",
    "\n",
    "> 在一个二维的格子世界上，有两个队伍进行对战模拟游戏：\n",
    "> - 每个团队由 $m$ 个智能体组成，他们的初始位置以 $5 × 5$ 均匀采样 围绕团队中心形成方形\n",
    "> - 每个智能体的动作集合为：向四周移动 $1$ 格（离散动作：0，1，2，3），攻击周围（$3 × 3$）格范围内的敌对智能体，或者不采取任何行动\n",
    "> - 每个智能体的攻击有一轮的冷却时间\n",
    "> - 起初每个智能体有 $3$ 点生命值，如果智能体在敌人的攻击范围内被攻击到了，则会扣 $1$ 生命值，生命值掉为 $0$ 则死亡\n",
    "> - 如果一支队伍中的所有智能体都死亡，另一支队伍将获胜（当前版本默认最大步长为 100）\n",
    "\n",
    "**己方队伍（红色）** 移动策略由算法训练；**敌方队伍（蓝色）** 默认使用固定的算法：攻击在范围内最近的敌人，如果攻击范围内没有敌人，则向敌人靠近。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2094d0991a8ba4ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.059005600Z",
     "start_time": "2025-09-08T09:16:30.012246Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ma_gym.envs.combat.combat import Combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2bba5b1bb4d945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.134766600Z",
     "start_time": "2025-09-08T09:16:30.024317100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.advantage import compute_advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390435480c3d3f5b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 23.1.4 IPPO 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef93f29fa5a733",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### PPO（PPO-截断版本）部分\n",
    "> 与之前的 **离散动作版本** 基本一致：\n",
    "1. 网络层数略微不同\n",
    "2. 一次更新中，只训练一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebe50dedff2a547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.138772300Z",
     "start_time": "2025-09-08T09:16:30.040865200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bdf12e7b8d49f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.140779600Z",
     "start_time": "2025-09-08T09:16:30.062014400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\" PPO算法,采用截断方式 \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps, gamma, device):\n",
    "        \n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        \n",
    "        states_np = np.array(transition_dict['states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        states = torch.tensor(states_np, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states_np = np.array(transition_dict['next_states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        next_states = torch.tensor(next_states_np, dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        \n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)  # 时序差分目标\n",
    "        td_delta = td_target - self.critic(states)                                 # 时序差分误差\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
    "        \n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach()\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        \n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage  # 截断\n",
    "        actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        #-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecc4e5cc7fa3e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "###  IPPO 部分\n",
    "> 训练时使用 **参数共享（parameter sharing）** 的技巧：对于**所有智能体**使用 **同一套策略参数** 。\n",
    "> - 这样做的**好处**是，能使得模型训练数据更多，同时训练更稳定\n",
    "> - 这样做的**前提**是，两个智能体是同质的（homogeneous），即它们的状态空间和动作空间是完全一致的，并且它们的优化目标也完全一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76182016714ed01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 环境设置："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf94e91a43dc59",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### 演示:\n",
    "> 可通过该演示，观察环境的各个字段设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df826c80ae015e7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:32:08.777003300Z",
     "start_time": "2025-09-08T13:32:04.534894700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [True, True]\n"
     ]
    }
   ],
   "source": [
    "from ma_gym.envs.combat.combat import Combat\n",
    "import time\n",
    "\n",
    "# 创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2\n",
    "env1 = Combat(grid_shape=(15, 15), n_agents=2, n_opponents=2, max_steps=20)\n",
    "\n",
    "# 开启渲染（mode=\"human\"）\n",
    "obs = env1.reset()\n",
    "done_n = [False] * env1.n_agents\n",
    "\n",
    "while not all(done_n):\n",
    "    actions = [env1.action_space[i].sample() for i in range(env1.n_agents)]\n",
    "    obs, reward_n, done_n, info = env1.step(actions)\n",
    "    print(reward_n, info, done_n)\n",
    "    env1.render(mode=\"human\")   # 可视化\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10a5128f5dae43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:46:11.738191300Z",
     "start_time": "2025-09-08T13:46:11.725157400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "env = Combat(grid_shape=(15, 15), n_agents=2, n_opponents=2, step_cost=-0.1)\n",
    "s = env.reset()\n",
    "print(env._max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106116513c9e07f7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99b0b0a981df7b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:33.425803400Z",
     "start_time": "2025-09-08T09:16:33.384802800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actor_lr = 3e-4\n",
    "critic_lr = 1e-3\n",
    "lmbda = 0.97\n",
    "eps = 0.2\n",
    "gamma = 0.99\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "state_dim = env.observation_space[0].shape[0]\n",
    "hidden_dim = 64\n",
    "action_dim = env.action_space[0].n\n",
    "#两个智能体共享同一个策略\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, \n",
    "            lmbda, eps, gamma, device)\n",
    "\n",
    "\n",
    "num_episodes = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fcdc69abdf03cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 测试与训练："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675ef6ae8b6d084",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***判断某一动作执行之后，己方智能体团队是否获胜：***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93419efebc60e36b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:33.437928500Z",
     "start_time": "2025-09-08T09:16:33.400593800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def is_win(step_count, max_steps, info):\n",
    "    \"\"\"\n",
    "    判断当前回合是否为获胜\n",
    "\n",
    "    参数:\n",
    "    - done: list, 每个己方智能体的 done 状态\n",
    "    - t: int, 当前步数\n",
    "    - max_steps: int, 最大步长\n",
    "    - info: dict, 当前环境信息，包含 'health' 字段 {agent_id: health_value}\n",
    "\n",
    "    返回:\n",
    "    - bool, True 表示获胜，False 表示其他情况\n",
    "    \"\"\"\n",
    "    # 条件1: 不是由于最大步长限制而停止\n",
    "    if step_count == max_steps:\n",
    "        return False\n",
    "    \n",
    "    # 条件2: 己方生命值不全为零\n",
    "    health_values = list(info['health'].values())\n",
    "    if all(h == 0 for h in health_values):\n",
    "        return False\n",
    "    \n",
    "    # 满足以上条件，判定胜利\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74e595f57120c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:17:48.753380600Z",
     "start_time": "2025-09-08T09:16:33.416780800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/10000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mall\u001b[39m(done)  \u001b[38;5;66;03m# 到达 最大步长 或者 一方智能体全部没有血了\u001b[39;00m\n\u001b[0;32m     45\u001b[0m win_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m win \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_dict_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m agent\u001b[38;5;241m.\u001b[39mupdate(transition_dict_2)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (i_episode \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[4], line 50\u001b[0m, in \u001b[0;36mPPO.update\u001b[1;34m(self, transition_dict)\u001b[0m\n\u001b[0;32m     48\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     49\u001b[0m critic_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_optimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\optim\\optimizer.py:485\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    481\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    482\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m             )\n\u001b[1;32m--> 485\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\optim\\optimizer.py:79\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 79\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\optim\\adam.py:246\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    234\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    236\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    237\u001b[0m         group,\n\u001b[0;32m    238\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m         state_steps,\n\u001b[0;32m    244\u001b[0m     )\n\u001b[1;32m--> 246\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoupled_weight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\optim\\optimizer.py:147\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\optim\\adam.py:933\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 933\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    936\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    937\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    940\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    941\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    947\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\optim\\adam.py:757\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[0;32m    755\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_foreach_sqrt(device_max_exp_avg_sqs)\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 757\u001b[0m     exp_avg_sq_sqrt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_sqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avg_sqs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    759\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_div_(exp_avg_sq_sqrt, bias_correction2_sqrt)\n\u001b[0;32m    760\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_add_(exp_avg_sq_sqrt, eps)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "win_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            transition_dict_1 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            transition_dict_2 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            s = env.reset()  # 返回 m 组状态\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                a_1 = agent.take_action(s[0])\n",
    "                a_2 = agent.take_action(s[1])\n",
    "                next_s, r, done, info = env.step([a_1, a_2])\n",
    "                win = is_win(env._step_count, env._max_steps, info)\n",
    "                \n",
    "                transition_dict_1['states'].append(s[0])\n",
    "                transition_dict_1['actions'].append(a_1)\n",
    "                transition_dict_1['next_states'].append(next_s[0])\n",
    "                # r[0] + 100 if win else r[0]\n",
    "                transition_dict_1['rewards'].append(\n",
    "                    r[0] + 100 if win else r[0])\n",
    "                transition_dict_1['dones'].append(False)\n",
    "                \n",
    "                transition_dict_2['states'].append(s[1])\n",
    "                transition_dict_2['actions'].append(a_2)\n",
    "                transition_dict_2['next_states'].append(next_s[1])\n",
    "                transition_dict_2['rewards'].append(\n",
    "                    r[1] + 100 if win else r[1])\n",
    "                transition_dict_2['dones'].append(False)\n",
    "                \n",
    "                s = next_s\n",
    "                terminal = all(done)  # 到达 最大步长 或者 一方智能体全部没有血了\n",
    "                \n",
    "            win_list.append(1 if win else 0)\n",
    "            agent.update(transition_dict_1)\n",
    "            agent.update(transition_dict_2)\n",
    "            \n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'win_rate':\n",
    "                    '%.3f' % np.mean(win_list[-100:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226c7e30b84a483",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 效果展示：\n",
    "> 与之前的实验 **追踪训练过程中的回报** 不同，这里将 IPPO 训练的 **智能体团队的胜率** 作为算法指标，展示效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab39844f13aa0ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:50:25.838350200Z",
     "start_time": "2025-09-08T09:50:25.787717600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "win_array = np.array(win_list)\n",
    "# 每100条轨迹的获胜率取一次平均，用于绘图\n",
    "win_array = np.mean(win_array.reshape(-1, 100), axis=1)\n",
    "episodes_list = np.arange(win_array.shape[0]) * 100\n",
    "\n",
    "# 创建 DataFrame\n",
    "df1 = pd.DataFrame({'Episodes': episodes_list, 'Returns': win_array})\n",
    "# 保存为 CSV 文件\n",
    "df1.to_csv('IPPO_Clip_Combat_win-rate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940e454b13b6b09",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "df = pd.read_csv('IPPO_Clip_Combat_win-rate_data.csv')  # 从 CSV 文件中读取数据\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['Episodes'], y=df['win-rate'], mode='lines', name='win-rate'))\n",
    "fig.update_layout(\n",
    "    title='IPPO_Clip on Combat',\n",
    "    xaxis_title='Episodes',\n",
    "    yaxis_title='win-rate',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274520958079b8ab",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "可以看出，当 **智能体数量较少** 的时候，IPPO 这种完全去中心化学习 **在一定程度上能够取得好的效果**，**但是最终达到的胜率也比较有限**。可能有以下几点原因：\n",
    "- 奖励设置依然稀疏，难以促进团队协同\n",
    "- 只依赖个体局部观测和局部奖励，智能体之间没有共享信息，难以形成协助\n",
    "- 环境对每个智能体来说是非平稳的（其他智能体在不断变化策略），训练容易震荡或收敛到局部最优，难以做到最优策略\n",
    "\n",
    "种种这些都会限制学习效果..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
