{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2b7c3acfea1c6b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 23.1 多智能体强化学习入门\n",
    "- 基本概念\n",
    "- IPPO 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478d1a2354f954b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "> 之前介绍的算法都是 **单智能体** 强化学习算法，其基本假设是动态环境是 **稳态的（stationary）**，即 **状态转移概率** 和 **奖励函数** 不变\n",
    "\n",
    "> 但在现实中，许多系统并非由单一主体组成，而是由 **多个智能体** 构成。在一些任务中，智能体可能需要合作以共同完成任务（如机器人协作、团队对抗游戏），也可能需要竞争以实现对抗模拟（如博弈论中的零和游戏）此时就要求环境中有多个智能体进行交互和学习，这样的任务被称为 **多智能体强化学习（multi-agent reinforcement learning，MARL）**\n",
    "\n",
    "> **MARL** 中的每个智能体在和环境交互的同时也在和 **其他智能体** 进行直接或者间接的**交互**，因此在每个智能体的视角下，环境是 **非稳态的（non-stationary）**：对于一个智能体而言，即使在 **相同的状态** 下采取 **相同的动作**，得到的状态转移和奖励信号的分布可能在**不断改变**\n",
    "> 所以多智能体的协作，本身会造就：\n",
    "- 训练的环境更为复杂\n",
    "> 其次:\n",
    "- 智能体目标可能是不同的，不同智能体需要最大化自己的利益\n",
    "- 训练难度更高，可能需要大规模分布式训练来提高效率\n",
    "\n",
    "***参考文献：***\n",
    "1. [多智能体深度强化学习的若干关键科学问题](https://kns.cnki.net/kcms2/article/abstract?v=hEVkP-djbGzQFZYseuDP5znRnCKidSEjzwYYgqH7xXiRLM90zl4RCyS7NqBJ9ADgnVnqy7EFO_RTQU-IvB4q7kgt0X64dkA9SwmnGm5AUMOwU9B38ZnNOfeVxU0gDSmgGyTGkNXHoPWIJwW0nmBKe-o19dokZVne9ebzRraoD1_TqMx5EDX-xQ==&uniplatform=NZKPT&language=CHS)\n",
    "2. [A Survey and Critique of Multiagent Deep Reinforcement Learning](https://arxiv.org/abs/1810.05587)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344c5e7bf346bfc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***多智能体环境表示：***\n",
    "> 可以用 **一个元组$(N,\\mathcal{S},\\mathcal{A},\\mathcal{R},P)$** 来表示 **一个多智能体环境**\n",
    "- 其中$N$是智能体的数目\n",
    "- $\\mathcal{S}=S_1\\times\\cdots\\times S_N$ 是 所有智能体的状态集合\n",
    "- $\\mathcal{A}=A_1\\times\\cdots\\times A_N$ 是 所有智能体的动作集合\n",
    "- $\\mathcal{R}=r_1\\times\\cdots\\times r_N$ 是 所有智能体奖励函数的集合\n",
    "- $P$是环境的状态转移概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea309dbf274bf86e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.1.1 多智能体强化学习的基本求解范式\n",
    "> 如果只是基于之前已经熟悉的 **单智能体算法**，拓展多智能体的概念，那么 **多智能体强化学习算法** 主要分两种思路：\n",
    "\n",
    "> 1. **完全中心化（fully centralized）方法** ：将多个智能体进行决策当作一个 **超级智能体** 在进行决策（*所有智能体的状态聚合在一起当作一个全局的超级状态；所有智能体的动作连起来作为一个联合动作*）。\n",
    "- 优点：由于已经知道了所有智能体的状态和动作，因此对这个 **超级智能体** 来说，**环境依旧是稳态**的，一些**单智能体的算法的收敛性**依旧可以得到保证。\n",
    "- 缺点：样的做法不能很好地扩展到智能体数量很多或者环境很大的情况，因为这时候将所有的信息简单暴力地拼在一起会**导致维度爆炸**，**训练复杂度巨幅提升**的问题往往不可解决。\n",
    "\n",
    "> 2. **完全去中心化（fully decentralized）方法** ：与完全中心化方法相反的范式便是假设 **每个智能体都在自身的环境中独立地进行学习，不考虑其他智能体的改变**。完全去中心化方法直接对 **每个智能体用一个单智能体强化学习算法来学习**。\n",
    "- 缺点：环境是非稳态的，智能体之间没有信息共享，训练的收敛性不能得到保证。\n",
    "- 优点：随着智能体数量的增加有比较好的扩展性，不会遇到维度灾难而导致训练不能进行下去。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb33398eb7ff0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.1.2 完全去中心化 算法 IPPO\n",
    "> 此类算法被称为 **独立学习（Independent Learning）**。由于对于每个智能体使用单智能体算法 **PPO（PPO-截断版本）** 进行训练，因此这个算法叫作 **独立 PPO（Independent PPO，IPPO）算法**。\n",
    "\n",
    "***算法流程：***\n",
    "\n",
    "对于$N$个智能体，**初始化** 每个智能体各自的策略以及价值函数\n",
    "for 训练轮数 $k= 0, 1, 2\\ldots$ do\n",
    "\n",
    "- 所有智能体在环境中交互 **分别获得各自的一条轨迹数据**\n",
    "- 对每个智能体，基于当前的价值函数用 GAE 计算优势函数的估计\n",
    "- 对每个智能体，通过最大化其 PPO-截断的目标来 **更新其策略**\n",
    "- 对每个智能体，通过均方误差损失函数 **优化其价值函数**\n",
    "\n",
    "end for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e123e8128dfd60",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.1.3 Combat 环境说明\n",
    "> **Combat 环境** 是 **ma_gym库**（[详细下载以及环境说明](https://github.com/koulanurag/ma-gym)） 中的一个多智能体环境：\n",
    "（由于该库所依赖的一些库的版本与之前算法使用的库版本不同，为了 **防止依赖冲突**，这里可以新建一个虚拟环境使用 **ma_gym库**）\n",
    "![Combat-v0](Illustrations/Combat-v0.gif)\n",
    "\n",
    "> 在一个二维的格子世界上，有两个队伍进行对战模拟游戏：\n",
    "> - 每个团队由 $m$ 个智能体组成，他们的初始位置以 $5 × 5$ 均匀采样 围绕团队中心形成方形\n",
    "> - 每个智能体的动作集合为：向四周移动 $1$ 格（离散动作：0，1，2，3），攻击周围（$3 × 3$）格范围内的敌对智能体，或者不采取任何行动\n",
    "> - 每个智能体的攻击有一轮的冷却时间\n",
    "> - 起初每个智能体有 $3$ 点生命值，如果智能体在敌人的攻击范围内被攻击到了，则会扣 $1$ 生命值，生命值掉为 $0$ 则死亡\n",
    "> - 如果一支队伍中的所有智能体都死亡，另一支队伍将获胜（当前版本默认最大步长为 100）\n",
    "\n",
    "**己方队伍（红色）** 移动策略由算法训练；**敌方队伍（蓝色）** 默认使用固定的算法：攻击在范围内最近的敌人，如果攻击范围内没有敌人，则向敌人靠近。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2094d0991a8ba4ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T14:11:47.317079200Z",
     "start_time": "2025-09-08T14:11:46.534153600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ma_gym.envs.combat.combat import Combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2bba5b1bb4d945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.134766600Z",
     "start_time": "2025-09-08T09:16:30.024317100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.advantage import compute_advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390435480c3d3f5b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 23.1.4 IPPO 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef93f29fa5a733",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PPO（PPO-截断版本）部分\n",
    "> 与之前的 **离散动作版本** 基本一致：\n",
    "1. 网络层数略微不同\n",
    "2. 一次更新中，只训练一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebe50dedff2a547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.138772300Z",
     "start_time": "2025-09-08T09:16:30.040865200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bdf12e7b8d49f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.140779600Z",
     "start_time": "2025-09-08T09:16:30.062014400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\" PPO算法,采用截断方式 \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps, gamma, device):\n",
    "        \n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        \n",
    "        states_np = np.array(transition_dict['states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        states = torch.tensor(states_np, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states_np = np.array(transition_dict['next_states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        next_states = torch.tensor(next_states_np, dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        \n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)  # 时序差分目标\n",
    "        td_delta = td_target - self.critic(states)                                 # 时序差分误差\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
    "        \n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach()\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        \n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage  # 截断\n",
    "        actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        #-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecc4e5cc7fa3e7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "###  IPPO 部分\n",
    "> 训练时使用 **参数共享（parameter sharing）** 的技巧：对于**所有智能体**使用 **同一套策略参数** 。\n",
    "> - 这样做的**好处**是，能使得模型训练数据更多，同时训练更稳定\n",
    "> - 这样做的**前提**是，两个智能体是同质的（homogeneous），即它们的状态空间和动作空间是完全一致的，并且它们的优化目标也完全一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76182016714ed01",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 环境设置："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf94e91a43dc59",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 演示:\n",
    "> 可通过该演示，观察环境的各个字段设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df826c80ae015e7d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-09-08T15:11:59.309628800Z",
     "start_time": "2025-09-08T15:11:31.525859900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 1\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 2\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 3\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 4\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 5\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 6\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 7\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 8\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 9\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 10\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 11\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 12\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 13\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 14\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 15\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 16\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 17\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 18\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 19\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 20\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 21\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 22\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 23\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 24\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 25\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 26\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 27\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 28\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 29\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 30\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 31\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 32\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 33\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 34\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 35\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 36\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 37\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 38\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 39\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 40\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 41\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 42\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 43\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 44\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 45\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 46\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 47\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 48\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 49\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 50\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 51\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 52\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 53\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 54\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 55\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 56\n",
      "[0, 0] {'health': {0: 3, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 57\n",
      "[-1, 0] {'health': {0: 2, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 58\n",
      "[0, 0] {'health': {0: 2, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 59\n",
      "[-1, 0] {'health': {0: 1, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 60\n",
      "[0, 0] {'health': {0: 1, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 61\n",
      "[-1, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 62\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 63\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 64\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 65\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 66\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 67\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 68\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 69\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 70\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 71\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 72\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 73\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 74\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 75\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 76\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 77\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 78\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 79\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 80\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 81\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 82\n",
      "[0, 0] {'health': {0: 0, 1: 3}} [False, False] dict_items([(0, 3), (1, 3)]) 83\n",
      "[0, -1] {'health': {0: 0, 1: 2}} [False, False] dict_items([(0, 3), (1, 3)]) 84\n",
      "[0, 1] {'health': {0: 0, 1: 2}} [False, False] dict_items([(0, 3), (1, 2)]) 85\n",
      "[0, -1] {'health': {0: 0, 1: 1}} [False, False] dict_items([(0, 3), (1, 2)]) 86\n",
      "[0, 0] {'health': {0: 0, 1: 1}} [False, False] dict_items([(0, 3), (1, 2)]) 87\n",
      "[0, 0] {'health': {0: 0, 1: 0}} [True, True] dict_items([(0, 3), (1, 1)]) 88\n"
     ]
    }
   ],
   "source": [
    "from ma_gym.envs.combat.combat import Combat\n",
    "import time\n",
    "\n",
    "# 创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2\n",
    "env1 = Combat(grid_shape=(15, 15), n_agents=2, n_opponents=2)\n",
    "\n",
    "# 开启渲染（mode=\"human\"）\n",
    "obs = env1.reset()\n",
    "done_n = [False] * env1.n_agents\n",
    "\n",
    "while not all(done_n):\n",
    "    actions = [env1.action_space[i].sample() for i in range(env1.n_agents)]\n",
    "    obs, reward_n, done_n, info = env1.step(actions)\n",
    "    print(reward_n, info, done_n, env1.opp_health.items(), env1._step_count)\n",
    "    env1.render(mode=\"human\")   # 可视化\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10a5128f5dae43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T13:46:11.738191300Z",
     "start_time": "2025-09-08T13:46:11.725157400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "env = Combat(grid_shape=(15, 15), n_agents=2, n_opponents=2, step_cost=-0.1)\n",
    "s = env.reset()\n",
    "print(env._max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106116513c9e07f7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99b0b0a981df7b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:33.425803400Z",
     "start_time": "2025-09-08T09:16:33.384802800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actor_lr = 3e-4\n",
    "critic_lr = 1e-3\n",
    "lmbda = 0.97\n",
    "eps = 0.2\n",
    "gamma = 0.99\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "state_dim = env.observation_space[0].shape[0]\n",
    "hidden_dim = 64\n",
    "action_dim = env.action_space[0].n\n",
    "#两个智能体共享同一个策略\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, \n",
    "            lmbda, eps, gamma, device)\n",
    "\n",
    "\n",
    "num_episodes = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fcdc69abdf03cc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 测试与训练："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e675ef6ae8b6d084",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***判断某一动作执行之后，己方智能体团队是否获胜：***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74e595f57120c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:17:48.753380600Z",
     "start_time": "2025-09-08T09:16:33.416780800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:  97%|█████████▋| 9714/10000 [37:13<01:05,  4.35it/s, episode=9700, win_rate=0.000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m terminal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m terminal:\n\u001B[1;32m---> 22\u001B[0m     a_1 \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtake_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m     a_2 \u001B[38;5;241m=\u001B[39m agent\u001B[38;5;241m.\u001B[39mtake_action(s[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m     24\u001B[0m     next_s, r, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep([a_1, a_2])\n",
      "Cell \u001B[1;32mIn[4], line 20\u001B[0m, in \u001B[0;36mPPO.take_action\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     18\u001B[0m action_dist \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdistributions\u001B[38;5;241m.\u001B[39mCategorical(probs)\n\u001B[0;32m     19\u001B[0m action \u001B[38;5;241m=\u001B[39m action_dist\u001B[38;5;241m.\u001B[39msample()\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43maction\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "win_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            transition_dict_1 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            transition_dict_2 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            s = env.reset()  # 返回 m 组状态\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                a_1 = agent.take_action(s[0])\n",
    "                a_2 = agent.take_action(s[1])\n",
    "                next_s, r, done, info = env.step([a_1, a_2])\n",
    "                \n",
    "                transition_dict_1['states'].append(s[0])\n",
    "                transition_dict_1['actions'].append(a_1)\n",
    "                transition_dict_1['next_states'].append(next_s[0])\n",
    "                # 如果敌方血量总和为 0，则认为赢了\n",
    "                transition_dict_1['rewards'].append(\n",
    "                    r[0] + 100 if (sum([v for k, v in env.opp_health.items()]) == 0) else r[0])\n",
    "                transition_dict_1['dones'].append(False)\n",
    "                \n",
    "                transition_dict_2['states'].append(s[1])\n",
    "                transition_dict_2['actions'].append(a_2)\n",
    "                transition_dict_2['next_states'].append(next_s[1])\n",
    "                transition_dict_2['rewards'].append(\n",
    "                    r[1] + 100 if (sum([v for k, v in env.opp_health.items()]) == 0) else r[1])\n",
    "                transition_dict_2['dones'].append(False)\n",
    "                \n",
    "                s = next_s\n",
    "                terminal = all(done)  # 到达 最大步长 或者 一方智能体全部没有血了\n",
    "                \n",
    "            win_list.append(1 if (sum([v for k, v in env.opp_health.items()]) == 0) else 0)\n",
    "            agent.update(transition_dict_1)\n",
    "            agent.update(transition_dict_2)\n",
    "            \n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'win_rate':\n",
    "                    '%.3f' % np.mean(win_list[-100:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226c7e30b84a483",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 效果展示：\n",
    "> 与之前的实验 **追踪训练过程中的回报** 不同，这里将 IPPO 训练的 **智能体团队的胜率** 作为算法指标，展示效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab39844f13aa0ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:50:25.838350200Z",
     "start_time": "2025-09-08T09:50:25.787717600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "win_array = np.array(win_list)\n",
    "# 每100条轨迹的获胜率取一次平均，用于绘图\n",
    "win_array = np.mean(win_array.reshape(-1, 100), axis=1)\n",
    "episodes_list = np.arange(win_array.shape[0]) * 100\n",
    "\n",
    "# 创建 DataFrame\n",
    "df1 = pd.DataFrame({'Episodes': episodes_list, 'Returns': win_array})\n",
    "# 保存为 CSV 文件\n",
    "df1.to_csv('IPPO_Clip_Combat_win-rate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940e454b13b6b09",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "df = pd.read_csv('IPPO_Clip_Combat_win-rate_data.csv')  # 从 CSV 文件中读取数据\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df['Episodes'], y=df['win-rate'], mode='lines', name='win-rate'))\n",
    "fig.update_layout(\n",
    "    title='IPPO_Clip on Combat',\n",
    "    xaxis_title='Episodes',\n",
    "    yaxis_title='win-rate',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274520958079b8ab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看出，当 **智能体数量较少** 的时候，IPPO 这种完全去中心化学习 **在一定程度上能够取得好的效果**，**但是最终达到的胜率也比较有限**。可能有以下几点原因：\n",
    "- 奖励设置依然稀疏，难以促进团队协同\n",
    "- 只依赖个体局部观测和局部奖励，智能体之间没有共享信息，难以形成协助\n",
    "- 环境对每个智能体来说是非平稳的（其他智能体在不断变化策略），训练容易震荡或收敛到局部最优，难以做到最优策略\n",
    "\n",
    "种种这些都会限制学习效果..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
