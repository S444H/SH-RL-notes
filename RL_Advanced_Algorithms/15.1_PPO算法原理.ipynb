{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 15.PPO（Proximal Policy Optimization）：近端策略优化\n",
    "> **Proximal Policy Optimization (PPO) 算法** 在 2017 年由 John Schulman 等人提出：[Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
    "> **TRPO** 计算的复杂性较高，尤其是在需要计算海森矩阵和执行二阶优化时，导致其不适用于计算资源有限的情况。**PPO** 基于 **TRPO**  的思想，但是其算法实现更加简单。并且大量的实验结果表明，与 **TRPO**  相比，**PPO** 能学习得一样好（甚至更快），这使得 **PPO** 成为非常流行的强化学习算法。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4f1413da964af82"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 15.1 PPO 算法原理\n",
    "> **TRPO** 使用 **泰勒展开近似、共轭梯度、线性搜索等** 方法直接求解：\n",
    "$$\\max_{\\theta} \\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_k}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta^{\\prime}}(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a)\\right]$$\n",
    "$$\\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_k}}}[D_{KL}(\\pi_{\\theta_k}(\\cdot|s),\\pi_{\\theta^{\\prime}}(\\cdot|s))]\\leq \\delta$$\n",
    "\n",
    "- **PPO** 采用了相对简单且高效的 ***方法*** 来实现这一目标\n",
    "- ***方法*** 主要有两种形式：**Clipped Surrogate Objective (剪切目标函数) 、 Adaptive KL Penalty (自适应 KL 惩罚)** "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1937c9ad9f37fbda"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### *Clipped Surrogate Objective (剪切目标函数)*\n",
    "> 这种形式是 **PPO** 最常用的方式，因为在大多数任务中能够提供较好的效果，并且实现起来更加简单和高效\n",
    "> 它通过引入 ***剪切机制*** 来约束策略更新的幅度：\n",
    "$$\\arg\\max_{\\theta}\\mathbb{E}_{s\\sim\\nu^{\\pi_{\\theta_k}}}\\mathbb{E}_{a\\sim\\pi_{\\theta_k}(\\cdot|s)}\\left[\\min\\left(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a) , \\operatorname{clip}\\left(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon,1+\\epsilon\\right)A^{\\pi_{\\theta_k}}(s,a)\\right)\\right]$$\n",
    "\n",
    "- $\\epsilon$ 是一个小的常数（例如0.1或0.2）,控制剪切的范围\n",
    "- $\\operatorname{clip}(x,l,r)$：把 $x$ 限制在 $[l,r]$ 内\n",
    "\n",
    "> $A^{\\pi_{\\theta_k}}(s,a)>0$，说明这个动作的价值高于平均，最大化这个式子会增大$\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(a|s)}$，但不会让其超过$1+\\epsilon$；\n",
    "> $A^{\\pi_{\\theta_k}}(s,a)<0$，最大化这个式子会减小$\\frac{\\pi_\\theta(\\alpha|s)}{\\pi_{\\theta_k}(a|s)}$，但不会让其超过$1-\\epsilon$。**以此避免策略更新过大，保持更好的稳定性**\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1c393bfb782bb00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 可见 **PPO-Clip** 避免了使用 **KL 散度约束**，而是使用 **剪切机制（Clipping）** 来替代。目标函数变为：\n",
    "$$L^{\\mathrm{clip}}(\\theta)=\\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t,\\mathrm{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "- $r_t(\\theta):$ 是当前策略和旧策略的概率比率：\n",
    "\n",
    "$$r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_\\mathrm{old}}(a_t|s_t)}$$\n",
    "- 剪切目标函数通过限制$r_t(\\theta)$的范围来 **控制策略更新**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9832d91dca0eeba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### *Adaptive KL Penalty (自适应 KL 惩罚)*\n",
    "> **PPO-惩罚（PPO-Penalty）** 则是用 **拉格朗日乘数法** 直接将 KL 散度的限制放进了目标函数中，这就变成了一个 **无约束** 的优化问题，在迭代的过程中不断更新 KL 散度前的系数：\n",
    "$$\\arg\\max_{\\theta}\\mathbb{E}_{s\\sim\\nu}\\mathbb{E}_{a\\sim\\pi_{\\theta_{k}}(\\cdot|s)}\\left[\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_{k}}(a|s)}A^{\\pi_{\\theta_{k}}}(s,a)-\\beta D_{KL}[\\pi_{\\theta_{k}}(\\cdot|s),\\pi_{\\theta}(\\cdot|s)]\\right]$$\n",
    "\n",
    "> 设：\n",
    "- $d_k = D_{KL}^{\\nu_{\\theta_k}}(\\pi_{\\theta_k}, \\pi_{\\theta})$ 是当前策略$\\pi_{\\theta}$与旧策略$\\pi_{\\theta_k}$之间的平均 KL 散度\n",
    "- $\\beta_k$是当前轮次的 KL 惩罚系数\n",
    "- $\\delta$是事先设定的一个超参数，用于限制学习策略和之前一轮策略的差距\n",
    "\n",
    "> 在 **TRPO** 中，KL 散度是一个硬约束，在 **PPO-惩罚（PPO-Penalty）** 则是：\n",
    "\n",
    ">| 当前 KL 距离 $d_k$ 与目标值 $\\delta$ 的关系 | 更新后的 $\\beta$                     |\n",
    "|----------------------------------|----------------------------------|\n",
    "| $d_k < \\delta / 1.5$             | $\\beta_{k+1} = \\beta_k / 2$      |\n",
    "| $d_k > \\delta \\times 1.5$        | $\\beta_{k+1} = \\beta_k \\times 2$ |\n",
    "| 其他情况                             | $\\beta_{k+1} = \\beta_k$          |\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "987c22ec578a1d4c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 综上\n",
    "- ***PPO 省去了共轭梯度求解这一步***\n",
    "- ***PPO 的优化过程可以全在 minibatch +自适应优化器 (如 Adam)中完成，适合大规模并行训练***"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58fcff8729e41588"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
