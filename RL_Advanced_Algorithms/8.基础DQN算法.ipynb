{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 8.基础 DQN 算法\n",
    "- 在 2013 年的 NIPS 深度学习研讨会上，DeepMind 公司的研究团队发表了 DQN 论文，首次展示了这一直接通过卷积神经网络接受像素输入来玩转各种雅达利（Atari）游戏的强化学习算法，由此拉开了 **深度强化学习** 的序幕..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "985c5ce37259e9ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入相关库"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37738b56fd40c38b"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# 基本库\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# 神经网络\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# Gymnasium 是一个用于开发和测试强化学习算法的工具库，为 OpenAI Gym 的更新版本（2021迁移开发）\n",
    "import gymnasium as gym\n",
    "# 自定义库（经之前的多次学习发现许多功能代码片段被复用,于是将这些功能包装到一个库中,便于调用）\n",
    "import rl_utils"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:31:25.552040800Z",
     "start_time": "2025-07-12T05:31:25.502819800Z"
    }
   },
   "id": "bc3eccd692eb79b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.1 引出\n",
    "第6节学习的Q-learning算法需要依赖一张Q表，将所有状态的Q值进行储存(如下图) :\n",
    "![Q表示意图](Illustrations/Q表示意图.png)\n",
    "即使状态向量仅有5个维度，将每一个维度的数值映射到 0∼5 六个整数上，状态依然达 $6^{5}=7776$ 种，Q表的大小则为 $7776\\times5=38880$ \n",
    "而实际研究中多为 **连续状态空间** ，肯定不止6个整数，状态空间将会指数级增大，造成维度灾难，对计算硬件的要求很高，\n",
    "同时将状态连续值映射到离散的区间，来降低环境的复杂程度，也会导致智能体无法精确地捕捉环境的动态变化细节，使算法性能降低，\n",
    "对此，可以用 **函数拟合（function approximation）** 的方法来估计Q值，直接使用连续状态值，对此引出**DQN算法**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf311880130dae7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.2 原理\n",
    "**深度 Q 网络算法（Deep Q-Networks（DQN））** ，结合了 **深度神经网络(具有强大的表达能力)** 和 Q-learning，引入了深度神经网络来逼近Q值，能够处理高维连续的状态空间，无需对本就连续的状态向量进行离散映射（为解决 **连续状态下离散动作** 的问题）\n",
    "假设神经网络用来拟合函数的参数是 $\\omega$，即每一个状态下s所有可能动作a的值都能表示为 **$Q_{\\omega}(s,a)$** ，而用于拟合函数 **$Q_{\\omega}(s,a)$** 的神经网络称为 **Q网络**\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6380f9235298849f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 神经网络的损失函数\n",
    "**Q网络** 的**损失函数**则构造为均方误差的形式：\n",
    "$$\\omega^*=\\arg\\min_\\omega\\frac{1}{2N}\\sum_{i=1}^N\\left[Q_\\omega\\left(s_i,a_i\\right)-\\left(r_i+\\gamma\\max_{a^{\\prime}}Q_\\omega\\left(s_i^{\\prime},a^{\\prime}\\right)\\right)\\right]^2$$\n",
    "- 除以2N是为了简化求导后的参数梯度\n",
    "\n",
    "蕴意价值函数增量更新的幅度越小则损失越小，以此损失函数为优化目标愈使 **Q网络** 所拟合的函数所选择的动作价值越大"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75c0b031bdfb1af4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 经验回放 + 目标网络\n",
    "DQN 是Q-learning算法的一种扩展，也是一种离线策略算法，可以将收集到的数据存储起来在后续的训练中使用\n",
    "DQN 引入了两个非常重要的模块——**经验回放和目标网络**，利用收集到的数据，它们能够帮助 DQN 取得稳定、出色的性能"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c91b719cd2fc981"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 经验回放\n",
    "Q-learning算法，每一个数据只会用来更新一次Q值\n",
    "而一般的**有监督学习**，训练数据独立同分布时，在训练神经网络的时候，训练集当中的每个数据会被使用多次\n",
    "所以 DQN 算法采用了 **经验回放（experience replay）** 方法：维护一个**回放缓冲区**，将每次从环境中采样得到的四元组数据（状态、动作、奖励、下一状态）存储到回放缓冲区中，训练 Q 网络的时候再从回放缓冲区中随机采样若干数据来进行训练。这么做解决了两个问题：\n",
    "1. 提高样本效率：每一个样本可以被使用多次，十分适合深度神经网络的梯度学习.\n",
    "2. 使样本满足独立假设（在 **马尔可夫决策过程（MDP）** 中交互采样得到的数据本身是不满足独立假设的，因为这一时刻的状态和上一时刻的状态有关）：非独立同分布的数据对训练神经网络有很大的影响，会使神经网络拟合到最近训练的数据上。采用经验回放可以**打破样本之间的相关性**，让其满足独立假设."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "72810d4100b3fe40"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# 也被打包进 rl_utils\n",
    "class ReplayBuffer:\n",
    "    \"\"\" 经验回放池 \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:31:25.574787Z",
     "start_time": "2025-07-12T05:31:25.514875800Z"
    }
   },
   "id": "f3f663fa0842fcc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 目标网络\n",
    "**目标网络（target network）** 的引入是为了增强神经网络对于全局的把控，增强神经网络训练的稳定性。为实现这一目的需要两套神经网络：**训练网络与目标网络**.\n",
    "**训练网络$Q_{\\omega}(s,a)$** 为实时更新参数，而 **目标网络$Q_{\\omega^{-}}(s,a)$** 则会滞后，每隔一定步数才会与训练网络同步参数.\n",
    "其中需要明确的是，对于**损失函数**：$$\\frac{1}{2}[Q_{\\omega}\\left(s,a\\right)-\\left(r+\\gamma\\max_{a^{\\prime}}Q_{\\omega^{-}}\\left(s^{\\prime},a^{\\prime}\\right)\\right)]^{2}$$\n",
    "-  Q值的计算是依据不同两套Q神经网络的\n",
    "\n",
    "以此,促成训练稳定的效果.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b909842e054504ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.3 CartPole环境\n",
    "为实现该算法,引入模拟环境: **车杆环境(连续状态,离散动作)**\n",
    "![CartPole环境示意图](Illustrations/cartpole环境.gif)\n",
    "\n",
    "#### 规则\n",
    "- 智能体的任务是通过左右移动保持车上的杆竖直，若杆的倾斜度数过大，或者车子离初始位置左右的偏离程度过大，或者坚持时间到达 200 帧，则游戏结束\n",
    "在游戏中每坚持一帧，智能体能获得分数为 1 的奖励，坚持时间越长，则最后的分数越高，坚持 200 帧即可获得最高的分数\n",
    "#### 详情\n",
    "- 智能体的状态是一个4维向量(连续)：\n",
    "![CartPole环境的状态空间](Illustrations/CartPole环境的状态空间.png)\n",
    "- 动作空间大小为2(离散)：\n",
    "![CartPole环境的动作空间](Illustrations/CartPole环境的动作空间.png)\n",
    "- 工作在CartPole环境中的Q网络：\n",
    "![在CartPole环境中的Q网络](Illustrations/在CartPole环境中的Q网络.png)\n",
    "神经网络的输出在动作无限的情况下可以是一个标量\n",
    "若有限(如以上只有两种动作)，可以为每一个动作输出一个Q值，选max"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b02d8d397d91b129"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**CartPole环境** 状态空间相对简单，只有 4 个变量，因此之后的网络结构的代码设计也相对简单：采用一层 128 个神经元的全连接并以 ReLU 作为激活函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9409e0aec3fc1a3"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class Qnet(torch.nn.Module):\n",
    "    \"\"\"只有一层隐藏层的Q网络 \"\"\" \n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # 隐藏层使用ReLU激活函数\n",
    "        return self.fc2(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:31:25.632711400Z",
     "start_time": "2025-07-12T05:31:25.522989800Z"
    }
   },
   "id": "71422e0ee014e46b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 8.4 DQN算法代码实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5741a38a94ab296b"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    \"\"\" DQN算法 \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, learning_rate, gamma,\n",
    "                 epsilon, target_update, device):\n",
    "        self.action_dim = action_dim\n",
    "        self.q_net = Qnet(state_dim, hidden_dim,\n",
    "                          self.action_dim).to(device)  # Q网络\n",
    "        # 目标网络\n",
    "        self.target_q_net = Qnet(state_dim, hidden_dim,\n",
    "                                 self.action_dim).to(device)\n",
    "        # 使用Adam优化器\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(),\n",
    "                                          lr=learning_rate)\n",
    "        self.gamma = gamma  # 折扣因子\n",
    "        self.epsilon = epsilon  # epsilon-贪婪策略\n",
    "        self.target_update = target_update  # 目标网络更新频率\n",
    "        self.count = 0  # 计数器,记录更新次数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):  # epsilon-贪婪策略采取动作\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.randint(self.action_dim)\n",
    "        else:\n",
    "            state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "            action = self.q_net(state).argmax().item()\n",
    "        return action\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)  # Q值\n",
    "        # 下个状态的最大Q值\n",
    "        max_next_q_values = self.target_q_net(next_states).max(1)[0].view(\n",
    "            -1, 1)\n",
    "        q_targets = rewards + self.gamma * max_next_q_values * (1 - dones\n",
    "                                                                )  # TD误差目标\n",
    "        dqn_loss = torch.mean(F.mse_loss(q_values, q_targets))  # 均方误差损失函数\n",
    "        self.optimizer.zero_grad()  # PyTorch中默认梯度会累积,这里需要显式将梯度置为0\n",
    "        dqn_loss.backward()  # 反向传播更新参数\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update == 0:\n",
    "            self.target_q_net.load_state_dict(\n",
    "                self.q_net.state_dict())  # 更新目标网络\n",
    "        self.count += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:31:25.650753600Z",
     "start_time": "2025-07-12T05:31:25.530746100Z"
    }
   },
   "id": "b8f54c8d6e980fe5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "超参数设置:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "df412e2db3b30f06"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "(array([ 0.01369617, -0.02302133, -0.04590265, -0.04834723], dtype=float32),\n {})"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Env设置(包括可重复性随机种子, 确保每次重新运行本节内容是结果一致)\n",
    "random.seed(0)       # 设置 Python 的随机种子\n",
    "np.random.seed(0)    # 设置 NumPy 的随机种子\n",
    "torch.manual_seed(0) # 设置 PyTorch CPU 随机种子\n",
    "torch.cuda.manual_seed_all(0) # 设置 PyTorch GPU 随机种子, 由于GPU并行性, 只能极大减小偏差\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped # 获取原始环境（绕过 TimeLimit 包装器）\n",
    "env.reset(seed=0)   # 环境通常依赖于其他随机数生成器来初始化状态、进行探索(推荐位于以上随机之后)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:31:25.664339800Z",
     "start_time": "2025-07-12T05:31:25.540353700Z"
    }
   },
   "id": "6f9f7a02ff9c0cda"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.01323574, -0.21745604, -0.04686959,  0.22950698], dtype=float32), 1.0, False, False, {})\n"
     ]
    }
   ],
   "source": [
    "# Gymnasium返回值不一样\n",
    "re = env.step(0)\n",
    "print(re) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:31:25.665341Z",
     "start_time": "2025-07-12T05:31:25.548136600Z"
    }
   },
   "id": "80cf4c8aa304b138"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n"
     ]
    }
   ],
   "source": [
    "# Agent\n",
    "state_dim = env.observation_space.shape[0]\n",
    "hidden_dim = 128\n",
    "action_dim = env.action_space.n\n",
    "print(state_dim,action_dim)\n",
    "lr = 2e-3\n",
    "gamma = 0.98\n",
    "epsilon = 0.01\n",
    "target_update = 10\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "agent = DQN(state_dim, hidden_dim, action_dim, lr, gamma, epsilon, target_update, device)\n",
    "\n",
    "num_episodes = 500\n",
    "buffer_size = 10000\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 经验回收池\n",
    "minimal_size = 500\n",
    "batch_size = 64"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:31:25.666846300Z",
     "start_time": "2025-07-12T05:31:25.557640200Z"
    }
   },
   "id": "5f14fa136d4faf9a"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 4 at dim 2 (got 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m rl_utils.train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\GitHub\\SH_RL_learning\\RL_Advanced_Algorithms\\rl_utils.py:69\u001B[39m, in \u001B[36mtrain_off_policy_agent\u001B[39m\u001B[34m(env, agent, num_episodes, replay_buffer, minimal_size, batch_size)\u001B[39m\n\u001B[32m     67\u001B[39m done = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m     action = agent.take_action(state)\n\u001B[32m     70\u001B[39m     next_state, reward, done, other, _ = env.step(action)  \u001B[38;5;66;03m# Gymnasium返回值不一样\u001B[39;00m\n\u001B[32m     71\u001B[39m     replay_buffer.add(state, action, reward, next_state, done)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 24\u001B[39m, in \u001B[36mDQN.take_action\u001B[39m\u001B[34m(self, state)\u001B[39m\n\u001B[32m     22\u001B[39m     action = np.random.randint(\u001B[38;5;28mself\u001B[39m.action_dim)\n\u001B[32m     23\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m24\u001B[39m     state = torch.tensor([state], dtype=torch.float).to(\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m     25\u001B[39m     action = \u001B[38;5;28mself\u001B[39m.q_net(state).argmax().item()\n\u001B[32m     26\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m action\n",
      "\u001B[31mValueError\u001B[39m: expected sequence of length 4 at dim 2 (got 0)"
     ]
    }
   ],
   "source": [
    "rl_utils.train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-12T05:31:25.667852400Z",
     "start_time": "2025-07-12T05:31:25.566745300Z"
    }
   },
   "id": "e6837bf566d2ef72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            episode_return = 0\n",
    "            state, info = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                action = agent.take_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)  # Gymnasium返回值不一样\n",
    "                replay_buffer.add(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                episode_return += reward\n",
    "                # 当buffer数据的数量超过一定值后,才进行Q网络训练\n",
    "                if replay_buffer.size() > minimal_size:\n",
    "                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                    transition_dict = {\n",
    "                        'states': b_s,\n",
    "                        'actions': b_a,\n",
    "                        'next_states': b_ns,\n",
    "                        'rewards': b_r,\n",
    "                        'dones': b_d\n",
    "                    }\n",
    "                    agent.update(transition_dict)\n",
    "            return_list.append(episode_return)\n",
    "            if (i_episode + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-07-12T05:31:25.633713200Z"
    }
   },
   "id": "765b3c0610f9247b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "episodes_list = list(range(len(return_list)))\n",
    "plt.plot(episodes_list, return_list)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('DQN on {}'.format('CartPole-v1'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-07-12T05:31:25.635733500Z"
    }
   },
   "id": "28200ee755f5209d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "去噪"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a7a1463fa016bc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mv_return = rl_utils.moving_average(return_list, 9)\n",
    "plt.plot(episodes_list, mv_return)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Returns')\n",
    "plt.title('DQN on {}'.format('CartPole-v1'))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-07-12T05:31:25.638226100Z"
    }
   },
   "id": "c44f4776110924b8"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "28eefe4c85232448"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
