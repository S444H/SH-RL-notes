{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2b7c3acfea1c6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 23.1 多智能体强化学习入门\n",
    "- 基本概念\n",
    "- IPPO 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478d1a2354f954b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "> 之前介绍的算法都是 **单智能体** 强化学习算法，其基本假设是动态环境是 **稳态的（stationary）**，即 **状态转移概率** 和 **奖励函数** 不变\n",
    "\n",
    "> 但在现实中，许多系统并非由单一主体组成，而是由 **多个智能体** 构成。在一些任务中，智能体可能需要合作以共同完成任务（如机器人协作、团队对抗游戏），也可能需要竞争以实现对抗模拟（如博弈论中的零和游戏）此时就要求环境中有多个智能体进行交互和学习，这样的任务被称为 **多智能体强化学习（multi-agent reinforcement learning，MARL）**\n",
    "\n",
    "> **MARL** 中的每个智能体在和环境交互的同时也在和 **其他智能体** 进行直接或者间接的**交互**，因此在每个智能体的视角下，环境是 **非稳态的（non-stationary）**：对于一个智能体而言，即使在 **相同的状态** 下采取 **相同的动作**，得到的状态转移和奖励信号的分布可能在**不断改变**\n",
    "> 所以多智能体的协作，本身会造就：\n",
    "- 训练的环境更为复杂\n",
    "> 其次:\n",
    "- 智能体目标可能是不同的，不同智能体需要最大化自己的利益\n",
    "- 训练难度更高，可能需要大规模分布式训练来提高效率\n",
    "\n",
    "***参考文献：***\n",
    "1. [多智能体深度强化学习的若干关键科学问题](https://kns.cnki.net/kcms2/article/abstract?v=hEVkP-djbGzQFZYseuDP5znRnCKidSEjzwYYgqH7xXiRLM90zl4RCyS7NqBJ9ADgnVnqy7EFO_RTQU-IvB4q7kgt0X64dkA9SwmnGm5AUMOwU9B38ZnNOfeVxU0gDSmgGyTGkNXHoPWIJwW0nmBKe-o19dokZVne9ebzRraoD1_TqMx5EDX-xQ==&uniplatform=NZKPT&language=CHS)\n",
    "2. [A Survey and Critique of Multiagent Deep Reinforcement Learning](https://arxiv.org/abs/1810.05587)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9344c5e7bf346bfc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "***多智能体环境表示：***\n",
    "> 可以用 **一个元组$(N,\\mathcal{S},\\mathcal{A},\\mathcal{R},P)$** 来表示 **一个多智能体环境**\n",
    "- 其中$N$是智能体的数目\n",
    "- $\\mathcal{S}=S_1\\times\\cdots\\times S_N$ 是 所有智能体的状态集合\n",
    "- $\\mathcal{A}=A_1\\times\\cdots\\times A_N$ 是 所有智能体的动作集合\n",
    "- $\\mathcal{R}=r_1\\times\\cdots\\times r_N$ 是 所有智能体奖励函数的集合\n",
    "- $P$是环境的状态转移概率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea309dbf274bf86e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 23.1.1 多智能体强化学习的基本求解范式\n",
    "> 如果只是基于之前已经熟悉的 **单智能体算法**，拓展多智能体的概念，那么 **多智能体强化学习算法** 主要分两种思路：\n",
    "\n",
    "> 1. **完全中心化（fully centralized）方法** ：将多个智能体进行决策当作一个 **超级智能体** 在进行决策（*所有智能体的状态聚合在一起当作一个全局的超级状态；所有智能体的动作连起来作为一个联合动作*）。\n",
    "- 优点：由于已经知道了所有智能体的状态和动作，因此对这个 **超级智能体** 来说，**环境依旧是稳态**的，一些**单智能体的算法的收敛性**依旧可以得到保证。\n",
    "- 缺点：样的做法不能很好地扩展到智能体数量很多或者环境很大的情况，因为这时候将所有的信息简单暴力地拼在一起会**导致维度爆炸**，**训练复杂度巨幅提升**的问题往往不可解决。\n",
    "\n",
    "> 2. **完全去中心化（fully decentralized）方法** ：与完全中心化方法相反的范式便是假设 **每个智能体都在自身的环境中独立地进行学习，不考虑其他智能体的改变**。完全去中心化方法直接对 **每个智能体用一个单智能体强化学习算法来学习**。\n",
    "- 缺点：环境是非稳态的，智能体之间没有信息共享，训练的收敛性不能得到保证。\n",
    "- 优点：随着智能体数量的增加有比较好的扩展性，不会遇到维度灾难而导致训练不能进行下去。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb33398eb7ff0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 23.1.2 完全去中心化 算法 IPPO\n",
    "> 此类算法被称为 **独立学习（Independent Learning）**。由于对于每个智能体使用单智能体算法 **PPO（PPO-截断版本）** 进行训练，因此这个算法叫作 **独立 PPO（Independent PPO，IPPO）算法**。\n",
    "\n",
    "***算法流程：***\n",
    "\n",
    "对于$N$个智能体，**初始化** 每个智能体各自的策略以及价值函数\n",
    "for 训练轮数 $k= 0, 1, 2\\ldots$ do\n",
    "\n",
    "- 所有智能体在环境中交互 **分别获得各自的一条轨迹数据**\n",
    "- 对每个智能体，基于当前的价值函数用 GAE 计算优势函数的估计\n",
    "- 对每个智能体，通过最大化其 PPO-截断的目标来 **更新其策略**\n",
    "- 对每个智能体，通过均方误差损失函数 **优化其价值函数**\n",
    "\n",
    "end for\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e123e8128dfd60",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 23.1.3 Combat 环境说明\n",
    "> **Combat 环境** 是 **ma_gym库**（[详细下载以及环境说明](https://github.com/koulanurag/ma-gym)） 中的一个多智能体环境：\n",
    "（由于该库所依赖的一些库的版本与之前算法使用的库版本不同，为了 **防止依赖冲突**，这里可以新建一个虚拟环境使用 **ma_gym库**）\n",
    "![Combat-v0](Illustrations/Combat-v0.gif)\n",
    "\n",
    "> 在一个二维的格子世界上，有两个队伍进行对战模拟游戏：\n",
    "> - 每个团队由 $m$ 个智能体组成，他们的初始位置以 $5 × 5$ 均匀采样 围绕团队中心形成方形\n",
    "> - 每个智能体的动作集合为：向四周移动 $1$ 格（离散动作：0，1，2，3），攻击周围（$3 × 3$）格范围内的敌对智能体，或者不采取任何行动\n",
    "> - 每个智能体的攻击有一轮的冷却时间\n",
    "> - 起初每个智能体有 $3$ 点生命值，如果智能体在敌人的攻击范围内被攻击到了，则会扣 $1$ 生命值，生命值掉为 $0$ 则死亡\n",
    "> - 如果一支队伍中的所有智能体都死亡，另一支队伍将获胜（当前版本默认最大步长为 100）\n",
    "\n",
    "**己方队伍** 移动策略由算法训练；**敌方队伍** 默认使用固定的算法：攻击在范围内最近的敌人，如果攻击范围内没有敌人，则向敌人靠近。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2094d0991a8ba4ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.059005600Z",
     "start_time": "2025-09-08T09:16:30.012246Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from ma_gym.envs.combat.combat import Combat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2bba5b1bb4d945",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.134766600Z",
     "start_time": "2025-09-08T09:16:30.024317100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.smoothing import moving_average\n",
    "from utils.advantage import compute_advantage\n",
    "from utils.training import train_on_policy_agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390435480c3d3f5b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 23.1.4 IPPO 代码实践"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cef93f29fa5a733",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### PPO（PPO-截断版本）部分\n",
    "> 与之前的 **离散动作版本** 基本一致：\n",
    "1. 网络层数略微不同\n",
    "2. 一次更新中，只训练一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebe50dedff2a547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.138772300Z",
     "start_time": "2025-09-08T09:16:30.040865200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return F.softmax(self.fc3(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc2(F.relu(self.fc1(x))))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bdf12e7b8d49f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:30.140779600Z",
     "start_time": "2025-09-08T09:16:30.062014400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    \"\"\" PPO算法,采用截断方式 \"\"\"\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda, eps, gamma, device):\n",
    "        \n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "\n",
    "    \n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        \n",
    "        states_np = np.array(transition_dict['states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        states = torch.tensor(states_np, dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states_np = np.array(transition_dict['next_states'])  # 转换成统一的大 np.ndarray，PyTorch更高效处理\n",
    "        next_states = torch.tensor(next_states_np, dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        \n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 - dones)  # 时序差分目标\n",
    "        td_delta = td_target - self.critic(states)                                 # 时序差分误差\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda, td_delta.cpu()).to(self.device)\n",
    "        \n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,actions)).detach()\n",
    "        \n",
    "        #----------------------------------------------------------------------------\n",
    "        log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "        ratio = torch.exp(log_probs - old_log_probs)\n",
    "        \n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1 - self.eps, 1 + self.eps) * advantage  # 截断\n",
    "        actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detach()))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        self.critic_optimizer.step()\n",
    "        #-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ecc4e5cc7fa3e7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "###  IPPO 部分\n",
    "> 训练时使用 **参数共享（parameter sharing）** 的技巧：对于**所有智能体**使用 **同一套策略参数** 。\n",
    "> - 这样做的**好处**是，能使得模型训练数据更多，同时训练更稳定\n",
    "> - 这样做的**前提**是，两个智能体是同质的（homogeneous），即它们的状态空间和动作空间是完全一致的，并且它们的优化目标也完全一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76182016714ed01",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 环境设置："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf94e91a43dc59",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### 演示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df826c80ae015e7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:33.370253200Z",
     "start_time": "2025-09-08T09:16:30.072087100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -1] {'health': {0: 3, 1: 2}} [False, False]\n",
      "[0, 0] {'health': {0: 3, 1: 2}} [False, False]\n",
      "[-1, 0] {'health': {0: 2, 1: 2}} [False, False]\n",
      "[0, 0] {'health': {0: 2, 1: 2}} [False, False]\n",
      "[0, -1] {'health': {0: 2, 1: 1}} [False, False]\n",
      "[0, 1] {'health': {0: 2, 1: 1}} [False, False]\n",
      "[-1, -1] {'health': {0: 1, 1: 0}} [False, False]\n",
      "[0, 0] {'health': {0: 1, 1: 0}} [False, False]\n",
      "[-1, 0] {'health': {0: 0, 1: 0}} [True, True]\n"
     ]
    }
   ],
   "source": [
    "from ma_gym.envs.combat.combat import Combat\n",
    "import time\n",
    "\n",
    "# 创建环境\n",
    "env1 = Combat(grid_shape=(15, 15), n_agents=2, n_opponents=2, max_steps=10)\n",
    "\n",
    "# 开启渲染（mode=\"human\"）\n",
    "obs = env1.reset()\n",
    "done_n = [False] * env1.n_agents\n",
    "\n",
    "while not all(done_n):\n",
    "    actions = [env1.action_space[i].sample() for i in range(env1.n_agents)]\n",
    "    obs, reward_n, done_n, info = env1.step(actions)\n",
    "    print(reward_n, info, done_n)\n",
    "    env1.render(mode=\"human\")   # 可视化\n",
    "    time.sleep(0.3)\n",
    "    \n",
    "env1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10a5128f5dae43f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:33.391827Z",
     "start_time": "2025-09-08T09:16:33.370253200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "team_size = 2  # m\n",
    "grid_size = (15, 15)\n",
    "#创建Combat环境，格子世界的大小为15x15，己方智能体和敌方智能体数量都为2\n",
    "env = Combat(grid_shape=grid_size, n_agents=team_size, n_opponents=team_size)\n",
    "s = env.reset()\n",
    "print(env._max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106116513c9e07f7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 参数设置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a99b0b0a981df7b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:33.425803400Z",
     "start_time": "2025-09-08T09:16:33.384802800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "actor_lr = 3e-4\n",
    "critic_lr = 1e-3\n",
    "lmbda = 0.97\n",
    "eps = 0.2\n",
    "gamma = 0.99\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "state_dim = env.observation_space[0].shape[0]\n",
    "hidden_dim = 64\n",
    "action_dim = env.action_space[0].n\n",
    "#两个智能体共享同一个策略\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, \n",
    "            lmbda, eps, gamma, device)\n",
    "\n",
    "\n",
    "num_episodes = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fcdc69abdf03cc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 测试与训练：\n",
    "> 与之前的实验不同：这里不再展示智能体获得的**回报**，而是将 IPPO 训练的**两个智能体团队的胜率**作为主要的实验结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93419efebc60e36b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:16:33.437928500Z",
     "start_time": "2025-09-08T09:16:33.400593800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def is_win(done, t, max_steps, info):\n",
    "    \"\"\"\n",
    "    判断当前回合是否获胜\n",
    "\n",
    "    参数:\n",
    "    - done_n: list, 每个己方智能体的 done 状态\n",
    "    - t: int, 当前步数\n",
    "    - max_steps: int, 最大步长\n",
    "    - info: dict, 当前环境信息，包含 'health' 字段 {agent_id: health_value}\n",
    "\n",
    "    返回:\n",
    "    - bool, True 表示获胜，False 表示未获胜\n",
    "    \"\"\"\n",
    "    # 条件1: 当前步数未超过最大步长\n",
    "    if t >= max_steps:\n",
    "        return False\n",
    "    \n",
    "    # 条件2: 所有己方智能体 done=True\n",
    "    if not all(done):\n",
    "        return False\n",
    "    \n",
    "    # 条件3: 己方生命值不全为零\n",
    "    health_values = list(info['health'].values())\n",
    "    if all(h == 0 for h in health_values):\n",
    "        return False\n",
    "    \n",
    "    # 满足以上条件，判定胜利\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b74e595f57120c7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-08T09:17:48.753380600Z",
     "start_time": "2025-09-08T09:16:33.416780800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   6%|▌         | 601/10000 [01:52<29:21,  5.34it/s, episode=600, return=0.020]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminal:\n\u001b[1;32m---> 23\u001b[0m     a_1 \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     a_2 \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtake_action(s[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     25\u001b[0m     next_s, r, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep([a_1, a_2])\n",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m, in \u001b[0;36mPPO.take_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     16\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray([state]), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     17\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state)\n\u001b[1;32m---> 18\u001b[0m action_dist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m action \u001b[38;5;241m=\u001b[39m action_dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\distributions\\categorical.py:73\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     70\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[0;32m     72\u001b[0m )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\distributions\\distribution.py:70\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     69\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, param)\n\u001b[1;32m---> 70\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[43mconstraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_is_all_true(valid):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m     )\n",
      "File \u001b[1;32mD:\\python\\Anaconda3\\envs\\ma-gym\\lib\\site-packages\\torch\\distributions\\constraints.py:472\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(value \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m ((\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mabs() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "win_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            transition_dict_1 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            transition_dict_2 = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            s = env.reset()  # 返回 m 组状态\n",
    "            t = 1\n",
    "            terminal = False\n",
    "            while not terminal:\n",
    "                a_1 = agent.take_action(s[0])\n",
    "                a_2 = agent.take_action(s[1])\n",
    "                next_s, r, done, info = env.step([a_1, a_2])\n",
    "                win = is_win(done, t, env._max_steps, info)\n",
    "                \n",
    "                transition_dict_1['states'].append(s[0])\n",
    "                transition_dict_1['actions'].append(a_1)\n",
    "                transition_dict_1['next_states'].append(next_s[0])\n",
    "                transition_dict_1['rewards'].append(\n",
    "                    r[0] + 100 if win else r[0] - 0.1)\n",
    "                transition_dict_1['dones'].append(False)\n",
    "                \n",
    "                transition_dict_2['states'].append(s[1])\n",
    "                transition_dict_2['actions'].append(a_2)\n",
    "                transition_dict_2['next_states'].append(next_s[1])\n",
    "                transition_dict_2['rewards'].append(\n",
    "                    r[1] + 100 if win else r[1] - 0.1)\n",
    "                transition_dict_2['dones'].append(False)\n",
    "                \n",
    "                s = next_s\n",
    "                t += 1\n",
    "                terminal = all(done)  # 到达 最大步长 或者 一方智能体全部没有血了\n",
    "                \n",
    "            win_list.append(1 if win else 0)\n",
    "            agent.update(transition_dict_1)\n",
    "            agent.update(transition_dict_2)\n",
    "            \n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(win_list[-100:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822df42df8f03683",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "在 Combat 环境中，共享了两个智能体之间的策略，以达到更好的效果。但这仅限于多个智能体同质的情况，若它们的状态空间或动作空间不一致，那便无法进行策略共享。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
