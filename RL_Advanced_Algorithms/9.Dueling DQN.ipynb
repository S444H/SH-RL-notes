{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 9. Dueling DQN\n",
    "- DQN 算法敲开了深度强化学习的大门，但是作为先驱性的工作，其本身存在着一些问题以及一些可以改进的地方。于是，在 DQN 之后，学术界涌现出了非常多的改进算法...\n",
    "\n",
    "其中有两个非常著名的算法：**Double DQN 和 Dueling DQN**\n",
    "本节学习 **Dueling DQN** --- [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)\n",
    "(更多、更详细的 DQN 改进方法，见 Rainbow 模型的论文及其引用文献 --- [Rainbow: Combining Improvements in Deep Reinforcement Learning](https://arxiv.org/abs/1710.02298)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5b978a944d6461f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9.1 Dueling DQN 原理\n",
    "根据前期的学习可知:在同一状态下,**所有动作**的动作价值的期望就是**该状态**的状态价值\n",
    "那么完全可以将 **状态价值** 剥离出来\n",
    "将 **状态动作价值函数Q** 减去 **状态价值函数V** 的结果定义为 **优势函数A** :$A(s,a)=Q(s,a)-V(s)$\n",
    "该函数可以独自表达各个动作在同一状态下的 **差异**，即衡量在状态 s 下选择动作 a 相对于平均水平的好坏"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2ed20acef51cee4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "实际上原始的DQN算法Q值也有大小之分,那么为何要多此一举定义一个独自表达差异的函数呢?\n",
    "本质上是要提升网络学习效率和泛化能力:\n",
    "- 不是所有状态都需要区分动作(核心)：如在 Atari 游戏中，角色正处于安全区，向左/向右/跳跃都不会立即影响得分，但原始 DQN 仍要为每个动作分别拟合 Q 值\n",
    "- 更结构化，对动作多差异小的状态更有优势：分离状态与差异,根据实际情况有选择的关注(当智能体前面没有车时，车辆自身动作并没有太大差异，此时智能体更关注状态价值；而当智能体前面有车时（智能体需要超车），智能体开始关注不同动作优势值的差异)\n",
    "\n",
    "![状态价值和优势值的简单例子](Illustrations/状态价值和优势值的简单例子.png)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48587e6f1a64664f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在 Dueling DQN 中，Q 网络被重建为：\n",
    "$$Q_{\\eta,\\alpha,\\beta}(s,a)=V_{\\eta,\\alpha}(s)+A_{\\eta,\\beta}(s,a)$$\n",
    "#### 参数说明:\n",
    "$\\eta$(整个函数近似器（神经网络）的共享参数)：在 Dueling DQN 网络中，该参数是整个函数近似器（神经网络）的共享参数，用于前面几层提取共享状态的表示\n",
    "$\\alpha$：专用于拟合状态价值函数的网络分支的参数，而该网络从共享特征中抽取更专注于“状态本身”的信息\n",
    "$\\beta$：专用于拟合优势函数的网络分支的参数，该网络主要用于捕捉特定动作的相对优势\n",
    "\n",
    "以下为网络结构对比图：\n",
    "![DQN与Dueling%20DQN网络结构](Illustrations/DQN与Dueling%20DQN网络结构.jpg)\n",
    "- 同时如此设计Q网络，可以使每一次更新时，函数都会被更新，这也会影响到其他动作的Q值(传统的 DQN 只会更新某个动作的值) \n",
    "\n",
    "因此，Dueling DQN 能够更加频繁、准确地学习状态价值函数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6cbd34d2f60e1865"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 不唯一性的问题：\n",
    "##### 来源\n",
    "$$Q(s,a)=V(s)+A(s,a)=\n",
    "\\begin{pmatrix}\n",
    "V(s)+C(s)\n",
    "\\end{pmatrix}+\n",
    "\\begin{pmatrix}\n",
    "A(s,a)-C(s)\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "- 其中$C(s)$是任意与动作无关的函数（只依赖于状态）\n",
    "\n",
    "可见分解并不是唯一，存在无限多组 V 和 A 的组合，使得加起来等于相同的 Q 这就导致了训练的不稳定性\n",
    "\n",
    "##### 解决\n",
    "为了消除这种不唯一性，Dueling DQN通常会使用一些归一化策略，使优势函数满足一定**约束**，最经典的有：\n",
    "1. 减去最大优势：强制最优动作的优势函数的实际输出为 0 \n",
    "$$Q(s,a)=V(s)+\\left(A(s,a)-\\max_{a^\\prime}A(s,a^\\prime)\\right)$$\n",
    "此时有$V(s)=\\max_{a}Q(s,a)$\n",
    "\n",
    "2. 减去优势函数的均值：优势函数去中心化，保证所有动作优势的平均值为 0\n",
    "$$Q(s,a)=V(s)+\\left(A(s,a)-\\frac{1}{|\\mathcal{A}|}\\sum_{a^{\\prime}}A(s,a^{\\prime})\\right)$$\n",
    "此时有$V(s)=\\frac{1}{|\\mathcal{A}|}\\sum_{a^{\\prime}}Q(s,a^{\\prime})$\n",
    "\n",
    "##### 常使用第2种方法,更稳定\n",
    "但会导致该结构形式下的 Q 值不再严格满足 Bellman 最优方程，以下为数学直观表达:\n",
    "$$A(s,a_1)=4,\\quad A(s,a_2)=2$$\n",
    "$$\\frac{1}{|\\mathcal{A}|}\\sum_{a^{\\prime}}Q(s,a^{\\prime})=(4+2)/2=3$$\n",
    "$$\\begin{gathered}\n",
    "Q(s,a_1)=10+(4-3)=11 \\\\\n",
    "Q(s,a_2)=10+(2-3)=9\n",
    "\\end{gathered}$$\n",
    "| Q值       | 原始方式 | 归一化后 | 差异   |\n",
    "|----------| ---- | ---- | ---- |\n",
    "| $Q(s,a_1)$ | 14   | 11   | ⬇️ 3 |\n",
    "| $Q(s,a_2)$ | 12   | 9    | ⬇️ 3 |\n",
    "| $\\max Q$ | 14   | 11   | ⬇️ 3 |\n",
    "- 网络学到的 Q 实际不是 Bellman 方程中的 Q 值，而是“减了常数”的版本，偏离真实更新目标\n",
    "\n",
    "尽管在训练后期这种影响，不会磨灭动作之间的本质差异，但是RL想接近精确最优，需要结构满足 Bellman 最优性\n",
    "复杂任务下，不精确会迅速放大误差"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afda6baacd610a59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 9.2 Dueling DQN 代码实现"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30c9bb6cd80ce21b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入相关库："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ed5d03e4874c7e9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm\n",
    "\"\"\"在绘图时引入,防止绘图时失效\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "\"\"\"\n",
    "# 神经网络\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-19T03:57:47.316569300Z",
     "start_time": "2025-07-19T03:57:47.294507100Z"
    }
   },
   "id": "47eb873491628644"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from utils.replay_buffer import ReplayBuffer\n",
    "from utils.train import train_on_policy_agent, train_off_policy_agent\n",
    "from utils.advantage import compute_advantage\n",
    "from utils.smoothing import moving_average"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-19T03:57:47.335109300Z",
     "start_time": "2025-07-19T03:57:47.301107200Z"
    }
   },
   "id": "b55a19c9bf8f9cc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> 定义状态价值函数和优势函数的复合神经网络VAnet："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be03b2e1a352fe7e"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class VAnet(torch.nn.Module):\n",
    "    \"\"\" 只有一层隐藏层的A网络和V网络 \"\"\" \n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(VAnet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_A = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_V = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        A = self.fc_A(F.relu(self.fc1(x)))\n",
    "        V = self.fc_V(F.relu(self.fc1(x)))\n",
    "        Q = V + A - A.mean(1).view(-1, 1)\n",
    "        return Q"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-19T03:57:47.348506700Z",
     "start_time": "2025-07-19T03:57:47.308058300Z"
    }
   },
   "id": "350b147cb1c34b30"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Dueling DQN 算法："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e3e02d16829f514f"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class DuelingDQN:\n",
    "    \"\"\" 标准 Dueling DQN 算法 \"\"\"\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 hidden_dim,\n",
    "                 action_dim,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon,\n",
    "                 target_update,\n",
    "                 device):\n",
    "        # ------------------------- Dueling DQN只是采取不一样的网络框架\n",
    "        self.q_net = VAnet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_q_net = VAnet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # -------------------------\n",
    "        self.action_dim = action_dim\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=learning_rate)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.target_update = target_update\n",
    "        self.count = 0\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        return self.q_net(state).argmax().item()  # 返回索引\n",
    "\n",
    "    def max_q_value(self, state):\n",
    "        state = torch.tensor(np.array([state]), dtype=torch.float).to(self.device)\n",
    "        return self.q_net(state).max().item()  # 返回数值\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        q_values = self.q_net(states).gather(1, actions)\n",
    "\n",
    "        with torch.no_grad():  # 禁用梯度追踪，只是在评估 target 网络，不需要训练它，节省显存，提升速度\n",
    "            max_next_q_values = self.target_q_net(next_states).max(1)[0].view(-1, 1)\n",
    "            q_targets = rewards + self.gamma * max_next_q_values * (1 - dones)\n",
    "\n",
    "        loss = F.mse_loss(q_values, q_targets)  # 默认已经是 mean\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.count % self.target_update == 0:\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.count += 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-07-19T03:57:47.350012900Z",
     "start_time": "2025-07-19T03:57:47.317569400Z"
    }
   },
   "id": "7acd11b72df37749"
  },
  {
   "cell_type": "markdown",
   "source": [
    "> Env设置(包括可重复性随机种子, 确保每次重新运行本节内容结果一致):"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86b94664b898511d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random.seed(0)       # 设置 Python 的随机种子\n",
    "np.random.seed(0)    # 设置 NumPy 的随机种子\n",
    "torch.manual_seed(0) # 设置 PyTorch CPU 随机种子\n",
    "torch.cuda.manual_seed_all(0) # 设置 PyTorch GPU 随机种子, 由于GPU并行性, 只能极大减小偏差\n",
    "\n",
    "env = gym.make('CartPole-v1')  # CartPole-v1 最大回合步数修改到了500步(v0为200)\n",
    "#env = env.unwrapped # 获取原始环境（绕过 TimeLimit 包装器）解除最大步数500限制\n",
    "env.reset(seed=0)   # 环境通常依赖于其他随机数生成器来初始化状态、进行探索(推荐位于以上随机之后)\n",
    "print(\"Environment spec:\", env.spec)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ead38e08c1ef7614"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
